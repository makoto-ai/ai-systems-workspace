#!/usr/bin/env python3
"""
Prompt Optimization for Golden Test Phase 4
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå›ºæœ‰åè©ä¿è­·ãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¼·åŒ–ãƒ»æ•°å€¤ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰
"""

import json
import argparse
import time
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import sys
import requests
import re

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    sys.path.append(str(Path(__file__).parent.parent / "tests" / "golden"))
    from evaluator import score
    from root_cause_analyzer import analyze_failure_root_cause, RootCause
except ImportError as e:
    print(f"Import error: {e}")
    print("Please ensure modules are accessible from project root")
    sys.exit(1)

class PromptOptimizer:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–å®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.api_key = os.getenv('GROQ_API_KEY')
        if not self.api_key:
            raise Exception("GROQ_API_KEY not found")
        
        # Phase4å‘ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        self.prompt_variants = {
            "baseline": """ä»¥ä¸‹ã®å…¥åŠ›ã«å¯¾ã—ã¦ã€é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç©ºç™½åŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›: {input_text}

å‡ºåŠ›ã¯å¿…ãšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼ˆèª¬æ˜æ–‡ä¸è¦ï¼‰:""",
            
            "compound_protected": """ä»¥ä¸‹ã®å…¥åŠ›ã«å¯¾ã—ã¦ã€é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç©ºç™½åŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
è¤‡åˆèªãƒ»å›ºæœ‰åè©ã¯åˆ†å‰²ã›ãšã€ãã®ã¾ã¾å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›: {input_text}

ä¾‹:
å…¥åŠ›: ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã®åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
å‡ºåŠ›: ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ä½œæˆ

å…¥åŠ›: å–¶æ¥­ãƒ­ãƒ¼ãƒ—ãƒ¬AIã‚·ã‚¹ãƒ†ãƒ ã®æ”¹å–„
å‡ºåŠ›: å–¶æ¥­ãƒ­ãƒ¼ãƒ—ãƒ¬ AIã‚·ã‚¹ãƒ†ãƒ  æ”¹å–„

å…¥åŠ›: CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è‡ªå‹•åŒ–è¨­å®š
å‡ºåŠ›: CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ è‡ªå‹•åŒ– è¨­å®š

å‡ºåŠ›ã¯å¿…ãšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼ˆèª¬æ˜æ–‡ä¸è¦ï¼‰:""",
            
            "format_enhanced": """ä»¥ä¸‹ã®å…¥åŠ›ã«å¯¾ã—ã¦ã€é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç©ºç™½åŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
è¤‡åˆèªã¯åˆ†å‰²ã›ãšã€æ•°å€¤ãƒ»å˜ä½ã¯æ­£ç¢ºã«ã€å°‚é–€ç”¨èªã¯åŸå½¢ã®ã¾ã¾å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›: {input_text}

ã€å‡ºåŠ›ä¾‹ã€‘
å…¥åŠ›: å£²ä¸Šåˆ†æã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ”¹å–„ã§90%ã®ç²¾åº¦å‘ä¸Š
å‡ºåŠ›: å£²ä¸Šåˆ†æ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ æ”¹å–„ 90% ç²¾åº¦å‘ä¸Š

å…¥åŠ›: éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã®å¿œç­”æ™‚é–“ã‚’200msçŸ­ç¸®
å‡ºåŠ›: éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ  å¿œç­”æ™‚é–“ 200ms çŸ­ç¸®

å…¥åŠ›: å–¶æ¥­ãƒ­ãƒ¼ãƒ—ãƒ¬è‡ªå‹•åŒ–ã«ã‚ˆã‚‹CIæ•´å‚™ã®åŠ¹ç‡åŒ–
å‡ºåŠ›: å–¶æ¥­ãƒ­ãƒ¼ãƒ—ãƒ¬ è‡ªå‹•åŒ– CIæ•´å‚™ åŠ¹ç‡åŒ–

ã€é‡è¦ã€‘å‡ºåŠ›ã¯å¿…ãšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼ˆèª¬æ˜æ–‡ãƒ»æ–‡ç« ã¯ä¸è¦ï¼‰:""",
            
            "template_explicit": """ã‚¿ã‚¹ã‚¯: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
å…¥åŠ›: {input_text}
å‡ºåŠ›å½¢å¼: ç©ºç™½åŒºåˆ‡ã‚Šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

ã€æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
1. è¤‡åˆèªã¯åˆ†å‰²ã—ãªã„ï¼ˆä¾‹ï¼šã€Œå–¶æ¥­ãƒ­ãƒ¼ãƒ—ãƒ¬ã€ã€Œåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã€ï¼‰
2. æ•°å€¤ã¨å˜ä½ã¯ã‚»ãƒƒãƒˆã§å‡ºåŠ›ï¼ˆä¾‹ï¼šã€Œ90%ã€ã€Œ200msã€ï¼‰
3. å°‚é–€ç”¨èªã¯çœç•¥ã—ãªã„ï¼ˆä¾‹ï¼šã€ŒAIã€â†’ã€ŒAIã‚·ã‚¹ãƒ†ãƒ ã€ã€ã€ŒCIã€â†’ã€ŒCIæ•´å‚™ã€ï¼‰
4. èª¬æ˜æ–‡ãƒ»æ–‡ç« ã¯ä¸€åˆ‡å«ã‚ãªã„

ã€å‡ºåŠ›ä¾‹ã€‘
å–¶æ¥­ã‚·ã‚¹ãƒ†ãƒ  â†’ å–¶æ¥­ã‚·ã‚¹ãƒ†ãƒ 
åˆ†ææ©Ÿèƒ½ã®å‘ä¸Š â†’ åˆ†ææ©Ÿèƒ½ å‘ä¸Š
90%ã®æ”¹å–„åŠ¹æœ â†’ 90% æ”¹å–„åŠ¹æœ

å‡ºåŠ›:"""
        }
    
    def get_model_cases(self, root_cause_filter: str = None, limit: int = None) -> List[Dict]:
        """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å–å¾—ï¼ˆroot_cause_filterã§çµã‚Šè¾¼ã¿å¯èƒ½ï¼‰"""
        cases_dir = Path("tests/golden/cases")
        if not cases_dir.exists():
            raise FileNotFoundError(f"Test cases directory not found: {cases_dir}")
        
        # ãƒ­ã‚°ã‹ã‚‰å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’ç‰¹å®š
        failed_cases = set()
        if root_cause_filter:
            logs_dir = Path("tests/golden/logs")
            if logs_dir.exists():
                log_files = sorted(logs_dir.glob("*.jsonl"), key=lambda x: x.stat().st_mtime, reverse=True)
                if log_files:
                    latest_log = log_files[0]
                    with open(latest_log, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                try:
                                    data = json.loads(line)
                                    if not data.get('passed', True):
                                        case_id = data.get('id', '')
                                        reference = data.get('reference', '')
                                        prediction = data.get('prediction', '')
                                        test_score = data.get('score', 0.0)
                                        
                                        # Root Causeåˆ†æ
                                        root_cause = analyze_failure_root_cause(case_id, reference, prediction, test_score)
                                        if root_cause.value == root_cause_filter:
                                            failed_cases.add(case_id)
                                except json.JSONDecodeError:
                                    continue
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
        target_cases = []
        for case_file in sorted(cases_dir.glob("*.json")):
            try:
                with open(case_file, 'r', encoding='utf-8') as f:
                    case_data = json.load(f)
                
                case_id = case_data.get("id", case_file.stem)
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if root_cause_filter and case_id not in failed_cases:
                    continue
                
                target_cases.append(case_data)
                
                # ä»¶æ•°åˆ¶é™
                if limit and len(target_cases) >= limit:
                    break
                    
            except Exception as e:
                print(f"âŒ Error loading {case_file}: {e}")
                continue
        
        return target_cases
    
    def predict_with_prompt(self, input_text: str, prompt_template: str) -> str:
        """æŒ‡å®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§äºˆæ¸¬å®Ÿè¡Œ"""
        prompt = prompt_template.format(input_text=input_text)
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': 'llama3-8b-8192',
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': 0.0,
            'max_tokens': 80
        }
        
        try:
            response = requests.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                prediction = result['choices'][0]['message']['content']
            else:
                raise Exception(f"API error: {response.status_code}")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            prediction = prediction.strip().replace('\n', ' ')
            prediction = ' '.join(prediction.split())  # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã«
            
            return prediction
            
        except Exception as e:
            print(f"âŒ Prediction error: {e}")
            return ""
    
    def run_optimization_experiment(self, cases: List[Dict], budget: int = 20) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        results = {}
        
        # äºˆç®—å†…ã§ã‚±ãƒ¼ã‚¹ã‚’åˆ¶é™
        if len(cases) > budget:
            cases = cases[:budget]
            print(f"âš ï¸ Budget limit: Testing {budget} cases out of {len(cases)}")
        
        for variant_name, prompt_template in self.prompt_variants.items():
            print(f"ğŸ”¬ Testing prompt variant: {variant_name}")
            
            variant_results = {
                "variant": variant_name,
                "cases": [],
                "pass_at_85": 0,  # Phase4: 0.85ã—ãã„å€¤ã§ã®åˆæ ¼ç‡
                "avg_score": 0.0,
                "jaccard_avg": 0.0,
                "api_calls": 0,
                "total_time": 0.0
            }
            
            total_score = 0.0
            total_jaccard = 0.0
            passed_at_85 = 0
            start_time = time.time()
            
            for case in cases:
                case_id = case.get("id", "unknown")
                reference = case.get("reference", "")
                input_text = case.get("input", "")
                
                try:
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆäºˆæ¸¬å®Ÿè¡Œ
                    case_start = time.time()
                    prediction = self.predict_with_prompt(input_text, prompt_template)
                    case_time = time.time() - case_start
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—
                    test_score = score(reference, prediction)
                    passed_85 = test_score >= 0.85  # Phase4ã—ãã„å€¤
                    
                    # Jaccardé¡ä¼¼åº¦è¨ˆç®—
                    jaccard = self._calculate_jaccard(reference, prediction)
                    
                    if passed_85:
                        passed_at_85 += 1
                    
                    total_score += test_score
                    total_jaccard += jaccard
                    variant_results["api_calls"] += 1
                    
                    case_result = {
                        "case_id": case_id,
                        "score": test_score,
                        "jaccard": jaccard,
                        "passed_85": passed_85,
                        "time_ms": case_time * 1000,
                        "reference": reference,
                        "prediction": prediction
                    }
                    variant_results["cases"].append(case_result)
                    
                    print(f"  {case_id}: {test_score:.3f} ({'âœ…' if passed_85 else 'âŒ'})")
                    
                except Exception as e:
                    print(f"  âŒ {case_id}: Error - {e}")
                    case_result = {
                        "case_id": case_id,
                        "score": 0.0,
                        "jaccard": 0.0,
                        "passed_85": False,
                        "error": str(e),
                        "time_ms": 0
                    }
                    variant_results["cases"].append(case_result)
            
            # çµ±è¨ˆè¨ˆç®—
            total_time = time.time() - start_time
            variant_results["pass_at_85"] = passed_at_85 / len(cases) if cases else 0
            variant_results["avg_score"] = total_score / len(cases) if cases else 0
            variant_results["jaccard_avg"] = total_jaccard / len(cases) if cases else 0
            variant_results["total_time"] = total_time
            
            print(f"  ğŸ“Š Pass@0.85: {variant_results['pass_at_85']:.1%}")
            print(f"  ğŸ“Š Avg Score: {variant_results['avg_score']:.3f}")
            print(f"  ğŸ“Š Jaccard: {variant_results['jaccard_avg']:.3f}")
            print(f"  â±ï¸ Total Time: {total_time:.2f}s")
            
            results[variant_name] = variant_results
        
        return results
    
    def _calculate_jaccard(self, reference: str, prediction: str) -> float:
        """Jaccardé¡ä¼¼åº¦è¨ˆç®—"""
        ref_tokens = set(reference.split()) if reference else set()
        pred_tokens = set(prediction.split()) if prediction else set()
        
        if not ref_tokens and not pred_tokens:
            return 1.0
        if not ref_tokens or not pred_tokens:
            return 0.0
        
        intersection = len(ref_tokens & pred_tokens)
        union = len(ref_tokens | pred_tokens)
        
        return intersection / union
    
    def generate_improvement_suggestions(self, results: Dict[str, Any]) -> List[Dict[str, str]]:
        """çµæœã‹ã‚‰æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = []
        
        # æœ€å„ªç§€ãƒãƒªã‚¢ãƒ³ãƒˆã‚’ç‰¹å®š
        best_variant = max(results.keys(), key=lambda k: results[k]["pass_at_85"])
        best_performance = results[best_variant]["pass_at_85"]
        
        if best_performance > results.get("baseline", {}).get("pass_at_85", 0):
            improvement = best_performance - results.get("baseline", {}).get("pass_at_85", 0)
            suggestions.append({
                "type": "prompt:best-variant",
                "description": f"{best_variant}ã«ã‚ˆã‚Š{improvement:.1%}ã®æ”¹å–„",
                "priority": "high" if improvement > 0.1 else "medium"
            })
        
        # è¤‡åˆèªä¿è­·ã®åŠ¹æœãƒã‚§ãƒƒã‚¯
        if "compound_protected" in results:
            compound_perf = results["compound_protected"]["pass_at_85"]
            baseline_perf = results.get("baseline", {}).get("pass_at_85", 0)
            if compound_perf > baseline_perf:
                suggestions.append({
                    "type": "prompt:compound-lock", 
                    "description": f"è¤‡åˆèªä¿è­·ã§{compound_perf - baseline_perf:.1%}æ”¹å–„",
                    "priority": "high"
                })
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¼·åŒ–ã®åŠ¹æœãƒã‚§ãƒƒã‚¯
        if "format_enhanced" in results:
            format_perf = results["format_enhanced"]["pass_at_85"]
            baseline_perf = results.get("baseline", {}).get("pass_at_85", 0)
            if format_perf > baseline_perf:
                suggestions.append({
                    "type": "prompt:format-enhance",
                    "description": f"ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¼·åŒ–ã§{format_perf - baseline_perf:.1%}æ”¹å–„",
                    "priority": "medium"
                })
        
        return suggestions

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Prompt Optimization Experiment")
    parser.add_argument("--budget", type=int, default=20,
                       help="å®Ÿé¨“äºˆç®—ï¼ˆAPIå‘¼ã³å‡ºã—æ•°ã®åˆ¶é™ï¼‰")
    parser.add_argument("--metric", choices=["jaccard", "score"], default="jaccard",
                       help="æœ€é©åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
    parser.add_argument("--out", type=str, default="out/prompt_opt_phase4.json",
                       help="çµæœå‡ºåŠ›ãƒ‘ã‚¹")
    parser.add_argument("--filter", choices=["MODEL", "PROMPT", "TOKENIZE"], 
                       help="ç‰¹å®šã®root_causeã®ã¿ãƒ†ã‚¹ãƒˆ")
    
    args = parser.parse_args()
    
    try:
        optimizer = PromptOptimizer()
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å–å¾—
        print(f"ğŸ” Collecting test cases (filter: {args.filter or 'all'})")
        target_cases = optimizer.get_model_cases(args.filter, limit=args.budget)
        
        if not target_cases:
            print(f"âŒ No cases found for filter: {args.filter}")
            return False
        
        print(f"ğŸ“‹ Found {len(target_cases)} cases (budget: {args.budget})")
        
        # æœ€é©åŒ–å®Ÿé¨“å®Ÿè¡Œ
        print(f"ğŸ§ª Running prompt optimization experiment")
        results = optimizer.run_optimization_experiment(target_cases, args.budget)
        
        # æ”¹å–„ææ¡ˆç”Ÿæˆ
        suggestions = optimizer.generate_improvement_suggestions(results)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = {
            "experiment": {
                "timestamp": datetime.now().isoformat(),
                "budget": args.budget,
                "metric": args.metric,
                "root_cause_filter": args.filter,
                "total_cases": len(target_cases)
            },
            "results": results,
            "improvement_suggestions": suggestions,
            "summary": {
                "best_variant": max(results.keys(), key=lambda k: results[k]["pass_at_85"]) if results else None,
                "best_pass_at_85": max(r["pass_at_85"] for r in results.values()) if results else 0,
                "baseline_improvement": (max(r["pass_at_85"] for r in results.values()) - 
                                       results.get("baseline", {}).get("pass_at_85", 0)) if results else 0
            }
        }
        
        # çµæœä¿å­˜
        output_path = Path(args.out)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ Prompt optimization report saved: {output_path}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"\nğŸ“Š Optimization Summary:")
        print(f"  Best Variant: {report['summary']['best_variant']}")
        print(f"  Best Pass@0.85: {report['summary']['best_pass_at_85']:.1%}")
        print(f"  Improvement: {report['summary']['baseline_improvement']:+.1%}")
        
        print(f"\nğŸ’¡ Top Suggestions:")
        for i, suggestion in enumerate(suggestions[:3], 1):
            print(f"  {i}. {suggestion['type']}: {suggestion['description']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Optimization failed: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)