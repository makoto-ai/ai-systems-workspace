#!/usr/bin/env python3
"""
Prompt Optimization for Golden Test Phase 4
„Éó„É≠„É≥„Éó„ÉàÊúÄÈÅ©ÂåñÂÆüÈ®ì„Ç∑„Çπ„ÉÜ„É†ÔºàÂõ∫ÊúâÂêçË©û‰øùË≠∑„Éª„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂº∑Âåñ„ÉªÊï∞ÂÄ§„ÉÜ„É≥„Éó„É¨„Éº„ÉàÔºâ
"""

import json
import argparse
import time
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import sys
import requests
import re

# „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É´„Éº„Éà„ÇíPython„Éë„Çπ„Å´ËøΩÂä†
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    sys.path.append(str(Path(__file__).parent.parent / "tests" / "golden"))
    from evaluator import score
    from root_cause_analyzer import analyze_failure_root_cause, RootCause
except ImportError as e:
    print(f"Import error: {e}")
    print("Please ensure modules are accessible from project root")
    sys.exit(1)

class PromptOptimizer:
    """„Éó„É≠„É≥„Éó„ÉàÊúÄÈÅ©ÂåñÂÆüÈ®ì„ÇØ„É©„Çπ"""
    
    def __init__(self):
        self.api_key = os.getenv('GROQ_API_KEY')
        if not self.api_key:
            raise Exception("GROQ_API_KEY not found")
        
        # Phase4Âêë„Åë„Éó„É≠„É≥„Éó„Éà„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥
        self.prompt_variants = {
            "baseline": """‰ª•‰∏ã„ÅÆÂÖ•Âäõ„Å´ÂØæ„Åó„Å¶„ÄÅÈñ¢ÈÄ£„Åô„Çã„Ç≠„Éº„ÉØ„Éº„Éâ„ÇíÁ©∫ÁôΩÂå∫Âàá„Çä„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

ÂÖ•Âäõ: {input_text}

Âá∫Âäõ„ÅØÂøÖ„Åö„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆ„ÅøÔºàË™¨ÊòéÊñá‰∏çË¶ÅÔºâ:""",
            
            "compound_protected": """‰ª•‰∏ã„ÅÆÂÖ•Âäõ„Å´ÂØæ„Åó„Å¶„ÄÅÈñ¢ÈÄ£„Åô„Çã„Ç≠„Éº„ÉØ„Éº„Éâ„ÇíÁ©∫ÁôΩÂå∫Âàá„Çä„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Ë§áÂêàË™û„ÉªÂõ∫ÊúâÂêçË©û„ÅØÂàÜÂâ≤„Åõ„Åö„ÄÅ„Åù„ÅÆ„Åæ„ÅæÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

ÂÖ•Âäõ: {input_text}

‰æã:
ÂÖ•Âäõ: „Éû„Éº„Ç±„ÉÜ„Ç£„É≥„Ç∞Êà¶Áï•„ÅÆÂàÜÊûê„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ‰ΩúÊàê
Âá∫Âäõ: „Éû„Éº„Ç±„ÉÜ„Ç£„É≥„Ç∞Êà¶Áï• ÂàÜÊûê„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ ‰ΩúÊàê

ÂÖ•Âäõ: Âñ∂Ê•≠„É≠„Éº„Éó„É¨AI„Ç∑„Çπ„ÉÜ„É†„ÅÆÊîπÂñÑ
Âá∫Âäõ: Âñ∂Ê•≠„É≠„Éº„Éó„É¨ AI„Ç∑„Çπ„ÉÜ„É† ÊîπÂñÑ

ÂÖ•Âäõ: CI/CD„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆËá™ÂãïÂåñË®≠ÂÆö
Âá∫Âäõ: CI/CD „Éë„Ç§„Éó„É©„Ç§„É≥ Ëá™ÂãïÂåñ Ë®≠ÂÆö

Âá∫Âäõ„ÅØÂøÖ„Åö„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆ„ÅøÔºàË™¨ÊòéÊñá‰∏çË¶ÅÔºâ:""",
            
            "format_enhanced": """‰ª•‰∏ã„ÅÆÂÖ•Âäõ„Å´ÂØæ„Åó„Å¶„ÄÅÈñ¢ÈÄ£„Åô„Çã„Ç≠„Éº„ÉØ„Éº„Éâ„ÇíÁ©∫ÁôΩÂå∫Âàá„Çä„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Ë§áÂêàË™û„ÅØÂàÜÂâ≤„Åõ„Åö„ÄÅÊï∞ÂÄ§„ÉªÂçò‰Ωç„ÅØÊ≠£Á¢∫„Å´„ÄÅÂ∞ÇÈñÄÁî®Ë™û„ÅØÂéüÂΩ¢„ÅÆ„Åæ„ÅæÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

ÂÖ•Âäõ: {input_text}

„ÄêÂá∫Âäõ‰æã„Äë
ÂÖ•Âäõ: Â£≤‰∏äÂàÜÊûê„ÅÆ„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„ÉâÊîπÂñÑ„Åß90%„ÅÆÁ≤æÂ∫¶Âêë‰∏ä
Âá∫Âäõ: Â£≤‰∏äÂàÜÊûê „ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ ÊîπÂñÑ 90% Á≤æÂ∫¶Âêë‰∏ä

ÂÖ•Âäõ: Èü≥Â£∞Ë™çË≠ò„Ç∑„Çπ„ÉÜ„É†„ÅÆÂøúÁ≠îÊôÇÈñì„Çí200msÁü≠Á∏Æ
Âá∫Âäõ: Èü≥Â£∞Ë™çË≠ò„Ç∑„Çπ„ÉÜ„É† ÂøúÁ≠îÊôÇÈñì 200ms Áü≠Á∏Æ

ÂÖ•Âäõ: Âñ∂Ê•≠„É≠„Éº„Éó„É¨Ëá™ÂãïÂåñ„Å´„Çà„ÇãCIÊï¥ÂÇô„ÅÆÂäπÁéáÂåñ
Âá∫Âäõ: Âñ∂Ê•≠„É≠„Éº„Éó„É¨ Ëá™ÂãïÂåñ CIÊï¥ÂÇô ÂäπÁéáÂåñ

„ÄêÈáçË¶Å„ÄëÂá∫Âäõ„ÅØÂøÖ„Åö„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆ„ÅøÔºàË™¨ÊòéÊñá„ÉªÊñáÁ´†„ÅØ‰∏çË¶ÅÔºâ:""",
            
            "template_explicit": """„Çø„Çπ„ÇØ: „Ç≠„Éº„ÉØ„Éº„ÉâÊäΩÂá∫
ÂÖ•Âäõ: {input_text}
Âá∫ÂäõÂΩ¢Âºè: Á©∫ÁôΩÂå∫Âàá„Çä„Ç≠„Éº„ÉØ„Éº„Éâ

„ÄêÊäΩÂá∫„É´„Éº„É´„Äë
1. Ë§áÂêàË™û„ÅØÂàÜÂâ≤„Åó„Å™„ÅÑÔºà‰æãÔºö„ÄåÂñ∂Ê•≠„É≠„Éº„Éó„É¨„Äç„ÄåÂàÜÊûê„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„ÄçÔºâ
2. Êï∞ÂÄ§„Å®Âçò‰Ωç„ÅØ„Çª„ÉÉ„Éà„ÅßÂá∫ÂäõÔºà‰æãÔºö„Äå90%„Äç„Äå200ms„ÄçÔºâ
3. Â∞ÇÈñÄÁî®Ë™û„ÅØÁúÅÁï•„Åó„Å™„ÅÑÔºà‰æãÔºö„ÄåAI„Äç‚Üí„ÄåAI„Ç∑„Çπ„ÉÜ„É†„Äç„ÄÅ„ÄåCI„Äç‚Üí„ÄåCIÊï¥ÂÇô„ÄçÔºâ
4. Ë™¨ÊòéÊñá„ÉªÊñáÁ´†„ÅØ‰∏ÄÂàáÂê´„ÇÅ„Å™„ÅÑ

„ÄêÂá∫Âäõ‰æã„Äë
Âñ∂Ê•≠„Ç∑„Çπ„ÉÜ„É† ‚Üí Âñ∂Ê•≠„Ç∑„Çπ„ÉÜ„É†
ÂàÜÊûêÊ©üËÉΩ„ÅÆÂêë‰∏ä ‚Üí ÂàÜÊûêÊ©üËÉΩ Âêë‰∏ä
90%„ÅÆÊîπÂñÑÂäπÊûú ‚Üí 90% ÊîπÂñÑÂäπÊûú

Âá∫Âäõ:"""
        }
    
    def get_model_cases(self, root_cause_filter: str = None, limit: int = None) -> List[Dict]:
        """„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„ÇíÂèñÂæóÔºàroot_cause_filter„ÅßÁµû„ÇäËæº„ÅøÂèØËÉΩÔºâ"""
        cases_dir = Path("tests/golden/cases")
        if not cases_dir.exists():
            raise FileNotFoundError(f"Test cases directory not found: {cases_dir}")
        
        # „É≠„Ç∞„Åã„ÇâÂ§±Êïó„Ç±„Éº„Çπ„ÇíÁâπÂÆö
        failed_cases = set()
        if root_cause_filter:
            logs_dir = Path("tests/golden/logs")
            if logs_dir.exists():
                log_files = sorted(logs_dir.glob("*.jsonl"), key=lambda x: x.stat().st_mtime, reverse=True)
                if log_files:
                    latest_log = log_files[0]
                    with open(latest_log, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                try:
                                    data = json.loads(line)
                                    if not data.get('passed', True):
                                        case_id = data.get('id', '')
                                        reference = data.get('reference', '')
                                        prediction = data.get('prediction', '')
                                        test_score = data.get('score', 0.0)
                                        
                                        # Root CauseÂàÜÊûê
                                        root_cause = analyze_failure_root_cause(case_id, reference, prediction, test_score)
                                        if root_cause.value == root_cause_filter:
                                            failed_cases.add(case_id)
                                except json.JSONDecodeError:
                                    continue
        
        # „ÉÜ„Çπ„Éà„Ç±„Éº„ÇπË™≠„ÅøËæº„Åø
        target_cases = []
        for case_file in sorted(cases_dir.glob("*.json")):
            try:
                with open(case_file, 'r', encoding='utf-8') as f:
                    case_data = json.load(f)
                
                case_id = case_data.get("id", case_file.stem)
                
                # „Éï„Ç£„É´„Çø„É™„É≥„Ç∞
                if root_cause_filter and case_id not in failed_cases:
                    continue
                
                target_cases.append(case_data)
                
                # ‰ª∂Êï∞Âà∂Èôê
                if limit and len(target_cases) >= limit:
                    break
                    
            except Exception as e:
                print(f"‚ùå Error loading {case_file}: {e}")
                continue
        
        return target_cases
    
    def predict_with_prompt(self, input_text: str, prompt_template: str) -> str:
        """ÊåáÂÆö„Éó„É≠„É≥„Éó„Éà„ÉÜ„É≥„Éó„É¨„Éº„Éà„Åß‰∫àÊ∏¨ÂÆüË°å"""
        prompt = prompt_template.format(input_text=input_text)
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': 'llama3-8b-8192',
            'messages': [{'role': 'user', 'content': prompt}],
            'temperature': 0.0,
            'max_tokens': 80
        }
        
        try:
            response = requests.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                prediction = result['choices'][0]['message']['content']
            else:
                raise Exception(f"API error: {response.status_code}")
            
            # „É¨„Çπ„Éù„É≥„Çπ„Çí„Çµ„Éã„Çø„Ç§„Ç∫
            prediction = prediction.strip().replace('\n', ' ')
            prediction = ' '.join(prediction.split())  # Ë§áÊï∞„Çπ„Éö„Éº„Çπ„ÇíÂçò‰∏Ä„Å´
            
            return prediction
            
        except Exception as e:
            print(f"‚ùå Prediction error: {e}")
            return ""
    
    def run_optimization_experiment(self, cases: List[Dict], budget: int = 20) -> Dict[str, Any]:
        """„Éó„É≠„É≥„Éó„ÉàÊúÄÈÅ©ÂåñÂÆüÈ®ì„ÇíÂÆüË°å"""
        results = {}
        
        # ‰∫àÁÆóÂÜÖ„Åß„Ç±„Éº„Çπ„ÇíÂà∂Èôê
        if len(cases) > budget:
            cases = cases[:budget]
            print(f"‚ö†Ô∏è Budget limit: Testing {budget} cases out of {len(cases)}")
        
        for variant_name, prompt_template in self.prompt_variants.items():
            print(f"üî¨ Testing prompt variant: {variant_name}")
            
            variant_results = {
                "variant": variant_name,
                "cases": [],
                "pass_at_85": 0,  # Phase4: 0.85„Åó„Åç„ÅÑÂÄ§„Åß„ÅÆÂêàÊ†ºÁéá
                "avg_score": 0.0,
                "jaccard_avg": 0.0,
                "api_calls": 0,
                "total_time": 0.0
            }
            
            total_score = 0.0
            total_jaccard = 0.0
            passed_at_85 = 0
            start_time = time.time()
            
            for case in cases:
                case_id = case.get("id", "unknown")
                reference = case.get("reference", "")
                input_text = case.get("input", "")
                
                try:
                    # „Éó„É≠„É≥„Éó„Éà‰∫àÊ∏¨ÂÆüË°å
                    case_start = time.time()
                    prediction = self.predict_with_prompt(input_text, prompt_template)
                    case_time = time.time() - case_start
                    
                    # „Çπ„Ç≥„Ç¢Ë®àÁÆó
                    test_score = score(reference, prediction)
                    passed_85 = test_score >= 0.85  # Phase4„Åó„Åç„ÅÑÂÄ§
                    
                    # JaccardÈ°û‰ººÂ∫¶Ë®àÁÆó
                    jaccard = self._calculate_jaccard(reference, prediction)
                    
                    if passed_85:
                        passed_at_85 += 1
                    
                    total_score += test_score
                    total_jaccard += jaccard
                    variant_results["api_calls"] += 1
                    
                    case_result = {
                        "case_id": case_id,
                        "score": test_score,
                        "jaccard": jaccard,
                        "passed_85": passed_85,
                        "time_ms": case_time * 1000,
                        "reference": reference,
                        "prediction": prediction
                    }
                    variant_results["cases"].append(case_result)
                    
                    print(f"  {case_id}: {test_score:.3f} ({'‚úÖ' if passed_85 else '‚ùå'})")
                    
                except Exception as e:
                    print(f"  ‚ùå {case_id}: Error - {e}")
                    case_result = {
                        "case_id": case_id,
                        "score": 0.0,
                        "jaccard": 0.0,
                        "passed_85": False,
                        "error": str(e),
                        "time_ms": 0
                    }
                    variant_results["cases"].append(case_result)
            
            # Áµ±Ë®àË®àÁÆó
            total_time = time.time() - start_time
            variant_results["pass_at_85"] = passed_at_85 / len(cases) if cases else 0
            variant_results["avg_score"] = total_score / len(cases) if cases else 0
            variant_results["jaccard_avg"] = total_jaccard / len(cases) if cases else 0
            variant_results["total_time"] = total_time
            
            print(f"  üìä Pass@0.85: {variant_results['pass_at_85']:.1%}")
            print(f"  üìä Avg Score: {variant_results['avg_score']:.3f}")
            print(f"  üìä Jaccard: {variant_results['jaccard_avg']:.3f}")
            print(f"  ‚è±Ô∏è Total Time: {total_time:.2f}s")
            
            results[variant_name] = variant_results
        
        return results
    
    def _calculate_jaccard(self, reference: str, prediction: str) -> float:
        """JaccardÈ°û‰ººÂ∫¶Ë®àÁÆó"""
        ref_tokens = set(reference.split()) if reference else set()
        pred_tokens = set(prediction.split()) if prediction else set()
        
        if not ref_tokens and not pred_tokens:
            return 1.0
        if not ref_tokens or not pred_tokens:
            return 0.0
        
        intersection = len(ref_tokens & pred_tokens)
        union = len(ref_tokens | pred_tokens)
        
        return intersection / union
    
    def generate_improvement_suggestions(self, results: Dict[str, Any]) -> List[Dict[str, str]]:
        """ÁµêÊûú„Åã„ÇâÊîπÂñÑÊèêÊ°à„ÇíÁîüÊàê"""
        suggestions = []
        
        # ÊúÄÂÑ™ÁßÄ„Éê„É™„Ç¢„É≥„Éà„ÇíÁâπÂÆö
        best_variant = max(results.keys(), key=lambda k: results[k]["pass_at_85"])
        best_performance = results[best_variant]["pass_at_85"]
        
        if best_performance > results.get("baseline", {}).get("pass_at_85", 0):
            improvement = best_performance - results.get("baseline", {}).get("pass_at_85", 0)
            suggestions.append({
                "type": "prompt:best-variant",
                "description": f"{best_variant}„Å´„Çà„Çä{improvement:.1%}„ÅÆÊîπÂñÑ",
                "priority": "high" if improvement > 0.1 else "medium"
            })
        
        # Ë§áÂêàË™û‰øùË≠∑„ÅÆÂäπÊûú„ÉÅ„Çß„ÉÉ„ÇØ
        if "compound_protected" in results:
            compound_perf = results["compound_protected"]["pass_at_85"]
            baseline_perf = results.get("baseline", {}).get("pass_at_85", 0)
            if compound_perf > baseline_perf:
                suggestions.append({
                    "type": "prompt:compound-lock", 
                    "description": f"Ë§áÂêàË™û‰øùË≠∑„Åß{compound_perf - baseline_perf:.1%}ÊîπÂñÑ",
                    "priority": "high"
                })
        
        # „Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂº∑Âåñ„ÅÆÂäπÊûú„ÉÅ„Çß„ÉÉ„ÇØ
        if "format_enhanced" in results:
            format_perf = results["format_enhanced"]["pass_at_85"]
            baseline_perf = results.get("baseline", {}).get("pass_at_85", 0)
            if format_perf > baseline_perf:
                suggestions.append({
                    "type": "prompt:format-enhance",
                    "description": f"„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂº∑Âåñ„Åß{format_perf - baseline_perf:.1%}ÊîπÂñÑ",
                    "priority": "medium"
                })
        
        return suggestions

def main():
    """„É°„Ç§„É≥Èñ¢Êï∞"""
    parser = argparse.ArgumentParser(description="Prompt Optimization Experiment")
    parser.add_argument("--budget", type=int, default=20,
                       help="ÂÆüÈ®ì‰∫àÁÆóÔºàAPIÂëº„Å≥Âá∫„ÅóÊï∞„ÅÆÂà∂ÈôêÔºâ")
    parser.add_argument("--metric", choices=["jaccard", "score"], default="jaccard",
                       help="ÊúÄÈÅ©Âåñ„É°„Éà„É™„ÇØ„Çπ")
    parser.add_argument("--out", type=str, default="out/prompt_opt_phase4.json",
                       help="ÁµêÊûúÂá∫Âäõ„Éë„Çπ")
    parser.add_argument("--filter", choices=["MODEL", "PROMPT", "TOKENIZE"], 
                       help="ÁâπÂÆö„ÅÆroot_cause„ÅÆ„Åø„ÉÜ„Çπ„Éà")
    
    args = parser.parse_args()
    
    try:
        optimizer = PromptOptimizer()
        
        # „ÉÜ„Çπ„Éà„Ç±„Éº„ÇπÂèñÂæó
        print(f"üîç Collecting test cases (filter: {args.filter or 'all'})")
        target_cases = optimizer.get_model_cases(args.filter, limit=args.budget)
        
        if not target_cases:
            print(f"‚ùå No cases found for filter: {args.filter}")
            return False
        
        print(f"üìã Found {len(target_cases)} cases (budget: {args.budget})")
        
        # ÊúÄÈÅ©ÂåñÂÆüÈ®ìÂÆüË°å
        print(f"üß™ Running prompt optimization experiment")
        results = optimizer.run_optimization_experiment(target_cases, args.budget)
        
        # ÊîπÂñÑÊèêÊ°àÁîüÊàê
        suggestions = optimizer.generate_improvement_suggestions(results)
        
        # „É¨„Éù„Éº„ÉàÁîüÊàê
        report = {
            "experiment": {
                "timestamp": datetime.now().isoformat(),
                "budget": args.budget,
                "metric": args.metric,
                "root_cause_filter": args.filter,
                "total_cases": len(target_cases)
            },
            "results": results,
            "improvement_suggestions": suggestions,
            "summary": {
                "best_variant": max(results.keys(), key=lambda k: results[k]["pass_at_85"]) if results else None,
                "best_pass_at_85": max(r["pass_at_85"] for r in results.values()) if results else 0,
                "baseline_improvement": (max(r["pass_at_85"] for r in results.values()) - 
                                       results.get("baseline", {}).get("pass_at_85", 0)) if results else 0
            }
        }
        
        # ÁµêÊûú‰øùÂ≠ò
        output_path = Path(args.out)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nüìÑ Prompt optimization report saved: {output_path}")
        
        # „Çµ„Éû„É™„ÉºË°®Á§∫
        print(f"\nüìä Optimization Summary:")
        print(f"  Best Variant: {report['summary']['best_variant']}")
        print(f"  Best Pass@0.85: {report['summary']['best_pass_at_85']:.1%}")
        print(f"  Improvement: {report['summary']['baseline_improvement']:+.1%}")
        
        print(f"\nüí° Top Suggestions:")
        for i, suggestion in enumerate(suggestions[:3], 1):
            print(f"  {i}. {suggestion['type']}: {suggestion['description']}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Optimization failed: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)