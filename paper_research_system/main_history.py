#!/usr/bin/env python3
"""
Academic Paper Research Assistant - Search History Manager
è«–æ–‡æ¤œç´¢å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
"""

from services.search_history_db import get_search_history_db
import click
from datetime import datetime, timedelta
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from typing import List, Dict, Any
import logging
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))


console = Console()
logger = logging.getLogger(__name__)


@click.group()
def history_cli():
    """è«–æ–‡æ¤œç´¢å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """


@history_cli.command()
@click.option("--limit", "-n", default=20, help="è¡¨ç¤ºä»¶æ•°")
@click.option(
    "--search-type",
    "-t",
    type=click.Choice(["integrated", "specialized"]),
    help="æ¤œç´¢ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿",
)
@click.option(
    "--domain",
    "-d",
    type=click.Choice(
        [
            "sales_psychology",
            "management_psychology",
            "behavioral_economics",
            "universal_psychology",
        ]
    ),
    help="ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿",
)
@click.option("--days", default=30, help="éå»Næ—¥é–“")
@click.option("--verbose", "-v", is_flag=True, help="è©³ç´°è¡¨ç¤º")
def list(limit: int, search_type: str, domain: str, days: int, verbose: bool):
    """æ¤œç´¢å±¥æ­´ã®ä¸€è¦§è¡¨ç¤º"""
    console.print(
        Panel.fit(
            "ğŸ“š Academic Paper Research Assistant\nğŸ•’ æ¤œç´¢å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ",
            style="bold blue",
        )
    )

    try:
        history_db = get_search_history_db()
        histories = history_db.get_search_history(
            limit=limit, search_type=search_type, domain=domain, days_back=days
        )

        if not histories:
            console.print("âŒ æ¤œç´¢å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        # å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
        _display_history_table(histories, verbose)

        console.print(
            f"\nğŸ’¡ è©³ç´°è¡¨ç¤º: [bold cyan]python3 main_history.py detail <ID>[/bold cyan]"
        )
        console.print(
            f"ğŸ’¡ çµ±è¨ˆè¡¨ç¤º: [bold cyan]python3 main_history.py stats[/bold cyan]"
        )

    except Exception as e:
        logger.error(f"å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        console.print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


@history_cli.command()
@click.argument("history_id", type=int)
def detail(history_id: int):
    """ç‰¹å®šæ¤œç´¢ã®è©³ç´°è¡¨ç¤º"""
    console.print(Panel.fit(f"ğŸ“„ æ¤œç´¢å±¥æ­´è©³ç´° - ID: {history_id}", style="bold green"))

    try:
        history_db = get_search_history_db()

        # å±¥æ­´åŸºæœ¬æƒ…å ±å–å¾—
        histories = history_db.get_search_history(limit=1000)  # å…¨å–å¾—ã—ã¦è©²å½“IDã‚’æ¢ã™
        target_history = None
        for h in histories:
            if h["id"] == history_id:
                target_history = h
                break

        if not target_history:
            console.print(f"âŒ ID {history_id} ã®å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        # æ¤œç´¢çµæœè©³ç´°å–å¾—
        results = history_db.get_search_results(history_id)

        # è©³ç´°è¡¨ç¤º
        _display_history_detail(target_history, results)

    except Exception as e:
        logger.error(f"å±¥æ­´è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        console.print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


@history_cli.command()
@click.option("--days", default=30, help="éå»Næ—¥é–“")
def stats(days: int):
    """æ¤œç´¢çµ±è¨ˆã®è¡¨ç¤º"""
    console.print(Panel.fit(f"ğŸ“Š æ¤œç´¢çµ±è¨ˆ - éå»{days}æ—¥é–“", style="bold magenta"))

    try:
        history_db = get_search_history_db()
        statistics = history_db.get_statistics(days_back=days)

        if not statistics:
            console.print("âŒ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        _display_statistics(statistics)

    except Exception as e:
        logger.error(f"çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        console.print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


@history_cli.command()
@click.option("--days", default=30, help="éå»Næ—¥é–“")
@click.option(
    "--granularity",
    "-g",
    type=click.Choice(["day", "week"]),
    default="day",
    help="é›†è¨ˆå˜ä½",
)
def trends(days: int, granularity: str):
    """æ¤œç´¢ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
    console.print(
        Panel.fit(
            f"ğŸ“ˆ æ¤œç´¢ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ - éå»{days}æ—¥é–“ï¼ˆ{granularity}åˆ¥ï¼‰",
            style="bold cyan",
        )
    )

    try:
        history_db = get_search_history_db()

        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿å–å¾—
        histories = history_db.get_search_history(limit=1000, days_back=days)

        if not histories:
            console.print("âŒ åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        _display_trend_analysis(histories, granularity)

    except Exception as e:
        logger.error(f"ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        console.print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


@history_cli.command()
@click.option("--days", default=30, help="éå»Næ—¥é–“")
def performance(days: int):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
    console.print(
        Panel.fit(f"âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ - éå»{days}æ—¥é–“", style="bold red")
    )

    try:
        history_db = get_search_history_db()
        histories = history_db.get_search_history(limit=1000, days_back=days)

        if not histories:
            console.print("âŒ åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        _display_performance_analysis(histories)

    except Exception as e:
        logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        console.print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


@history_cli.command()
@click.argument("search_query", required=False)
@click.option("--limit", "-n", default=10, help="è¡¨ç¤ºä»¶æ•°")
def search(search_query: str, limit: int):
    """å±¥æ­´å†…æ¤œç´¢"""
    if not search_query:
        console.print("âŒ æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        return

    console.print(Panel.fit(f"ğŸ” å±¥æ­´å†…æ¤œç´¢: '{search_query}'", style="bold yellow"))

    try:
        history_db = get_search_history_db()

        # å…¨å±¥æ­´ã‹ã‚‰æ¤œç´¢
        all_histories = history_db.get_search_history(limit=1000)
        matching_histories = []

        for history in all_histories:
            if search_query.lower() in history["query"].lower():
                matching_histories.append(history)

        if not matching_histories:
            console.print(f"âŒ '{search_query}' ã«é–¢é€£ã™ã‚‹å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        # çµæœè¡¨ç¤º
        limited_results = matching_histories[:limit]
        _display_history_table(limited_results, verbose=False)

        if len(matching_histories) > limit:
            console.print(
                f"\nğŸ’¡ {
                    len(matching_histories) -
                    limit}ä»¶ã®è¿½åŠ çµæœãŒã‚ã‚Šã¾ã™ã€‚--limit ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¡¨ç¤ºæ•°ã‚’å¢—ã‚„ã›ã¾ã™ã€‚"
            )

    except Exception as e:
        logger.error(f"å±¥æ­´æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        console.print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def _display_history_table(histories: List[Dict[str, Any]], verbose: bool = False):
    """å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º"""
    table = Table(title="ğŸ“š æ¤œç´¢å±¥æ­´ä¸€è¦§")

    table.add_column("ID", style="cyan", width=6)
    table.add_column("æ¤œç´¢ã‚¯ã‚¨ãƒª", style="green", width=25)
    table.add_column("ã‚¿ã‚¤ãƒ—", style="blue", width=12)
    table.add_column("ãƒ‰ãƒ¡ã‚¤ãƒ³", style="magenta", width=15)
    table.add_column("çµæœæ•°", style="yellow", width=8)
    table.add_column("å®Ÿè¡Œæ™‚é–“", style="red", width=10)
    table.add_column("å®Ÿè¡Œæ—¥æ™‚", style="white", width=16)

    if verbose:
        table.add_column("å½¢å¼", width=8)
        table.add_column("Obsidian", width=8)

    for history in histories:
        # å®Ÿè¡Œæ™‚é–“ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        exec_time = (
            f"{
            history['execution_time_seconds']:.1f}s"
            if history["execution_time_seconds"]
            else "N/A"
        )

        # æ—¥æ™‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        timestamp = datetime.fromisoformat(history["timestamp"].replace("Z", "+00:00"))
        formatted_time = timestamp.strftime("%m/%d %H:%M")

        # ãƒ‰ãƒ¡ã‚¤ãƒ³åã®çŸ­ç¸®
        domain_short = {
            "sales_psychology": "å–¶æ¥­å¿ƒç†",
            "management_psychology": "ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ",
            "behavioral_economics": "è¡Œå‹•çµŒæ¸ˆ",
            "universal_psychology": "æ±ç”¨å¿ƒç†",
        }.get(history["domain"], history["domain"] or "-")

        # åŸºæœ¬ã‚«ãƒ©ãƒ 
        row = [
            str(history["id"]),
            (
                history["query"][:23] + "..."
                if len(history["query"]) > 23
                else history["query"]
            ),
            history["search_type"],
            domain_short,
            str(history["total_results"]),
            exec_time,
            formatted_time,
        ]

        # è©³ç´°ã‚«ãƒ©ãƒ 
        if verbose:
            row.extend(
                [
                    history["output_format"],
                    "âœ…" if history["saved_to_obsidian"] else "âŒ",
                ]
            )

        table.add_row(*row)

    console.print(table)
    console.print(f"\nğŸ“Š åˆè¨ˆ: {len(histories)}ä»¶")


def _display_history_detail(history: Dict[str, Any], results: List[Dict[str, Any]]):
    """å±¥æ­´è©³ç´°è¡¨ç¤º"""
    # åŸºæœ¬æƒ…å ±ãƒ‘ãƒãƒ«
    info_text = f"""
ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {history['query']}
ğŸ“‹ æ¤œç´¢ã‚¿ã‚¤ãƒ—: {history['search_type']}
ğŸ¯ ãƒ‰ãƒ¡ã‚¤ãƒ³: {history['domain'] or 'N/A'}
ğŸ§  æ€è€ƒãƒ¢ãƒ¼ãƒ‰: {history['thinking_mode'] or 'N/A'}
ğŸ“Š æœ€å¤§çµæœæ•°: {history['max_results']}
ğŸ“„ å‡ºåŠ›å½¢å¼: {history['output_format']}
â±ï¸ å®Ÿè¡Œæ™‚é–“: {history['execution_time_seconds']:.2f}ç§’
ğŸ“ˆ å®Ÿéš›çµæœæ•°: {history['total_results']}
ğŸ—‚ï¸ Obsidianä¿å­˜: {'âœ…' if history['saved_to_obsidian'] else 'âŒ'}
ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {history['timestamp']}
"""

    if history["notes"]:
        info_text += f"\nğŸ“ ãƒãƒ¼ãƒˆ: {history['notes']}"

    console.print(Panel(info_text.strip(), title="ğŸ“‹ æ¤œç´¢æƒ…å ±", border_style="blue"))

    # æ¤œç´¢çµæœè©³ç´°
    if results:
        results_table = Table(title="ğŸ“„ æ¤œç´¢çµæœè©³ç´°")
        results_table.add_column("é †ä½", width=6)
        results_table.add_column("ã‚¿ã‚¤ãƒˆãƒ«", width=35)
        results_table.add_column("å¹´", width=6)
        results_table.add_column("å¼•ç”¨æ•°", width=8)
        results_table.add_column("API", width=12)
        results_table.add_column("ã‚¹ã‚³ã‚¢", width=8)

        for result in results:
            relevance_score = (
                f"{
                result['relevance_score']:.1f}"
                if result["relevance_score"]
                else "N/A"
            )
            title_truncated = (
                result["title"][:33] + "..."
                if len(result["title"]) > 33
                else result["title"]
            )

            results_table.add_row(
                str(result["rank_position"]),
                title_truncated,
                (
                    str(result["publication_year"])
                    if result["publication_year"]
                    else "N/A"
                ),
                str(result["citation_count"]) if result["citation_count"] else "0",
                result["api_source"],
                relevance_score,
            )

        console.print(results_table)
    else:
        console.print("âŒ æ¤œç´¢çµæœã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


def _display_statistics(stats: Dict[str, Any]):
    """çµ±è¨ˆè¡¨ç¤º"""
    basic = stats.get("basic", {})

    # åŸºæœ¬çµ±è¨ˆ
    basic_panel = f"""
ğŸ“Š ç·æ¤œç´¢å›æ•°: {basic.get('total_searches', 0)}ä»¶
ğŸ” ãƒ¦ãƒ‹ãƒ¼ã‚¯æ¤œç´¢: {basic.get('unique_queries', 0)}ä»¶
ğŸ“ˆ å¹³å‡çµæœæ•°: {basic.get('avg_results', 0):.1f}ä»¶/å›
â±ï¸ å¹³å‡å®Ÿè¡Œæ™‚é–“: {basic.get('avg_execution_time', 0):.1f}ç§’
ğŸ—‚ï¸ Obsidianä¿å­˜: {basic.get('saved_to_obsidian_count', 0)}ä»¶
"""

    console.print(Panel(basic_panel.strip(), title="ğŸ“Š åŸºæœ¬çµ±è¨ˆ", border_style="green"))

    # äººæ°—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    popular_keywords = stats.get("popular_keywords", [])
    if popular_keywords:
        keywords_table = Table(title="ğŸ”¥ äººæ°—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ TOP10")
        keywords_table.add_column("é †ä½", width=6)
        keywords_table.add_column("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", width=30)
        keywords_table.add_column("æ¤œç´¢å›æ•°", width=10)

        for i, keyword in enumerate(popular_keywords, 1):
            keywords_table.add_row(
                str(i), keyword["keyword"], str(keyword["search_count"])
            )

        console.print(keywords_table)

    # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥çµ±è¨ˆ
    domain_stats = stats.get("domain_stats", [])
    if domain_stats:
        domain_table = Table(title="ğŸ¯ ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥æ¤œç´¢")
        domain_table.add_column("ãƒ‰ãƒ¡ã‚¤ãƒ³", width=20)
        domain_table.add_column("æ¤œç´¢å›æ•°", width=10)

        for domain in domain_stats:
            domain_name = {
                "sales_psychology": "å–¶æ¥­å¿ƒç†å­¦",
                "management_psychology": "ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆå¿ƒç†å­¦",
                "behavioral_economics": "è¡Œå‹•çµŒæ¸ˆå­¦",
                "universal_psychology": "æ±ç”¨å¿ƒç†å­¦",
            }.get(domain["domain"], domain["domain"])

            domain_table.add_row(domain_name, str(domain["count"]))

        console.print(domain_table)

    # APIä½¿ç”¨çµ±è¨ˆ
    api_stats = stats.get("api_stats", [])
    if api_stats:
        api_table = Table(title="ğŸ”Œ APIä½¿ç”¨çµ±è¨ˆ")
        api_table.add_column("API", width=15)
        api_table.add_column("ä½¿ç”¨å›æ•°", width=10)

        for api in api_stats:
            api_table.add_row(api["api_source"].title(), str(api["count"]))

        console.print(api_table)


def _display_trend_analysis(histories: List[Dict[str, Any]], granularity: str):
    """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æè¡¨ç¤º"""
    from collections import defaultdict, Counter

    # æ—¥ä»˜åˆ¥é›†è¨ˆ
    date_counts = defaultdict(int)
    date_exec_times = defaultdict(list)
    date_domains = defaultdict(Counter)

    for history in histories:
        timestamp = datetime.fromisoformat(history["timestamp"].replace("Z", "+00:00"))

        if granularity == "week":
            # é€±ã®é–‹å§‹æ—¥ï¼ˆæœˆæ›œæ—¥ï¼‰ã‚’è¨ˆç®—
            days_since_monday = timestamp.weekday()
            week_start = timestamp - timedelta(days=days_since_monday)
            date_key = week_start.strftime("%Y-%m-%d")
        else:
            date_key = timestamp.strftime("%Y-%m-%d")

        date_counts[date_key] += 1

        if history["execution_time_seconds"]:
            date_exec_times[date_key].append(history["execution_time_seconds"])

        if history["domain"]:
            date_domains[date_key][history["domain"]] += 1

    # æ—¥åˆ¥æ¤œç´¢å›æ•°ãƒ†ãƒ¼ãƒ–ãƒ«
    if date_counts:
        trend_table = Table(title=f"ğŸ“… {granularity}åˆ¥æ¤œç´¢ãƒˆãƒ¬ãƒ³ãƒ‰")
        trend_table.add_column("æ—¥ä»˜", width=12)
        trend_table.add_column("æ¤œç´¢å›æ•°", width=10)
        trend_table.add_column("å¹³å‡å®Ÿè¡Œæ™‚é–“", width=12)
        trend_table.add_column("ä¸»è¦ãƒ‰ãƒ¡ã‚¤ãƒ³", width=15)

        sorted_dates = sorted(date_counts.keys(), reverse=True)[:14]  # æœ€æ–°14æ—¥/é€±

        for date_key in sorted_dates:
            count = date_counts[date_key]

            # å¹³å‡å®Ÿè¡Œæ™‚é–“
            exec_times = date_exec_times[date_key]
            avg_time = (
                f"{
                sum(exec_times) /
                len(exec_times):.1f}s"
                if exec_times
                else "N/A"
            )

            # ä¸»è¦ãƒ‰ãƒ¡ã‚¤ãƒ³
            domain_counter = date_domains[date_key]
            top_domain = (
                domain_counter.most_common(1)[0][0] if domain_counter else "N/A"
            )
            domain_display = {
                "sales_psychology": "å–¶æ¥­å¿ƒç†",
                "management_psychology": "ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ",
                "behavioral_economics": "è¡Œå‹•çµŒæ¸ˆ",
                "universal_psychology": "æ±ç”¨å¿ƒç†",
            }.get(top_domain, top_domain)

            trend_table.add_row(date_key, str(count), avg_time, domain_display)

        console.print(trend_table)

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
    keyword_analysis = _analyze_keywords([h["query"] for h in histories])
    if keyword_analysis:
        keyword_table = Table(title="ğŸ”¤ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
        keyword_table.add_column("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", width=20)
        keyword_table.add_column("å‡ºç¾å›æ•°", width=10)
        keyword_table.add_column("é–¢é€£èª", width=30)

        for keyword, data in keyword_analysis.items():
            related = ", ".join(data["related"][:3])  # ä¸Šä½3ã¤ã®é–¢é€£èª
            keyword_table.add_row(keyword, str(data["count"]), related)

        console.print(keyword_table)


def _display_performance_analysis(histories: List[Dict[str, Any]]):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æè¡¨ç¤º"""
    from statistics import mean, median

    # å®Ÿè¡Œæ™‚é–“åˆ†æ
    exec_times = [
        h["execution_time_seconds"] for h in histories if h["execution_time_seconds"]
    ]
    result_counts = [h["total_results"] for h in histories]

    if exec_times:
        perf_stats = f"""
âš¡ å®Ÿè¡Œæ™‚é–“çµ±è¨ˆ:
  â€¢ å¹³å‡: {mean(exec_times):.2f}ç§’
  â€¢ ä¸­å¤®å€¤: {median(exec_times):.2f}ç§’
  â€¢ æœ€é€Ÿ: {min(exec_times):.2f}ç§’
  â€¢ æœ€é…: {max(exec_times):.2f}ç§’

ğŸ“Š çµæœæ•°çµ±è¨ˆ:
  â€¢ å¹³å‡çµæœæ•°: {mean(result_counts):.1f}ä»¶
  â€¢ æœ€å¤§çµæœæ•°: {max(result_counts)}ä»¶
  â€¢ æœ€å°çµæœæ•°: {min(result_counts)}ä»¶
"""

        console.print(
            Panel(
                perf_stats.strip(), title="ğŸ“Š åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", border_style="yellow"
            )
        )

    # æ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    type_performance = {}
    for history in histories:
        search_type = history["search_type"]
        if search_type not in type_performance:
            type_performance[search_type] = {"times": [], "results": []}

        if history["execution_time_seconds"]:
            type_performance[search_type]["times"].append(
                history["execution_time_seconds"]
            )
        type_performance[search_type]["results"].append(history["total_results"])

    if type_performance:
        type_table = Table(title="ğŸ” æ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
        type_table.add_column("æ¤œç´¢ã‚¿ã‚¤ãƒ—", width=15)
        type_table.add_column("å¹³å‡å®Ÿè¡Œæ™‚é–“", width=12)
        type_table.add_column("å¹³å‡çµæœæ•°", width=10)
        type_table.add_column("æ¤œç´¢å›æ•°", width=8)

        for search_type, data in type_performance.items():
            avg_time = (
                f"{
                mean(
                    data['times']):.2f}s"
                if data["times"]
                else "N/A"
            )
            avg_results = (
                f"{
                mean(
                    data['results']):.1f}"
                if data["results"]
                else "N/A"
            )

            type_table.add_row(
                search_type, avg_time, avg_results, str(len(data["results"]))
            )

        console.print(type_table)

    # æ™‚é–“å¸¯åˆ¥åˆ†æ
    hour_counts = {}
    for history in histories:
        timestamp = datetime.fromisoformat(history["timestamp"].replace("Z", "+00:00"))
        hour = timestamp.hour
        hour_counts[hour] = hour_counts.get(hour, 0) + 1

    if hour_counts:
        # æœ€ã‚‚ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ™‚é–“å¸¯ã‚’ç‰¹å®š
        peak_hours = sorted(hour_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        peak_lines = [
            f"ğŸ• æ¤œç´¢æ™‚é–“å¸¯åˆ†æ:",
            f"  â€¢ æœ€ã‚‚ã‚¢ã‚¯ãƒ†ã‚£ãƒ–: {peak_hours[0][0]:02d}:00 ({peak_hours[0][1]}å›)",
        ]

        if len(peak_hours) > 1:
            peak_lines.append(
                f"  â€¢ 2ç•ªç›®: {
                    peak_hours[1][0]:02d}:00 ({
                    peak_hours[1][1]}å›)"
            )
        if len(peak_hours) > 2:
            peak_lines.append(
                f"  â€¢ 3ç•ªç›®: {
                    peak_hours[2][0]:02d}:00 ({
                    peak_hours[2][1]}å›)"
            )

        peak_text = "\n".join(peak_lines)

        console.print(
            Panel(peak_text.strip(), title="ğŸ•’ æ™‚é–“å¸¯åˆ†æ", border_style="green")
        )


def _analyze_keywords(queries: List[str]) -> Dict[str, Dict]:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ"""
    import re
    from collections import Counter, defaultdict

    # ã™ã¹ã¦ã®ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    all_words = []
    word_cooccurrence = defaultdict(Counter)

    for query in queries:
        # è‹±èªã¨æ—¥æœ¬èªã®å˜èªã‚’æŠ½å‡º
        words = re.findall(r"\b[a-zA-Z]{3,}\b|[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]{2,}", query.lower())
        all_words.extend(words)

        # å…±èµ·é–¢ä¿‚ã‚’è¨˜éŒ²
        for i, word1 in enumerate(words):
            for word2 in words[i + 1 :]:
                word_cooccurrence[word1][word2] += 1
                word_cooccurrence[word2][word1] += 1

    # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ3å›ä»¥ä¸Šå‡ºç¾ï¼‰
    word_counts = Counter(all_words)
    frequent_words = {word: count for word, count in word_counts.items() if count >= 2}

    # é–¢é€£èªã®ç‰¹å®š
    result = {}
    for word, count in frequent_words.items():
        related_words = word_cooccurrence[word].most_common(5)
        result[word] = {"count": count, "related": [w for w, _ in related_words]}

    # é »åº¦é †ã§ã‚½ãƒ¼ãƒˆ
    return dict(sorted(result.items(), key=lambda x: x[1]["count"], reverse=True)[:10])


if __name__ == "__main__":
    history_cli()
