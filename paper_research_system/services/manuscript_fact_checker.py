#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“ åŸç¨¿äº‹å®Ÿç¢ºèªãƒ»æ·»å‰Šã‚·ã‚¹ãƒ†ãƒ 
YouTubeåŸç¨¿ã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºã¨ä¿¡é ¼æ€§å‘ä¸Š

Features:
- åŸç¨¿ã‹ã‚‰ä¸»å¼µãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ç ”ç©¶åã‚’è‡ªå‹•æŠ½å‡º
- è«–æ–‡æ¤œç´¢ã«ã‚ˆã‚‹äº‹å®Ÿç¢ºèª
- ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º
- ä»£æ›¿ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹æ¤œç´¢
- æ–‡ä½“ä¿æŒãƒªãƒ©ã‚¤ã‚¿ãƒ¼
"""

import re
import json
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import datetime
from dataclasses import dataclass
import asyncio

from services.safe_rate_limited_search_service import get_safe_rate_limited_search_service
from services.obsidian_paper_saver import ObsidianPaperSaver


@dataclass
class ExtractedClaim:
    """æŠ½å‡ºã•ã‚ŒãŸä¸»å¼µ"""
    claim_type: str  # "ç ”ç©¶çµæœ", "çµ±è¨ˆ", "å®šç¾©", "ä¸»å¼µ"
    content: str
    researcher_name: Optional[str] = None
    publication_year: Optional[str] = None
    statistic_value: Optional[str] = None
    confidence_level: str = "medium"  # high, medium, low


@dataclass
class FactCheckResult:
    """äº‹å®Ÿç¢ºèªçµæœ"""
    original_claim: ExtractedClaim
    is_hallucination: bool
    evidence_papers: List[Any]
    alternative_evidence: List[Any]
    verification_score: float  # 0-1
    recommendation: str


class ManuscriptFactChecker:
    """åŸç¨¿äº‹å®Ÿç¢ºèªã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.search_service = get_safe_rate_limited_search_service()
        self.obsidian_saver = ObsidianPaperSaver()
        
        # ç ”ç©¶è€…åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè‹±èªé‡è¦–ï¼‰
        self.researcher_patterns = [
            r'According to ([A-Za-z]+(?:\s+[A-Za-z]+)*)',
            r'([A-Za-z]+)\s+and\s+([A-Za-z]+)',
            r"([A-Za-z]+(?:\s+and\s+[A-Za-z]+)*)'s\s+\d{4}\s+(?:study|research|paper)",
        ]
        
        # å¹´ä»£ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè‹±èªé‡è¦–ï¼‰
        self.year_patterns = [
            r'(\d{4})\s*study',
            r'(\d{4})\s*research',
            r'(\d{4})å¹´',
        ]
        
        # çµ±è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³
        self.statistic_patterns = [
            r'(\d+(?:\.\d+)?(?:ã€œ|ï½|-)?\d*(?:\.\d+)?)(?:%|ï¼…|ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ)',
            r'(\d+(?:\.\d+)?(?:å€|å›|ä»¶|äºº|ç¤¾))',
            r'ç´„?(\d+(?:\.\d+)?(?:ã€œ|ï½|-)?\d*(?:\.\d+)?)(?:%|ï¼…|ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ)',
        ]
        
        # ä¸»å¼µãƒ‘ã‚¿ãƒ¼ãƒ³
        self.claim_patterns = [
            r'([^ã€‚]*?)(?:ã“ã¨ãŒ|ã¨ã„ã†|ãã†ã§ã™|ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™|ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã—ãŸ)',
            r'([^ã€‚]*?)(?:ã‚’ç¤ºã—ã¦ã„ã¾ã™|ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸ|ã“ã¨ãŒåˆ¤æ˜)',
            r'([^ã€‚]*?)(?:ç ”ç©¶ã§ã¯|èª¿æŸ»ã§ã¯|ã«ã‚ˆã‚‹ã¨)',
        ]

    def extract_claims_from_manuscript(self, manuscript: str) -> List[ExtractedClaim]:
        """åŸç¨¿ã‹ã‚‰ä¸»å¼µã‚’æŠ½å‡º"""
        print(f"ğŸ“‹ åŸç¨¿åˆ†æé–‹å§‹: '{manuscript}'")
        claims = []
        sentences = self._split_into_sentences(manuscript)
        print(f"ğŸ“‹ æ–‡æ•°: {len(sentences)}")
        
        for sentence in sentences:
            print(f"\nğŸ“‹ æ–‡å‡¦ç†: '{sentence}'")
            # ç ”ç©¶è€…åã‚’æŠ½å‡º
            researchers = self._extract_researchers(sentence)
            print(f"  ç ”ç©¶è€…: {researchers}")
            
            # å¹´ä»£ã‚’æŠ½å‡º
            years = self._extract_years(sentence)
            print(f"  å¹´ä»£: {years}")
            
            # çµ±è¨ˆã‚’æŠ½å‡º
            statistics = self._extract_statistics(sentence)
            
            # ä¸»å¼µã‚’æŠ½å‡º
            claim_content = self._extract_claim_content(sentence)
            
            if claim_content or researchers or statistics:
                claim = ExtractedClaim(
                    claim_type=self._determine_claim_type(sentence),
                    content=sentence,
                    researcher_name=researchers[0] if researchers else None,
                    publication_year=years[0] if years else None,
                    statistic_value=statistics[0] if statistics else None,
                    confidence_level=self._assess_confidence(sentence)
                )
                claims.append(claim)
        
        return claims

    def fact_check_claims(self, claims: List[ExtractedClaim]) -> List[FactCheckResult]:
        """ä¸»å¼µã®äº‹å®Ÿç¢ºèª"""
        results = []
        
        for claim in claims:
            # è«–æ–‡æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
            search_queries = self._generate_search_queries(claim)
            
            # å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢
            all_evidence = []
            search_successful = False
            
            for query in search_queries:
                try:
                    papers = asyncio.run(
                        self.search_service.search_papers(query, max_results=5)
                    )
                    all_evidence.extend(papers)
                    search_successful = True
                except Exception as e:
                    print(f"âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            
            # æ¤œç´¢ãŒå®Œå…¨ã«å¤±æ•—ã—ãŸå ´åˆã¯å¼·åˆ¶çš„ã«ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š
            if not search_successful:
                print(f"ğŸš¨ å…¨ã¦ã®æ¤œç´¢ãŒå¤±æ•—: {claim.content[:50]}... â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š")
                all_evidence = []
            
            # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š
            is_hallucination = self._detect_hallucination(claim, all_evidence)
            
            # ä»£æ›¿ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹æ¤œç´¢
            alternative_evidence = []
            if is_hallucination or len(all_evidence) < 2:
                alternative_evidence = self._search_alternative_evidence(claim)
            
            # æ¤œè¨¼ã‚¹ã‚³ã‚¢è¨ˆç®—
            verification_score = self._calculate_verification_score(claim, all_evidence)
            
            # æ¨å¥¨äº‹é …ç”Ÿæˆ
            recommendation = self._generate_recommendation(claim, all_evidence, is_hallucination)
            
            result = FactCheckResult(
                original_claim=claim,
                is_hallucination=is_hallucination,
                evidence_papers=all_evidence,
                alternative_evidence=alternative_evidence,
                verification_score=verification_score,
                recommendation=recommendation
            )
            results.append(result)
        
        return results

    def generate_corrected_manuscript(self, 
                                    original_manuscript: str,
                                    fact_check_results: List[FactCheckResult]) -> str:
        """ä¿®æ­£ç‰ˆåŸç¨¿ã‚’ç”Ÿæˆ"""
        corrected_manuscript = original_manuscript
        
        # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç®‡æ‰€ã‚’ä¿®æ­£
        for result in fact_check_results:
            if result.is_hallucination:
                # ä»£æ›¿ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã§ç½®ãæ›ãˆ
                replacement = self._create_evidence_based_replacement(result)
                corrected_manuscript = self._replace_maintaining_style(
                    corrected_manuscript,
                    result.original_claim.content,
                    replacement
                )
            elif result.verification_score < 0.5:
                # ä¿¡é ¼æ€§ã‚’å¼·åŒ–
                enhancement = self._enhance_credibility(result)
                corrected_manuscript = self._enhance_text_maintaining_style(
                    corrected_manuscript,
                    result.original_claim.content,
                    enhancement
                )
        
        # YouTubeæœ€é©åŒ–
        corrected_manuscript = self._optimize_for_youtube(corrected_manuscript)
        
        return corrected_manuscript

    def _split_into_sentences(self, text: str) -> List[str]:
        """æ–‡ç« ã‚’æ–‡å˜ä½ã«åˆ†å‰²"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        return [s.strip() for s in sentences if s.strip()]

    def _extract_researchers(self, sentence: str) -> List[str]:
        """ç ”ç©¶è€…åã‚’æŠ½å‡º"""
        print(f"ğŸ” ç ”ç©¶è€…åæŠ½å‡º: '{sentence}'")
        researchers = []
        
        for i, pattern in enumerate(self.researcher_patterns):
            matches = re.findall(pattern, sentence)
            print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}: {matches}")
            
            if matches:
                if isinstance(matches[0], tuple):
                    # ã‚¿ãƒ—ãƒ«: Smith and Johnson â†’ ('Smith', 'Johnson')
                    for match in matches:
                        combined_name = " ".join([name.strip() for name in match if name.strip()])
                        if combined_name:
                            researchers.append(combined_name)
                else:
                    # æ–‡å­—åˆ—: According to Smith and Johnson â†’ 'Smith and Johnson'
                    researchers.extend([m.strip() for m in matches if m.strip()])
        
        unique = list(set(researchers))
        print(f"ğŸ” æœ€çµ‚çµæœ: {unique}")
        return unique

    def _extract_years(self, sentence: str) -> List[str]:
        """å¹´ä»£ã‚’æŠ½å‡º"""
        print(f"ğŸ” å¹´ä»£æŠ½å‡º: '{sentence}'")
        years = []
        
        for i, pattern in enumerate(self.year_patterns):
            matches = re.findall(pattern, sentence)
            print(f"  å¹´ä»£ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}: {matches}")
            years.extend(matches)
        
        unique = list(set(years))
        print(f"ğŸ” æœ€çµ‚çµæœ: {unique}")
        return unique

    def _extract_statistics(self, sentence: str) -> List[str]:
        """çµ±è¨ˆå€¤ã‚’æŠ½å‡º"""
        statistics = []
        for pattern in self.statistic_patterns:
            matches = re.findall(pattern, sentence)
            statistics.extend(matches)
        return list(set(statistics))

    def _extract_claim_content(self, sentence: str) -> str:
        """ä¸»å¼µå†…å®¹ã‚’æŠ½å‡º"""
        for pattern in self.claim_patterns:
            match = re.search(pattern, sentence)
            if match:
                return match.group(1).strip()
        return sentence

    def _determine_claim_type(self, sentence: str) -> str:
        """ä¸»å¼µã®ç¨®é¡ã‚’åˆ¤å®š"""
        if re.search(r'ç ”ç©¶|è«–æ–‡|èª¿æŸ»', sentence):
            return "ç ”ç©¶çµæœ"
        elif re.search(r'%|ï¼…|ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ|å€|å›', sentence):
            return "çµ±è¨ˆ"
        elif re.search(r'ã¨ã¯|ã¨ã„ã†|å®šç¾©', sentence):
            return "å®šç¾©"
        else:
            return "ä¸»å¼µ"

    def _assess_confidence(self, sentence: str) -> str:
        """ä¿¡é ¼åº¦ã‚’è©•ä¾¡"""
        high_confidence_markers = ['ç ”ç©¶ã«ã‚ˆã‚‹ã¨', 'è«–æ–‡ã§ã¯', 'ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ã¨', 'èª¿æŸ»çµæœ']
        medium_confidence_markers = ['ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™', 'ãã†ã§ã™', 'ã¨ã•ã‚Œã¦ã„ã¾ã™']
        low_confidence_markers = ['ã‚‰ã—ã„', 'ã®ã‚ˆã†ã§ã™', 'æ€ã„ã¾ã™', 'æ„Ÿã˜ã¾ã™']
        
        for marker in high_confidence_markers:
            if marker in sentence:
                return "high"
        for marker in medium_confidence_markers:
            if marker in sentence:
                return "medium"
        for marker in low_confidence_markers:
            if marker in sentence:
                return "low"
        return "medium"

    def _generate_search_queries(self, claim: ExtractedClaim) -> List[str]:
        """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ"""
        queries = []
        
        # å…·ä½“çš„ãªç ”ç©¶è€…åã¨å¹´ä»£ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’æœ€å„ªå…ˆã§å³å¯†ãƒã‚§ãƒƒã‚¯
        if claim.researcher_name and claim.publication_year:
            # ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒª: ç ”ç©¶è€…å + å¹´ä»£
            queries.append(f"{claim.researcher_name} {claim.publication_year}")
            
            # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³: ç ”ç©¶è€…åã®ã¿
            if claim.researcher_name:
                queries.append(claim.researcher_name)
                
            print(f"ğŸ” å…·ä½“çš„ç ”ç©¶ã‚’æ¤œè¨¼: '{claim.researcher_name}' ({claim.publication_year}å¹´)")
            return queries
        
        # ç ”ç©¶è€…åã®ã¿ã®å ´åˆ
        elif claim.researcher_name:
            queries.append(claim.researcher_name)
            print(f"ğŸ” ç ”ç©¶è€…ã‚’æ¤œè¨¼: '{claim.researcher_name}'")
            return queries
        
        # å¹´ä»£ã®ã¿ã®å ´åˆã¯å†…å®¹ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã‚‚è¿½åŠ 
        elif claim.publication_year:
            print(f"ğŸ” å¹´ä»£æŒ‡å®šã‚ã‚Š: {claim.publication_year}å¹´")
            
        # å†…å®¹ãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªï¼ˆå…·ä½“çš„ãªç ”ç©¶ä¸»å¼µãŒãªã„å ´åˆã®ã¿ï¼‰
        if "å–¶æ¥­" in claim.content:
            queries.append("sales performance personality traits")
        if "å¤–å‘æ€§" in claim.content:
            queries.append("extroversion sales performance")
        if "èª å®Ÿæ€§" in claim.content:
            queries.append("conscientiousness job performance")
        if "éºä¼" in claim.content or "ï¼…" in claim.content:
            queries.append("personality heritability genetics")
        
        # ä¸€èˆ¬çš„ãªã‚¯ã‚¨ãƒª
        keywords = re.findall(r'[A-Za-z]+', claim.content)
        if keywords:
            queries.append(" ".join(keywords[:3]))
        
        if not queries:
            print(f"âš ï¸ ã‚¯ã‚¨ãƒªç”Ÿæˆå¤±æ•—: {claim.content[:50]}...")
        
        return list(set(queries))

    def _detect_hallucination(self, claim: ExtractedClaim, evidence: List[Any]) -> bool:
        """ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º"""
        if not evidence:
            print(f"ğŸ“‹ ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹0ä»¶: {claim.content[:50]}... â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³")
            return True
        
        # å…·ä½“çš„ãªç ”ç©¶è€…åã¨å¹´ä»£ãŒä¸¡æ–¹ä¸»å¼µã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å³å¯†ãƒã‚§ãƒƒã‚¯
        if claim.researcher_name and claim.publication_year:
            matching_papers = []
            
            for paper in evidence:
                # ç ”ç©¶è€…åãƒã‚§ãƒƒã‚¯
                author_match = False
                if hasattr(paper, 'authors') and paper.authors:
                    for author in paper.authors:
                        if claim.researcher_name.lower() in author.name.lower():
                            author_match = True
                            break
                
                # å¹´ä»£ãƒã‚§ãƒƒã‚¯ï¼ˆÂ±1å¹´ã®è¨±å®¹ç¯„å›²ï¼‰
                year_match = False
                if hasattr(paper, 'publication_year') and paper.publication_year:
                    try:
                        if abs(int(paper.publication_year) - int(claim.publication_year)) <= 1:
                            year_match = True
                    except (ValueError, TypeError):
                        pass
                
                # ä¸¡æ–¹ãƒãƒƒãƒã™ã‚‹è«–æ–‡ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if author_match and year_match:
                    matching_papers.append(paper)
            
            if not matching_papers:
                print(f"ğŸ“‹ ç ”ç©¶è€…'{claim.researcher_name}'å¹´ä»£'{claim.publication_year}'ãŒä¸€è‡´ã™ã‚‹è«–æ–‡ãªã— â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³")
                return True
            else:
                print(f"âœ… ç ”ç©¶è€…'{claim.researcher_name}'å¹´ä»£'{claim.publication_year}'ã®è«–æ–‡ã‚’ç¢ºèª: {len(matching_papers)}ä»¶")
                return False
        
        # ç ”ç©¶è€…åã®ã¿ã®å ´åˆ
        if claim.researcher_name:
            found_researcher = False
            for paper in evidence:
                if hasattr(paper, 'authors') and paper.authors:
                    for author in paper.authors:
                        if claim.researcher_name.lower() in author.name.lower():
                            found_researcher = True
                            break
            if not found_researcher:
                print(f"ğŸ“‹ ç ”ç©¶è€…'{claim.researcher_name}'ãŒè¦‹ã¤ã‹ã‚‰ãªã„ â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³")
                return True
        
        # å¹´ä»£ã®ã¿ã®å ´åˆ  
        if claim.publication_year:
            found_year = False
            for paper in evidence:
                if hasattr(paper, 'publication_year') and paper.publication_year:
                    try:
                        if abs(int(paper.publication_year) - int(claim.publication_year)) <= 2:
                            found_year = True
                            break
                    except (ValueError, TypeError):
                        pass
            if not found_year:
                print(f"ğŸ“‹ å¹´ä»£'{claim.publication_year}'ã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚‰ãªã„ â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³") 
                return True
        
        print(f"âœ… åŸºæœ¬çš„ãªäº‹å®Ÿç¢ºèªã‚’ã‚¯ãƒªã‚¢")
        return False

    def _search_alternative_evidence(self, claim: ExtractedClaim) -> List[Any]:
        """ä»£æ›¿ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚’æ¤œç´¢"""
        alternative_queries = []
        
        if "å–¶æ¥­" in claim.content:
            alternative_queries.extend([
                "sales personality Big Five performance",
                "extroversion conscientiousness sales success",
                "personality traits job performance meta-analysis"
            ])
        
        if "éºä¼" in claim.content:
            alternative_queries.extend([
                "personality heritability twin studies",
                "Big Five genetics behavioral genetics"
            ])
        
        all_alternatives = []
        for query in alternative_queries:
            try:
                papers = asyncio.run(
                    self.search_service.search_papers(query, max_results=3)
                )
                all_alternatives.extend(papers)
            except Exception as e:
                print(f"âš ï¸ ä»£æ›¿æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return all_alternatives

    def _calculate_verification_score(self, claim: ExtractedClaim, evidence: List[Any]) -> float:
        """æ¤œè¨¼ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if not evidence:
            return 0.0
        
        score = 0.0
        total_citations = sum(getattr(paper, 'citation_count', 0) or 0 for paper in evidence)
        
        # å¼•ç”¨æ•°ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        if total_citations > 1000:
            score += 0.4
        elif total_citations > 100:
            score += 0.3
        elif total_citations > 10:
            score += 0.2
        
        # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹æ•°ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        if len(evidence) >= 3:
            score += 0.3
        elif len(evidence) >= 2:
            score += 0.2
        elif len(evidence) >= 1:
            score += 0.1
        
        # å¹´ä»£ã®æ–°ã—ã•
        recent_papers = [p for p in evidence if hasattr(p, 'publication_year') 
                        and p.publication_year and int(p.publication_year) >= 2010]
        if recent_papers:
            score += 0.3
        
        return min(score, 1.0)

    def _generate_recommendation(self, claim: ExtractedClaim, evidence: List[Any], is_hallucination: bool) -> str:
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        if is_hallucination:
            return "ä»£æ›¿ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã§å®Œå…¨ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã‚’æ¨å¥¨"
        elif len(evidence) == 0:
            return "ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ä¸è¶³ - å‰Šé™¤ã¾ãŸã¯ä»£æ›¿è¡¨ç¾ã‚’æ¨å¥¨"
        elif len(evidence) < 2:
            return "ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹è£œå¼·ã‚’æ¨å¥¨"
        else:
            return "ä¿¡é ¼æ€§ååˆ† - å¼•ç”¨æƒ…å ±ã®è¿½åŠ ã‚’æ¨å¥¨"

    def _create_evidence_based_replacement(self, result: FactCheckResult) -> str:
        """ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ã®ç½®æ›æ–‡ã‚’ä½œæˆ"""
        if not result.alternative_evidence:
            return "ã€è¦æ¤œè¨¼ã€‘" + result.original_claim.content
        
        best_paper = max(result.alternative_evidence, 
                        key=lambda p: getattr(p, 'citation_count', 0) or 0)
        
        # ç ”ç©¶è€…åã‚’æŠ½å‡º
        author_name = "ç ”ç©¶è€…"
        if hasattr(best_paper, 'authors') and best_paper.authors:
            author_name = best_paper.authors[0].name.split()[-1]  # å§“ã‚’å–å¾—
        
        # å¹´ä»£ã‚’æŠ½å‡º
        year = "æœ€è¿‘"
        if hasattr(best_paper, 'publication_year') and best_paper.publication_year:
            year = f"{best_paper.publication_year}å¹´"
        
        # å¼•ç”¨æ•°æƒ…å ±
        citation_info = ""
        if hasattr(best_paper, 'citation_count') and best_paper.citation_count:
            if best_paper.citation_count > 1000:
                citation_info = f"ï¼ˆ{best_paper.citation_count:,}å›ä»¥ä¸Šå¼•ç”¨ã•ã‚Œã¦ã„ã‚‹æ¨©å¨çš„ç ”ç©¶ï¼‰"
        
        # å…ƒã®ä¸»å¼µã®æ ¸å¿ƒçš„ãªå†…å®¹ã‚’æŠ½å‡º
        original_content = result.original_claim.content
        
        # ä¸»å¼µã®ä¸»è¦éƒ¨åˆ†ã‚’ä¿æŒ
        core_claim = self._extract_core_claim(original_content)
        
        # èªå°¾ã‚‚ä¿æŒ
        ending = self._extract_ending(original_content)
        
        replacement = f"{author_name}ã®{year}ã®ç ”ç©¶{citation_info}ã«ã‚ˆã‚‹ã¨ã€{core_claim}{ending}"
        return replacement

    def _extract_core_claim(self, text: str) -> str:
        """æ–‡ç« ã‹ã‚‰æ ¸å¿ƒçš„ãªä¸»å¼µã‚’æŠ½å‡º"""
        # ç ”ç©¶è€…åã¨å¹´ä»£ã‚’é™¤å»
        cleaned = re.sub(r'[A-Za-z]+(?:\s+[A-Za-z]+)*?(?:ã•ã‚“|æ°|åšå£«|æ•™æˆ|ç ”ç©¶è€…)?(?:é”|ã‚‰|ç­‰)?ã®', '', text)
        cleaned = re.sub(r'\d{4}å¹´ã®ç ”ç©¶ã«ã‚ˆã‚‹ã¨ã€?', '', cleaned)
        cleaned = re.sub(r'ã«ã‚ˆã‚‹ã¨ã€?', '', cleaned)
        
        # èªå°¾ã‚’é™¤å»
        cleaned = re.sub(r'(?:ãã†ã§ã™|ã§ã™|ã¾ã™|ã§ã‚ã‚‹|ã |ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™)$', '', cleaned)
        
        # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
        cleaned = cleaned.strip()
        
        return cleaned

    def _extract_ending(self, text: str) -> str:
        """æ–‡ç« ã‹ã‚‰èªå°¾ã‚’æŠ½å‡º"""
        ending_match = re.search(r'(ãã†ã§ã™|ã§ã™|ã¾ã™|ã§ã‚ã‚‹|ã |ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™)$', text)
        if ending_match:
            return ending_match.group(1)
        return "ãã†ã§ã™"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    def _replace_maintaining_style(self, text: str, original: str, replacement: str) -> str:
        """æ–‡ä½“ã‚’ç¶­æŒã—ãªãŒã‚‰ç½®æ›"""
        return text.replace(original, replacement)

    def _enhance_credibility(self, result: FactCheckResult) -> str:
        """ä¿¡é ¼æ€§å¼·åŒ–æƒ…å ±ã‚’ç”Ÿæˆ"""
        if not result.evidence_papers:
            return ""
        
        best_paper = max(result.evidence_papers, 
                        key=lambda p: getattr(p, 'citation_count', 0) or 0)
        
        enhancement = ""
        if hasattr(best_paper, 'citation_count') and best_paper.citation_count:
            if best_paper.citation_count > 1000:
                enhancement = f"ï¼ˆ{best_paper.citation_count:,}å›å¼•ç”¨ã®æ¨©å¨çš„ç ”ç©¶ï¼‰"
            elif best_paper.citation_count > 100:
                enhancement = f"ï¼ˆ{best_paper.citation_count}å›å¼•ç”¨ï¼‰"
        
        return enhancement

    def _enhance_text_maintaining_style(self, text: str, original: str, enhancement: str) -> str:
        """æ–‡ä½“ã‚’ç¶­æŒã—ãªãŒã‚‰æƒ…å ±ã‚’å¼·åŒ–"""
        if not enhancement:
            return text
        
        # æ–‡ã®çµ‚ã‚ã‚Šã«æŒ¿å…¥
        enhanced = original + enhancement
        return text.replace(original, enhanced)

    def _optimize_for_youtube(self, text: str) -> str:
        """YouTubeæœ€é©åŒ–"""
        # è¦–è´è€…ã®é–¢å¿ƒã‚’å¼•ãè¡¨ç¾ã«å¤‰æ›
        optimizations = {
            r'ç ”ç©¶ã«ã‚ˆã‚‹ã¨': 'ãªã‚“ã¨ã€æ¨©å¨çš„ãªç ”ç©¶ã«ã‚ˆã‚‹ã¨',
            r'ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã—ãŸ': 'ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã‚“ã§ã™',
            r'ç¤ºã—ã¦ã„ã¾ã™': 'å®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ã‚“ã§ã™',
            r'é‡è¦ãªã®ãŒ': 'ç‰¹ã«é‡è¦ãªãƒã‚¤ãƒ³ãƒˆãŒ',
            r'å•é¡Œã¯': 'ã“ã“ã§é‡è¦ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œã¯',
        }
        
        optimized_text = text
        for pattern, replacement in optimizations.items():
            optimized_text = re.sub(pattern, replacement, optimized_text)
        
        return optimized_text

    def run_full_fact_check(self, manuscript: str) -> Dict[str, Any]:
        """å®Œå…¨ãªäº‹å®Ÿç¢ºèªãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ"""
        print("ğŸ” åŸç¨¿äº‹å®Ÿç¢ºèªã‚’é–‹å§‹...")
        
        # 1. ä¸»å¼µæŠ½å‡º
        print("ğŸ“ ä¸»å¼µã‚’æŠ½å‡ºä¸­...")
        claims = self.extract_claims_from_manuscript(manuscript)
        print(f"âœ… {len(claims)}å€‹ã®ä¸»å¼µã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        
        # 2. äº‹å®Ÿç¢ºèª
        print("ğŸ” äº‹å®Ÿç¢ºèªã‚’å®Ÿè¡Œä¸­...")
        fact_check_results = self.fact_check_claims(claims)
        
        # 3. ä¿®æ­£ç‰ˆç”Ÿæˆ
        print("âœï¸ ä¿®æ­£ç‰ˆåŸç¨¿ã‚’ç”Ÿæˆä¸­...")
        corrected_manuscript = self.generate_corrected_manuscript(manuscript, fact_check_results)
        
        # 4. çµæœã‚’Obsidianã«ä¿å­˜
        self._save_results_to_obsidian(manuscript, fact_check_results, corrected_manuscript)
        
        # 5. çµæœã‚µãƒãƒªãƒ¼
        hallucination_count = sum(1 for r in fact_check_results if r.is_hallucination)
        low_confidence_count = sum(1 for r in fact_check_results if r.verification_score < 0.5)
        
        return {
            "original_manuscript": manuscript,
            "corrected_manuscript": corrected_manuscript,
            "total_claims": len(claims),
            "hallucination_count": hallucination_count,
            "low_confidence_count": low_confidence_count,
            "fact_check_results": fact_check_results,
            "improvement_summary": self._generate_improvement_summary(fact_check_results)
        }

    def _save_results_to_obsidian(self, original: str, results: List[FactCheckResult], corrected: str):
        """çµæœã‚’Obsidianã«ä¿å­˜"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        content = f"""# åŸç¨¿äº‹å®Ÿç¢ºèªçµæœ - {timestamp}

## ğŸ“ å…ƒã®åŸç¨¿
{original}

## âœ… ä¿®æ­£ç‰ˆåŸç¨¿
{corrected}

## ğŸ” äº‹å®Ÿç¢ºèªè©³ç´°
"""
        
        for i, result in enumerate(results, 1):
            content += f"""
### {i}. ä¸»å¼µåˆ†æ
- **å…ƒã®ä¸»å¼µ**: {result.original_claim.content}
- **ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³**: {'âŒ ã‚ã‚Š' if result.is_hallucination else 'âœ… ãªã—'}
- **æ¤œè¨¼ã‚¹ã‚³ã‚¢**: {result.verification_score:.2f}
- **æ¨å¥¨äº‹é …**: {result.recommendation}
- **ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹æ•°**: {len(result.evidence_papers)}å€‹

"""

        filename = f"åŸç¨¿äº‹å®Ÿç¢ºèª_{timestamp}.md"
        file_path = self.obsidian_saver.vault_path / "fact-check-reports" / filename
        file_path.parent.mkdir(exist_ok=True)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“ çµæœã‚’Obsidianã«ä¿å­˜: {filename}")

    def _generate_improvement_summary(self, results: List[FactCheckResult]) -> str:
        """æ”¹å–„ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        hallucinations = [r for r in results if r.is_hallucination]
        low_confidence = [r for r in results if r.verification_score < 0.5]
        
        summary = f"""
ğŸ¯ åŸç¨¿æ”¹å–„ã‚µãƒãƒªãƒ¼:
- ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ä¿®æ­£: {len(hallucinations)}ç®‡æ‰€
- ä¿¡é ¼æ€§å¼·åŒ–: {len(low_confidence)}ç®‡æ‰€
- å…¨ä½“çš„ãªä¿¡é ¼æ€§å‘ä¸Šåº¦: {(1 - len(hallucinations) / max(len(results), 1)) * 100:.0f}%
"""
        return summary
    def _check_flexible_author_match(self, claim_researcher: str, paper) -> bool:
        """æŸ”è»Ÿãªç ”ç©¶è€…åãƒãƒƒãƒãƒ³ã‚°"""
        if not hasattr(paper, 'authors') or not paper.authors:
            return False
        
        # ç ”ç©¶è€…åã‚’åˆ†å‰²ï¼ˆ"Barrick and Mount" â†’ ["Barrick", "Mount"]ï¼‰
        researcher_parts = self._split_researcher_name(claim_researcher)
        paper_authors = [author.name.lower() for author in paper.authors]
        
        print(f"  ğŸ” ãƒãƒƒãƒãƒ³ã‚°: {researcher_parts} vs {paper_authors}")
        
        # å…¨ã¦ã®å§“ãŒè«–æ–‡ã®è‘—è€…ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        matches = []
        for part in researcher_parts:
            part_found = False
            for author_name in paper_authors:
                if part.lower() in author_name:
                    matches.append(f"{part} â†’ {author_name}")
                    part_found = True
                    break
            if not part_found:
                print(f"  âŒ æœªç™ºè¦‹: {part}")
                return False
        
        print(f"  âœ… å…¨ãƒãƒƒãƒ: {matches}")
        return True

    def _split_researcher_name(self, researcher_name: str) -> List[str]:
        """ç ”ç©¶è€…åã‚’åˆ†å‰²"""
        # "Smith and Johnson" â†’ ["Smith", "Johnson"]
        # "According to Smith" â†’ ["Smith"]
        parts = []
        
        # " and "ã§åˆ†å‰²
        if " and " in researcher_name:
            parts = [part.strip() for part in researcher_name.split(" and ")]
        else:
            # å˜ä¸€åã®å ´åˆã€æœ€å¾Œã®å˜èªã‚’å§“ã¨ã—ã¦ä½¿ç”¨
            words = researcher_name.strip().split()
            if words:
                parts = [words[-1]]  # æœ€å¾Œã®å˜èªã‚’å§“ã¨ã™ã‚‹
        
        return [part for part in parts if part and len(part) > 2]  # 2æ–‡å­—ä»¥ä¸Šã®æœ‰åŠ¹ãªå§“ã®ã¿

    
# æ–°ã—ã„fact_check_claimsãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆä¿®æ­£ç‰ˆï¼‰
def fact_check_claims(self, claims: List[ExtractedClaim]) -> List[FactCheckResult]:
    """ä¸»å¼µã®äº‹å®Ÿç¢ºèªï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    print("ğŸ”§ ä¿®æ­£ç‰ˆfact_check_claimså®Ÿè¡Œé–‹å§‹")
    results = []
    
    for claim in claims:
        print(f"ğŸ”§ ã‚¯ãƒ¬ãƒ¼ãƒ å‡¦ç†: {claim.researcher_name} ({claim.publication_year})")
        # è«–æ–‡æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        search_queries = self._generate_search_queries(claim)
        print(f"ğŸ” ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {search_queries}")
        
        # å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢
        all_evidence = []
        search_successful = False
        
        print(f"ğŸ”§ æ¤œç´¢ãƒ«ãƒ¼ãƒ—é–‹å§‹: {len(search_queries)}å€‹ã®ã‚¯ã‚¨ãƒª")
        for i, query in enumerate(search_queries):
            try:
                print(f"ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒªå®Ÿè¡Œ [{i+1}/{len(search_queries)}]: \"{query}\"")
                papers = asyncio.run(
                    self.search_service.search_papers(query, max_results=5)
                )
                print(f"  â†’ {len(papers)}ä»¶ã®è«–æ–‡å–å¾—")
                all_evidence.extend(papers)
                search_successful = True
            except Exception as e:
                print(f"âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ”§ æ¤œç´¢ãƒ«ãƒ¼ãƒ—å®Œäº†")
        
        # æ¤œç´¢ãŒå®Œå…¨ã«å¤±æ•—ã—ãŸå ´åˆã¯å¼·åˆ¶çš„ã«ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š
        if not search_successful:
            print(f"ğŸš¨ å…¨ã¦ã®æ¤œç´¢ãŒå¤±æ•—: {claim.content[:50]}... â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š")
            all_evidence = []
        else:
            print(f"âœ… æ¤œç´¢æˆåŠŸ: {len(all_evidence)}ä»¶ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹å–å¾—")
        
        # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š
        print(f"ğŸ” ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®šé–‹å§‹: ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹æ•°={len(all_evidence)}")
        is_hallucination = self._detect_hallucination(claim, all_evidence)
        print(f"ï¿½ï¿½ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®šçµæœ: {is_hallucination}")
        
        # ä»£æ›¿ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹æ¤œç´¢
        alternative_evidence = []
        if is_hallucination or len(all_evidence) < 2:
            alternative_evidence = self._search_alternative_evidence(claim)
        
        # æ¤œè¨¼ã‚¹ã‚³ã‚¢è¨ˆç®—
        verification_score = self._calculate_verification_score(claim, all_evidence)
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendation = self._generate_recommendation(claim, all_evidence, is_hallucination)
        
        result = FactCheckResult(
            original_claim=claim,
            is_hallucination=is_hallucination,
            evidence_papers=all_evidence,
            alternative_evidence=alternative_evidence,
            verification_score=verification_score,
            recommendation=recommendation
        )
        results.append(result)
    
    return results

# ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä¸Šæ›¸ã
ManuscriptFactChecker.fact_check_claims = fact_check_claims


# å®Œå…¨æ–°è¦ã®ç ”ç©¶è€…åæŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰
def new_extract_researchers(self, sentence):
    """æ–°ã—ã„ç ”ç©¶è€…åæŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰"""
    print(f"ğŸ”§ æ–°è¦æŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè¡Œ: '{sentence}'")
    researchers = []
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: According to X

# === æœ€é©åŒ–ã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã«å†è¿½åŠ ï¼‰ ===

def new_extract_researchers(self, sentence):
    """æœ€é©åŒ–ã•ã‚ŒãŸç ”ç©¶è€…åæŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰"""
    import re
    print(f"ğŸ”§ æ–°è¦æŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè¡Œ: '{sentence}'")
    researchers = []
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: According to X
    pattern1 = r'According to ([A-Za-z]+(?:\s+[A-Za-z]+)*)'
    matches1 = re.findall(pattern1, sentence)
    if matches1:
        print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³1çµæœ: {matches1}")
        researchers.extend(matches1)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: X and Y (ç›´æ¥å‡¦ç†)
    pattern2 = r'([A-Za-z]+)\s+and\s+([A-Za-z]+)'
    matches2 = re.findall(pattern2, sentence)
    if matches2:
        print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³2çµæœ: {matches2}")
        for match in matches2:
            combined = f"{match[0]} and {match[1]}"
            researchers.append(combined)
            print(f"  ã‚¿ãƒ—ãƒ«çµåˆ: {match} â†’ '{combined}'")
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: X's study
    pattern3 = r"([A-Za-z]+(?:\s+and\s+[A-Za-z]+)*)'s\s+\d{4}\s+(?:study|research|paper)"
    matches3 = re.findall(pattern3, sentence)
    if matches3:
        print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³3çµæœ: {matches3}")
        researchers.extend(matches3)
    
    # é‡è¤‡é™¤å»ã—ã€andã‚’å«ã‚€ã‚‚ã®ã‚’å…ˆé ­ã«
    unique = list(set(researchers))
    and_first = [r for r in unique if " and " in r] + [r for r in unique if " and " not in r]
    
    print(f"ğŸ”§ æ–°è¦ãƒ¡ã‚½ãƒƒãƒ‰çµæœ: {and_first}")
    return and_first

def new_detect_hallucination(self, claim, evidence):
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ¡ã‚½ãƒƒãƒ‰"""
    print(f"ğŸ”§ æ–°è¦ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¤å®š: '{claim.researcher_name}' ({claim.publication_year})")
    
    if not evidence:
        print(f"ğŸ“‹ ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹0ä»¶ â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³")
        return True
    
    print(f"ğŸ“‹ ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹{len(evidence)}ä»¶ã§æ¤œè¨¼é–‹å§‹")
    
    # ç ”ç©¶è€…åã¨å¹´ä»£ã®ä¸¡æ–¹ãŒã‚ã‚‹å ´åˆ
    if claim.researcher_name and claim.publication_year:
        matching_papers = []
        
        for i, paper in enumerate(evidence):
            print(f"ğŸ” è«–æ–‡{i+1}ãƒã‚§ãƒƒã‚¯: {paper.title[:40]}...")
            
            # è‘—è€…ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            author_match = False
            if " and " in claim.researcher_name:
                parts = [p.strip() for p in claim.researcher_name.split(" and ")]
                print(f"  åå‰åˆ†å‰²: {parts}")
                
                all_parts_found = True
                matches = []
                for part in parts:
                    part_found = False
                    for author in paper.authors:
                        if part.lower() in author.name.lower():
                            matches.append(f"{part} â†’ {author.name}")
                            part_found = True
                            break
                    if not part_found:
                        all_parts_found = False
                        break
                
                author_match = all_parts_found
                if matches:
                    print(f"  ãƒãƒƒãƒãƒ³ã‚°: {matches}")
            else:
                # å˜ä¸€åã®å ´åˆ
                for author in paper.authors:
                    if claim.researcher_name.lower() in author.name.lower():
                        author_match = True
                        print(f"  ãƒãƒƒãƒãƒ³ã‚°: {claim.researcher_name} â†’ {author.name}")
                        break
            
            print(f"  â†’ è‘—è€…ãƒãƒƒãƒ: {author_match}")
            
            # å¹´ä»£ãƒã‚§ãƒƒã‚¯ï¼ˆÂ±1å¹´ã®è¨±å®¹ç¯„å›²ï¼‰
            year_match = False
            if hasattr(paper, 'publication_year') and paper.publication_year:
                try:
                    year_diff = abs(int(paper.publication_year) - int(claim.publication_year))
                    year_match = year_diff <= 1
                    print(f"  â†’ å¹´ä»£ãƒãƒƒãƒ: {year_match} ({paper.publication_year} vs {claim.publication_year})")
                except (ValueError, TypeError):
                    print(f"  â†’ å¹´ä»£ã‚¨ãƒ©ãƒ¼")
                    pass
            
            # ä¸¡æ–¹ãƒãƒƒãƒã™ã‚‹è«–æ–‡ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if author_match and year_match:
                matching_papers.append(paper)
                print(f"  âœ… å®Œå…¨ãƒãƒƒãƒ!")
        
        if not matching_papers:
            print(f"ğŸ“‹ å®Œå…¨ãƒãƒƒãƒã™ã‚‹è«–æ–‡ãªã— â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³")
            return True
        else:
            print(f"âœ… å®Œå…¨ãƒãƒƒãƒã™ã‚‹è«–æ–‡ç™ºè¦‹: {len(matching_papers)}ä»¶ â†’ å®Ÿåœ¨ç ”ç©¶")
            return False
    
    print(f"âœ… åŸºæœ¬ãƒã‚§ãƒƒã‚¯é€šé")
    return False

# ãƒ¡ã‚½ãƒƒãƒ‰ç½®ãæ›ãˆ
ManuscriptFactChecker._extract_researchers = new_extract_researchers
ManuscriptFactChecker._detect_hallucination = new_detect_hallucination
