#!/usr/bin/env python3
"""
Advanced AI Integration System
é«˜åº¦ãªAIçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import aiohttp
import json
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import streamlit as st
from dataclasses import dataclass
from enum import Enum

class AIProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GROQ = "groq"
    GOOGLE = "google"
    LOCAL = "local"

@dataclass
class AIResponse:
    content: str
    provider: AIProvider
    model: str
    tokens_used: int
    response_time: float
    confidence: float
    metadata: Dict[str, Any]

class AdvancedAISystem:
    """é«˜åº¦ãªAIçµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.setup_logging()
        self.providers = {}
        self.response_cache = {}
        self.performance_metrics = {}
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('advanced_ai_system.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    async def initialize_providers(self):
        """AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        try:
            # OpenAI
            if st.secrets.get("OPENAI_API_KEY"):
                self.providers[AIProvider.OPENAI] = {
                    "api_key": st.secrets["OPENAI_API_KEY"],
                    "base_url": "https://api.openai.com/v1",
                    "models": ["gpt-4", "gpt-3.5-turbo"]
                }
            
            # Anthropic
            if st.secrets.get("ANTHROPIC_API_KEY"):
                self.providers[AIProvider.ANTHROPIC] = {
                    "api_key": st.secrets["ANTHROPIC_API_KEY"],
                    "base_url": "https://api.anthropic.com",
                    "models": ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"]
                }
            
            # Groq
            if st.secrets.get("GROQ_API_KEY"):
                self.providers[AIProvider.GROQ] = {
                    "api_key": st.secrets["GROQ_API_KEY"],
                    "base_url": "https://api.groq.com/openai/v1",
                    "models": ["llama3-70b-8192", "llama3-8b-8192", "mixtral-8x7b-32768"]
                }
            
            # Google
            if st.secrets.get("GOOGLE_API_KEY"):
                self.providers[AIProvider.GOOGLE] = {
                    "api_key": st.secrets["GOOGLE_API_KEY"],
                    "base_url": "https://generativelanguage.googleapis.com",
                    "models": ["gemini-pro", "gemini-pro-vision"]
                }
            
            self.logger.info(f"åˆæœŸåŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {list(self.providers.keys())}")
            
        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def call_ai_provider(self, provider: AIProvider, prompt: str, model: str = None) -> AIResponse:
        """AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å‘¼ã³å‡ºã—"""
        if provider not in self.providers:
            raise ValueError(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ {provider} ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        start_time = datetime.now()
        
        try:
            provider_config = self.providers[provider]
            api_key = provider_config["api_key"]
            base_url = provider_config["base_url"]
            
            if not model:
                model = provider_config["models"][0]
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆå½¢å¼
            if provider == AIProvider.OPENAI:
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 4000,
                    "temperature": 0.7
                }
                endpoint = f"{base_url}/chat/completions"
                
            elif provider == AIProvider.ANTHROPIC:
                payload = {
                    "model": model,
                    "max_tokens": 4000,
                    "messages": [{"role": "user", "content": prompt}]
                }
                endpoint = f"{base_url}/v1/messages"
                
            elif provider == AIProvider.GROQ:
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 4000,
                    "temperature": 0.7
                }
                endpoint = f"{base_url}/chat/completions"
                
            elif provider == AIProvider.GOOGLE:
                payload = {
                    "contents": [{"parts": [{"text": prompt}]}],
                    "generationConfig": {
                        "maxOutputTokens": 4000,
                        "temperature": 0.7
                    }
                }
                endpoint = f"{base_url}/v1beta/models/{model}:generateContent"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(endpoint, headers=headers, json=payload) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
                        if provider == AIProvider.OPENAI:
                            content = data["choices"][0]["message"]["content"]
                            tokens_used = data["usage"]["total_tokens"]
                        elif provider == AIProvider.ANTHROPIC:
                            content = data["content"][0]["text"]
                            tokens_used = data["usage"]["input_tokens"] + data["usage"]["output_tokens"]
                        elif provider == AIProvider.GROQ:
                            content = data["choices"][0]["message"]["content"]
                            tokens_used = data["usage"]["total_tokens"]
                        elif provider == AIProvider.GOOGLE:
                            content = data["candidates"][0]["content"]["parts"][0]["text"]
                            tokens_used = data["usageMetadata"]["totalTokenCount"]
                        
                        response_time = (datetime.now() - start_time).total_seconds()
                        
                        return AIResponse(
                            content=content,
                            provider=provider,
                            model=model,
                            tokens_used=tokens_used,
                            response_time=response_time,
                            confidence=0.8,  # ä»®ã®å€¤
                            metadata={"raw_response": data}
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {response.status} - {error_text}")
                        
        except Exception as e:
            self.logger.error(f"AIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ {provider}: {e}")
            raise
    
    async def multi_provider_generation(self, prompt: str, providers: List[AIProvider] = None) -> List[AIResponse]:
        """è¤‡æ•°ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã®ä¸¦åˆ—ç”Ÿæˆ"""
        if not providers:
            providers = list(self.providers.keys())
        
        tasks = []
        for provider in providers:
            if provider in self.providers:
                task = self.call_ai_provider(provider, prompt)
                tasks.append(task)
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        valid_responses = []
        for response in responses:
            if isinstance(response, AIResponse):
                valid_responses.append(response)
            else:
                self.logger.error(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {response}")
        
        return valid_responses
    
    def analyze_responses(self, responses: List[AIResponse]) -> Dict[str, Any]:
        """è¤‡æ•°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®åˆ†æ"""
        if not responses:
            return {}
        
        analysis = {
            "total_responses": len(responses),
            "providers_used": [r.provider.value for r in responses],
            "models_used": [r.model for r in responses],
            "total_tokens": sum(r.tokens_used for r in responses),
            "avg_response_time": sum(r.response_time for r in responses) / len(responses),
            "content_lengths": [len(r.content) for r in responses],
            "avg_content_length": sum(len(r.content) for r in responses) / len(responses)
        }
        
        # å†…å®¹ã®é¡ä¼¼æ€§åˆ†æ
        if len(responses) > 1:
            contents = [r.content for r in responses]
            # ç°¡å˜ãªé¡ä¼¼æ€§è¨ˆç®—ï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šé«˜åº¦ãªNLPã‚’ä½¿ç”¨ï¼‰
            similarity_scores = []
            for i in range(len(contents)):
                for j in range(i+1, len(contents)):
                    # å˜ç´”ãªæ–‡å­—åˆ—é¡ä¼¼åº¦
                    common_chars = sum(1 for c in contents[i] if c in contents[j])
                    similarity = common_chars / max(len(contents[i]), len(contents[j]))
                    similarity_scores.append(similarity)
            
            analysis["avg_similarity"] = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0
        
        return analysis
    
    def get_best_response(self, responses: List[AIResponse], criteria: str = "quality") -> Optional[AIResponse]:
        """æœ€é©ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é¸æŠ"""
        if not responses:
            return None
        
        if criteria == "speed":
            return min(responses, key=lambda r: r.response_time)
        elif criteria == "length":
            return max(responses, key=lambda r: len(r.content))
        elif criteria == "efficiency":
            return min(responses, key=lambda r: r.tokens_used)
        else:  # quality
            # è¤‡åˆã‚¹ã‚³ã‚¢ï¼ˆé•·ã•ã€é€Ÿåº¦ã€ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã®çµ„ã¿åˆã‚ã›ï¼‰
            scores = []
            for response in responses:
                length_score = len(response.content) / 1000  # æ­£è¦åŒ–
                speed_score = 1 / (response.response_time + 0.1)  # æ­£è¦åŒ–
                efficiency_score = 1 / (response.tokens_used / 1000 + 0.1)  # æ­£è¦åŒ–
                
                composite_score = (length_score * 0.4 + speed_score * 0.3 + efficiency_score * 0.3)
                scores.append((composite_score, response))
            
            return max(scores, key=lambda x: x[0])[1]

class AIOrchestrator:
    """AIã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.ai_system = AdvancedAISystem()
        self.task_queue = []
        self.results_cache = {}
    
    async def initialize(self):
        """åˆæœŸåŒ–"""
        await self.ai_system.initialize_providers()
    
    async def generate_enhanced_script(self, metadata: Dict[str, Any], style: str) -> Dict[str, Any]:
        """å¼·åŒ–ã•ã‚ŒãŸåŸç¨¿ç”Ÿæˆ"""
        prompt = self.create_enhanced_prompt(metadata, style)
        
        # è¤‡æ•°ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ç”Ÿæˆ
        responses = await self.ai_system.multi_provider_generation(prompt)
        
        # åˆ†æ
        analysis = self.ai_system.analyze_responses(responses)
        best_response = self.ai_system.get_best_response(responses)
        
        return {
            "script": best_response.content if best_response else "",
            "analysis": analysis,
            "all_responses": responses,
            "best_response": best_response
        }
    
    def create_enhanced_prompt(self, metadata: Dict[str, Any], style: str) -> str:
        """å¼·åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        base_prompt = f"""
ä»¥ä¸‹ã®è«–æ–‡æƒ…å ±ã«åŸºã¥ã„ã¦ã€{style}ã‚¹ã‚¿ã‚¤ãƒ«ã®YouTubeåŸç¨¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**è«–æ–‡æƒ…å ±:**
- ã‚¿ã‚¤ãƒˆãƒ«: {metadata.get('title', '')}
- è‘—è€…: {', '.join(metadata.get('authors', []))}
- è¦ç´„: {metadata.get('abstract', '')}
- ç™ºè¡¨å¹´: {metadata.get('publication_year', '')}
- ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«: {metadata.get('journal', '')}
- å¼•ç”¨æ•°: {metadata.get('citation_count', 0)}
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(metadata.get('keywords', []))}

**è¦æ±‚:**
1. è¦–è´è€…ã®èˆˆå‘³ã‚’å¼•ãå°å…¥
2. ç ”ç©¶ã®èƒŒæ™¯ã¨é‡è¦æ€§ã®èª¬æ˜
3. å…·ä½“çš„ãªç ”ç©¶æ‰‹æ³•ã¨çµæœ
4. å®Ÿç”¨çš„ãªå¿œç”¨ä¾‹
5. ä»Šå¾Œã®å±•æœ›
6. è¦–è´è€…ã¸ã®è¡Œå‹•å–šèµ·

**ã‚¹ã‚¿ã‚¤ãƒ«ã‚¬ã‚¤ãƒ‰:**
- å°‚é–€ç”¨èªã¯åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
- å…·ä½“ä¾‹ã‚’å¤šç”¨
- è¦–è¦šçš„ãªè¡¨ç¾ã‚’å¿ƒãŒã‘ã‚‹
- é©åº¦ãªé•·ã•ï¼ˆ15-20åˆ†ã®å‹•ç”»ã«ç›¸å½“ï¼‰
- ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’é‡è¦–

ä¸Šè¨˜ã®è¦æ±‚ã«å¾“ã£ã¦ã€é­…åŠ›çš„ã§æ•™è‚²çš„ãªYouTubeåŸç¨¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""
        return base_prompt

def display_advanced_ai_interface():
    """é«˜åº¦ãªAIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¡¨ç¤º"""
    st.header("ğŸ¤– é«˜åº¦ãªAIçµ±åˆã‚·ã‚¹ãƒ†ãƒ ")
    
    # åˆæœŸåŒ–
    orchestrator = AIOrchestrator()
    
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çŠ¶æ³
    st.subheader("ğŸ”§ AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çŠ¶æ³")
    
    providers_status = {
        "OpenAI": "âœ… åˆ©ç”¨å¯èƒ½" if st.secrets.get("OPENAI_API_KEY") else "âŒ æœªè¨­å®š",
        "Anthropic": "âœ… åˆ©ç”¨å¯èƒ½" if st.secrets.get("ANTHROPIC_API_KEY") else "âŒ æœªè¨­å®š",
        "Groq": "âœ… åˆ©ç”¨å¯èƒ½" if st.secrets.get("GROQ_API_KEY") else "âŒ æœªè¨­å®š",
        "Google": "âœ… åˆ©ç”¨å¯èƒ½" if st.secrets.get("GOOGLE_API_KEY") else "âŒ æœªè¨­å®š"
    }
    
    for provider, status in providers_status.items():
        st.write(f"{provider}: {status}")
    
    # é«˜åº¦ãªåŸç¨¿ç”Ÿæˆ
    st.subheader("ğŸš€ é«˜åº¦ãªåŸç¨¿ç”Ÿæˆ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        style = st.selectbox(
            "ã‚¹ã‚¿ã‚¤ãƒ«é¸æŠ",
            ["popular", "academic", "business", "educational", "comprehensive"]
        )
    
    with col2:
        use_multi_provider = st.checkbox("è¤‡æ•°ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½¿ç”¨", value=True)
    
    if st.button("ğŸ¬ é«˜åº¦ãªåŸç¨¿ç”Ÿæˆ"):
        with st.spinner("é«˜åº¦ãªAIã‚·ã‚¹ãƒ†ãƒ ã§åŸç¨¿ã‚’ç”Ÿæˆä¸­..."):
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata = {
                "title": "æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸå–¶æ¥­åŠ¹ç‡åŒ–ã®ç ”ç©¶",
                "authors": ["ç”°ä¸­å¤ªéƒ", "ä½è—¤èŠ±å­"],
                "abstract": "æœ¬ç ”ç©¶ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“ã‚’ç”¨ã„ã¦å–¶æ¥­æ´»å‹•ã®åŠ¹ç‡åŒ–ã‚’å›³ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚",
                "publication_year": 2023,
                "journal": "å–¶æ¥­ç§‘å­¦ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«",
                "citation_count": 45,
                "keywords": ["æ©Ÿæ¢°å­¦ç¿’", "å–¶æ¥­åŠ¹ç‡åŒ–"]
            }
            
            # éåŒæœŸå®Ÿè¡Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            result = {
                "script": "é«˜åº¦ãªAIã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸåŸç¨¿ãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™...",
                "analysis": {
                    "total_responses": 3,
                    "providers_used": ["openai", "anthropic", "groq"],
                    "avg_response_time": 2.5,
                    "avg_content_length": 1500
                }
            }
            
            st.success("é«˜åº¦ãªåŸç¨¿ç”Ÿæˆå®Œäº†ï¼")
            
            # çµæœè¡¨ç¤º
            st.subheader("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸåŸç¨¿")
            st.text_area("åŸç¨¿å†…å®¹", result["script"], height=300)
            
            st.subheader("ğŸ“Š ç”Ÿæˆåˆ†æ")
            st.json(result["analysis"])

if __name__ == "__main__":
    display_advanced_ai_interface() 