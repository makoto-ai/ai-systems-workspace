#!/usr/bin/env python3
"""
Normalization Rules Suggestion Tool
æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«å€™è£œè‡ªå‹•ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import yaml
import argparse
import unicodedata
import re
from pathlib import Path
from typing import Dict, List, Set, Any
from collections import defaultdict

class NormRuleSuggester:
    """æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«ææ¡ˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        # åŸºæœ¬çš„ãªè¨˜å·çµ±ä¸€ãƒ«ãƒ¼ãƒ«
        self.punctuation_rules = {
            # æ‹¬å¼§é¡
            "ï¼»": "[", "ã€": "[", "ï¼ˆ": "(", "ã€”": "[",
            "ï¼½": "]", "ã€‘": "]", "ï¼‰": ")", "ã€•": "]",
            # å¥èª­ç‚¹
            "ï¼Œ": ",", "ï¼": ".", "ï¼›": ";", "ï¼š": ":",
            "ï¼Ÿ": "?", "ï¼": "!", "ã€œ": "~", "ï½": "~",
            # é•·éŸ³ãƒ»ãƒ€ãƒƒã‚·ãƒ¥
            "ãƒ¼": "-", "â€•": "-", "â€": "-", "â€“": "-", "â€”": "-",
            # å¼•ç”¨ç¬¦
            """: '"', """: '"', "'": "'", "'": "'",
            "ã€Œ": '"', "ã€": '"', "ã€": '"', "ã€": '"'
        }
        
        # å˜ä½ãƒ»è¡¨è¨˜ã‚†ã‚Œ
        self.unit_rules = {
            "ï¼…": "%", "ãŒ«": "%", "ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ": "%",
            "ãƒŸãƒªç§’": "ms", "ãƒŸãƒªã‚»ã‚«ãƒ³ãƒ‰": "ms", "msec": "ms",
            "ç§’": "s", "ã‚»ã‚«ãƒ³ãƒ‰": "s", "sec": "s",
            "åˆ†": "min", "åˆ†é–“": "min", "minute": "min",
            "æ™‚é–“": "h", "æ™‚": "h", "hour": "h"
        }
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰åŒç¾©èª
        self.domain_synonyms = {
            "å°å…¥": ["å®Ÿè£…", "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"],
            "åˆæ ¼ç‡": ["ãƒ‘ã‚¹ç‡", "æˆåŠŸç‡", "é€šéç‡"],
            "åˆ¤å®š": ["è©•ä¾¡", "ãƒã‚§ãƒƒã‚¯", "æ¤œè¨¼"],
            "è¾æ›¸": ["æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«", "ãƒ«ãƒ¼ãƒ«ã‚»ãƒƒãƒˆ", "ãƒãƒƒãƒ”ãƒ³ã‚°"],
            "å¯è¦–åŒ–": ["è¡¨ç¤º", "æç”»", "ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°"],
            "ç›£è¦–": ["ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°", "è¦³æ¸¬", "è¿½è·¡"],
            "åˆ†æ": ["è§£æ", "èª¿æŸ»", "æ¤œè¨¼"],
            "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰": ["ç®¡ç†ç”»é¢", "ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«", "ç”»é¢"],
            "ãƒ¡ãƒˆãƒªã‚¯ã‚¹": ["æŒ‡æ¨™", "æ¸¬å®šå€¤", "æ•°å€¤"],
            "ã‚·ã‚¹ãƒ†ãƒ ": ["ä»•çµ„ã¿", "æ©Ÿæ§‹", "æ§‹é€ "],
            "ãƒ„ãƒ¼ãƒ«": ["é“å…·", "æ‰‹æ®µ", "æ©Ÿèƒ½"],
            "æ§‹ç¯‰": ["ä½œæˆ", "å»ºè¨­", "è¨­ç½®"]
        }
    
    def analyze_failures(self, failures_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤±æ•—ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãƒ«ãƒ¼ãƒ«å€™è£œã‚’ç”Ÿæˆ"""
        failures = failures_data.get("new_failures", [])
        
        # å„ç¨®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åé›†
        punctuation_issues = []
        number_issues = []
        similar_pairs = []
        synonym_candidates = defaultdict(set)
        
        for failure in failures:
            reference = failure.get("reference", "")
            prediction = failure.get("prediction", "")
            diff_analysis = failure.get("diff_analysis", {})
            
            # è¨˜å·ãƒ»å¥èª­ç‚¹ã®å•é¡Œã‚’æ¤œå‡º
            punct_issues = self._detect_punctuation_issues(reference, prediction)
            punctuation_issues.extend(punct_issues)
            
            # æ•°å€¤ã®å•é¡Œã‚’æ¤œå‡º
            num_issues = self._detect_number_issues(reference, prediction)
            number_issues.extend(num_issues)
            
            # é¡ä¼¼ãƒšã‚¢ã‹ã‚‰åŒç¾©èªå€™è£œã‚’æŠ½å‡º
            pairs = diff_analysis.get("similar_pairs", [])
            for pair in pairs:
                if pair.get("similarity", 0) > 0.7:
                    missing = pair["missing"]
                    extra = pair["extra"]
                    similar_pairs.append(pair)
                    
                    # åŒç¾©èªå€™è£œã¨ã—ã¦ç™»éŒ²
                    if len(missing) > 2 and len(extra) > 2:  # çŸ­ã™ãã‚‹èªã¯é™¤å¤–
                        synonym_candidates[missing].add(extra)
                        synonym_candidates[extra].add(missing)
        
        return {
            "punctuation_issues": punctuation_issues,
            "number_issues": number_issues,
            "similar_pairs": similar_pairs,
            "synonym_candidates": dict(synonym_candidates)
        }
    
    def _detect_punctuation_issues(self, reference: str, prediction: str) -> List[Dict[str, str]]:
        """è¨˜å·ãƒ»å¥èª­ç‚¹ã®å•é¡Œã‚’æ¤œå‡º"""
        issues = []
        
        # å…¨è§’/åŠè§’ã®é•ã„ã‚’æ¤œå‡º
        ref_chars = set(reference)
        pred_chars = set(prediction)
        
        for char in ref_chars | pred_chars:
            # NFKCæ­£è¦åŒ–ã§å¤‰åŒ–ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            normalized = unicodedata.normalize('NFKC', char)
            if char != normalized:
                issues.append({
                    "type": "nfkc_normalization",
                    "original": char,
                    "normalized": normalized,
                    "context": f"'{char}' â†’ '{normalized}'"
                })
        
        return issues
    
    def _detect_number_issues(self, reference: str, prediction: str) -> List[Dict[str, Any]]:
        """æ•°å€¤ã®å•é¡Œã‚’æ¤œå‡º"""
        issues = []
        
        # æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        ref_numbers = re.findall(r'\d+(?:\.\d+)?', reference)
        pred_numbers = re.findall(r'\d+(?:\.\d+)?', prediction)
        
        # æ•°å€¤ã®è¿‘ä¼¼ãƒã‚§ãƒƒã‚¯
        for ref_num_str in ref_numbers:
            ref_num = float(ref_num_str)
            for pred_num_str in pred_numbers:
                pred_num = float(pred_num_str)
                
                # çµ¶å¯¾èª¤å·®ãƒ»ç›¸å¯¾èª¤å·®ãƒã‚§ãƒƒã‚¯
                abs_diff = abs(ref_num - pred_num)
                rel_diff = abs_diff / max(abs(ref_num), abs(pred_num), 1e-10)
                
                if abs_diff <= 1 or rel_diff <= 0.05:  # Â±1 or Â±5%
                    issues.append({
                        "type": "number_approximation",
                        "reference_value": ref_num,
                        "prediction_value": pred_num,
                        "abs_diff": abs_diff,
                        "rel_diff": rel_diff
                    })
        
        return issues
    
    def generate_rules(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æçµæœã‹ã‚‰æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ"""
        rules = {
            "normalization": {
                "nfkc_enabled": True,
                "casefold_enabled": True,
                "space_compression": True,
                "punctuation_mapping": self.punctuation_rules.copy(),
                "unit_mapping": self.unit_rules.copy()
            },
            "similarity": {
                "number_tolerance": {
                    "absolute_threshold": 1.0,
                    "relative_threshold": 0.05
                },
                "token_similarity_threshold": 0.92
            },
            "synonyms": self.domain_synonyms.copy()
        }
        
        # åˆ†æçµæœã‹ã‚‰è¿½åŠ ãƒ«ãƒ¼ãƒ«ã‚’ç”Ÿæˆ
        
        # è¨˜å·å•é¡Œã‹ã‚‰è¿½åŠ ãƒãƒƒãƒ”ãƒ³ã‚°
        for issue in analysis.get("punctuation_issues", []):
            if issue["type"] == "nfkc_normalization":
                rules["normalization"]["punctuation_mapping"][issue["original"]] = issue["normalized"]
        
        # é¡ä¼¼ãƒšã‚¢ã‹ã‚‰åŒç¾©èªè¿½åŠ 
        synonym_candidates = analysis.get("synonym_candidates", {})
        for key, values in synonym_candidates.items():
            if len(values) > 0:
                # æ—¢å­˜ã®åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ ã¾ãŸã¯æ–°è¦ä½œæˆ
                found_group = False
                for existing_key, existing_values in rules["synonyms"].items():
                    if key in existing_values or key == existing_key:
                        existing_values.extend(values)
                        found_group = True
                        break
                
                if not found_group:
                    rules["synonyms"][key] = list(values)
        
        return rules
    
    def save_rules(self, rules: Dict[str, Any], output_path: str):
        """ãƒ«ãƒ¼ãƒ«ã‚’YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # YAMLã‚³ãƒ¡ãƒ³ãƒˆä»˜ãã§ä¿å­˜
        yaml_content = f"""# Golden Test Normalization Rules
# è‡ªå‹•ç”Ÿæˆæ—¥æ™‚: {datetime.now().isoformat()}

# åŸºæœ¬æ­£è¦åŒ–è¨­å®š
normalization:
  nfkc_enabled: {rules['normalization']['nfkc_enabled']}
  casefold_enabled: {rules['normalization']['casefold_enabled']}
  space_compression: {rules['normalization']['space_compression']}
  
  # è¨˜å·ãƒ»å¥èª­ç‚¹çµ±ä¸€
  punctuation_mapping:
"""
        
        for original, normalized in rules['normalization']['punctuation_mapping'].items():
            yaml_content += f'    "{original}": "{normalized}"\n'
        
        yaml_content += f"""
  # å˜ä½ãƒ»è¡¨è¨˜çµ±ä¸€
  unit_mapping:
"""
        
        for original, normalized in rules['normalization']['unit_mapping'].items():
            yaml_content += f'    "{original}": "{normalized}"\n'
        
        yaml_content += f"""

# é¡ä¼¼åº¦åˆ¤å®šè¨­å®š
similarity:
  number_tolerance:
    absolute_threshold: {rules['similarity']['number_tolerance']['absolute_threshold']}
    relative_threshold: {rules['similarity']['number_tolerance']['relative_threshold']}
  token_similarity_threshold: {rules['similarity']['token_similarity_threshold']}

# åŒç¾©èªã‚°ãƒ«ãƒ¼ãƒ—
synonyms:
"""
        
        for key, values in rules['synonyms'].items():
            yaml_content += f'  "{key}":\n'
            for value in values:
                yaml_content += f'    - "{value}"\n'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(yaml_content)
        
        print(f"âœ… æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«å€™è£œã‚’ä¿å­˜: {output_file}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Normalization Rules Suggestion Tool")
    parser.add_argument("--in", dest="input_file", type=str, required=True,
                       help="å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆnew_fails.jsonï¼‰")
    parser.add_argument("--out", type=str, required=True,
                       help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆnorm_rule_candidates.yamlï¼‰")
    
    args = parser.parse_args()
    
    try:
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(args.input_file, 'r', encoding='utf-8') as f:
            failures_data = json.load(f)
        
        print(f"ğŸ“Š æ–°è¦å¤±æ•— {failures_data.get('total_new_failures', 0)} ä»¶ã‚’åˆ†æä¸­...")
        
        # ãƒ«ãƒ¼ãƒ«ææ¡ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        suggester = NormRuleSuggester()
        
        # å¤±æ•—ãƒ‡ãƒ¼ã‚¿åˆ†æ
        analysis = suggester.analyze_failures(failures_data)
        
        print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:")
        print(f"  - è¨˜å·å•é¡Œ: {len(analysis['punctuation_issues'])}ä»¶")
        print(f"  - æ•°å€¤å•é¡Œ: {len(analysis['number_issues'])}ä»¶")
        print(f"  - é¡ä¼¼ãƒšã‚¢: {len(analysis['similar_pairs'])}ä»¶")
        print(f"  - åŒç¾©èªå€™è£œ: {len(analysis['synonym_candidates'])}ã‚°ãƒ«ãƒ¼ãƒ—")
        
        # ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
        rules = suggester.generate_rules(analysis)
        
        # ãƒ«ãƒ¼ãƒ«ä¿å­˜
        suggester.save_rules(rules, args.out)
        
        print(f"âœ… æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«å€™è£œç”Ÿæˆå®Œäº†")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    import sys
    from datetime import datetime
    
    success = main()
    sys.exit(0 if success else 1)
