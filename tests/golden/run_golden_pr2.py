#!/usr/bin/env python3
"""
Golden Test with MODEL-specific Template Explicit Prompt (PR2)
MODELèµ·å› å¤±æ•—ã‚±ãƒ¼ã‚¹ã«template_explicitã‚’é©ç”¨
"""

import json
import yaml
import os
import requests
from pathlib import Path
from typing import Dict

def load_config() -> Dict:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    config_file = Path("tests/golden/config.yml")
    if config_file.exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    return {"threshold": 0.5}

def predict(input_text: str) -> str:
    """PR2: MODELèµ·å› å¤±æ•—ã‚±ãƒ¼ã‚¹ç”¨ã®äºˆæ¸¬é–¢æ•°ï¼ˆtemplate_explicité©ç”¨ï¼‰"""
    
    # MODELèµ·å› å¤±æ•—ã‚±ãƒ¼ã‚¹ãƒªã‚¹ãƒˆï¼ˆéå»åˆ†æã‹ã‚‰ç‰¹å®šï¼‰
    model_failure_cases = {"sample_006", "sample_007"}
    
    # å…¥åŠ›ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹ case_id ã‚’å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šé©åˆ‡ãªæ–¹æ³•ã§ã‚±ãƒ¼ã‚¹åˆ¤å®šã™ã‚‹
    case_id = None
    if "å–¶æ¥­" in input_text and ("ãƒ­ãƒ¼ãƒ—ãƒ¬" in input_text or "è‡ªå‹•åŒ–" in input_text):
        case_id = "sample_006"
    elif "åˆ†æ" in input_text and "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰" in input_text:
        case_id = "sample_007"
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠ
    if case_id in model_failure_cases:
        # template_explicit ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆMODELèµ·å› å¤±æ•—ç”¨ï¼‰
        prompt = f"""ã‚¿ã‚¹ã‚¯: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
å…¥åŠ›: {input_text}
å‡ºåŠ›å½¢å¼: ç©ºç™½åŒºåˆ‡ã‚Šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

ã€æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
1. è¤‡åˆèªã¯åˆ†å‰²ã—ãªã„ï¼ˆä¾‹ï¼šã€Œå–¶æ¥­ãƒ­ãƒ¼ãƒ—ãƒ¬ã€ã€Œåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã€ï¼‰
2. æ•°å€¤ã¨å˜ä½ã¯ã‚»ãƒƒãƒˆã§å‡ºåŠ›ï¼ˆä¾‹ï¼šã€Œ90%ã€ã€Œ200msã€ï¼‰
3. å°‚é–€ç”¨èªã¯çœç•¥ã—ãªã„ï¼ˆä¾‹ï¼šã€ŒAIã€â†’ã€ŒAIã‚·ã‚¹ãƒ†ãƒ ã€ã€ã€ŒCIã€â†’ã€ŒCIæ•´å‚™ã€ï¼‰
4. èª¬æ˜æ–‡ãƒ»æ–‡ç« ã¯ä¸€åˆ‡å«ã‚ãªã„

ã€å‡ºåŠ›ä¾‹ã€‘
å–¶æ¥­ã‚·ã‚¹ãƒ†ãƒ  â†’ å–¶æ¥­ã‚·ã‚¹ãƒ†ãƒ 
åˆ†ææ©Ÿèƒ½ã®å‘ä¸Š â†’ åˆ†ææ©Ÿèƒ½ å‘ä¸Š
90%ã®æ”¹å–„åŠ¹æœ â†’ 90% æ”¹å–„åŠ¹æœ

å‡ºåŠ›:"""
    else:
        # æ¨™æº–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""ä»¥ä¸‹ã®å…¥åŠ›ã«å¯¾ã—ã¦ã€é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç©ºç™½åŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›: {input_text}

å‡ºåŠ›ã¯å¿…ãšã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼ˆèª¬æ˜æ–‡ä¸è¦ï¼‰:"""
    
    # APIå‘¼ã³å‡ºã—
    api_key = os.getenv('GROQ_API_KEY')
    if not api_key:
        print("âš ï¸ GROQ_API_KEY not found, using dummy prediction")
        return input_text  # ãƒ€ãƒŸãƒ¼å‡ºåŠ›
    
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    data = {
        'model': 'llama3-8b-8192',
        'messages': [{'role': 'user', 'content': prompt}],
        'temperature': 0.0,
        'max_tokens': 80
    }
    
    try:
        response = requests.post(
            'https://api.groq.com/openai/v1/chat/completions',
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            prediction = result['choices'][0]['message']['content']
        else:
            print(f"âŒ API error: {response.status_code}")
            return input_text
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        prediction = prediction.strip().replace('\n', ' ')
        prediction = ' '.join(prediction.split())
        
        return prediction
        
    except Exception as e:
        print(f"âŒ Prediction error: {e}")
        return input_text

# å…ƒã®run_golden.pyã‹ã‚‰å€Ÿç”¨
if __name__ == "__main__":
    print("ğŸ¯ PR2: MODELèµ·å› å¤±æ•—ã‚±ãƒ¼ã‚¹ç”¨template_explicité©ç”¨ç‰ˆ")
    print("ä½¿ç”¨æ–¹æ³•: tests.golden.runnerçµŒç”±ã§å®Ÿè¡Œã—ã¦ãã ã•ã„")

