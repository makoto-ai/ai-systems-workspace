#!/usr/bin/env python3
"""
Golden Test Root Cause Analyzer
å¤±æ•—ç†ç”±ã®è‡ªå‹•åˆ†é¡ã¨ã‚¿ã‚°ä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import re
from pathlib import Path
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional

class RootCause(Enum):
    """å¤±æ•—ç†ç”±ã®åˆ†é¡"""
    TOKENIZE = "TOKENIZE"      # ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²å•é¡Œ
    NORMALIZE = "NORMALIZE"    # æ­£è¦åŒ–è¾æ›¸ä¸è¶³
    PROMPT = "PROMPT"          # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆå•é¡Œ
    MODEL = "MODEL"            # ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ä¸å®‰å®š
    DATA_DRIFT = "DATA_DRIFT"  # ãƒ‡ãƒ¼ã‚¿å“è³ªåŠ£åŒ–
    FLAKY = "FLAKY"           # å†ç¾æ€§ã®ãªã„å¶ç™ºçš„å¤±æ•—
    INFRA = "INFRA"           # ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ç’°å¢ƒå•é¡Œ

def analyze_failure_root_cause(case_id: str, reference: str, prediction: str, score: float) -> RootCause:
    """å¤±æ•—ã‚±ãƒ¼ã‚¹ã®æ ¹æœ¬åŸå› ã‚’è‡ªå‹•åˆ†æ"""
    
    # ç©ºã®äºˆæ¸¬ â†’ ã‚¤ãƒ³ãƒ•ãƒ©å•é¡Œã®å¯èƒ½æ€§
    if not prediction.strip():
        return RootCause.INFRA
    
    ref_tokens = set(reference.split())
    pred_tokens = set(prediction.split())
    
    # å®Œå…¨ã«ç•°ãªã‚‹èªå½™ â†’ ãƒ¢ãƒ‡ãƒ«å•é¡Œ
    if len(ref_tokens & pred_tokens) == 0:
        return RootCause.MODEL
    
    # éƒ¨åˆ†ä¸€è‡´ãŒã‚ã‚‹ãŒä½ã‚¹ã‚³ã‚¢ â†’ æ­£è¦åŒ–å•é¡Œã®å¯èƒ½æ€§
    if 0 < score < 0.3:
        # é¡ä¼¼èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for ref_token in ref_tokens:
            for pred_token in pred_tokens:
                if _is_similar_token(ref_token, pred_token):
                    return RootCause.NORMALIZE
        return RootCause.TOKENIZE
    
    # ä¸­ç¨‹åº¦ã®ã‚¹ã‚³ã‚¢ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²å•é¡Œ
    if 0.3 <= score < 0.7:
        return RootCause.TOKENIZE
    
    # é«˜ã‚¹ã‚³ã‚¢ã ãŒä¸åˆæ ¼ â†’ Flakyï¼ˆå¢ƒç•Œç·šä¸Šï¼‰
    if score >= 0.7:
        return RootCause.FLAKY
    
    return RootCause.MODEL

def _is_similar_token(token1: str, token2: str) -> bool:
    """ãƒˆãƒ¼ã‚¯ãƒ³ã®é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯"""
    # éƒ¨åˆ†æ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
    if token1 in token2 or token2 in token1:
        return True
    
    # è‹±æ—¥æ··åœ¨ãƒã‚§ãƒƒã‚¯
    similar_pairs = [
        ("ai", "aiã‚·ã‚¹ãƒ†ãƒ "),
        ("å­¦ç¿’", "å­¦ç¿’æ”¹å–„"),
        ("åˆ†æ", "åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"),
        ("ci", "ciæ•´å‚™"),
    ]
    
    for pair in similar_pairs:
        if (token1.lower() in pair and token2.lower() in pair) or \
           (token2.lower() in pair and token1.lower() in pair):
            return True
    
    return False

def extract_failures_from_log(log_path: Path) -> List[Dict]:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’æŠ½å‡º"""
    failures = []
    
    if not log_path.exists():
        return failures
    
    with open(log_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    data = json.loads(line)
                    if not data.get('passed', True):  # å¤±æ•—ã‚±ãƒ¼ã‚¹ã®ã¿
                        failures.append(data)
                except json.JSONDecodeError:
                    continue
    
    return failures

def analyze_recent_failures() -> Dict[str, List[Dict]]:
    """ç›´è¿‘ã®ãƒ­ã‚°ã‹ã‚‰å¤±æ•—åˆ†æã‚’å®Ÿè¡Œ"""
    logs_dir = Path("tests/golden/logs")
    
    if not logs_dir.exists():
        return {}
    
    # æœ€æ–°2ã¤ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    log_files = sorted(logs_dir.glob("*.jsonl"), key=lambda x: x.stat().st_mtime, reverse=True)[:2]
    
    analysis_results = {}
    
    for log_file in log_files:
        failures = extract_failures_from_log(log_file)
        analyzed_failures = []
        
        for failure in failures:
            root_cause = analyze_failure_root_cause(
                failure.get('id', ''),
                failure.get('reference', ''),
                failure.get('prediction', ''),
                failure.get('score', 0.0)
            )
            
            analyzed_failures.append({
                'case_id': failure.get('id'),
                'root_cause': root_cause.value,
                'score': failure.get('score', 0.0),
                'reference': failure.get('reference', ''),
                'prediction': failure.get('prediction', ''),
                'description': _generate_description(root_cause, failure)
            })
        
        analysis_results[log_file.stem] = analyzed_failures
    
    return analysis_results

def _generate_description(root_cause: RootCause, failure: Dict) -> str:
    """å¤±æ•—ç†ç”±ã®èª¬æ˜æ–‡ç”Ÿæˆ"""
    case_id = failure.get('id', 'unknown')
    
    descriptions = {
        RootCause.TOKENIZE: f"è¤‡åˆèªãƒ»åˆ†å‰²å•é¡Œ",
        RootCause.NORMALIZE: f"æ­£è¦åŒ–è¾æ›¸ä¸è¶³",
        RootCause.PROMPT: f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆå•é¡Œ",
        RootCause.MODEL: f"ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ä¸å®‰å®š",
        RootCause.DATA_DRIFT: f"ãƒ‡ãƒ¼ã‚¿å“è³ªåŠ£åŒ–",
        RootCause.FLAKY: f"å†ç¾æ€§ãªã—ï¼ˆå¢ƒç•Œç·šä¸Šï¼‰",
        RootCause.INFRA: f"ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ç’°å¢ƒå•é¡Œ"
    }
    
    return descriptions.get(root_cause, "ä¸æ˜ãªåŸå› ")

def update_observation_log_with_analysis():
    """è¦³æ¸¬ãƒ­ã‚°ã«åˆ†æçµæœã‚’è¿½è¨˜"""
    analysis_results = analyze_recent_failures()
    
    if not analysis_results:
        print("åˆ†æå¯¾è±¡ã®å¤±æ•—ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print("ğŸ” Root Cause Analysis Results:")
    print("=" * 40)
    
    for log_file, failures in analysis_results.items():
        print(f"\nğŸ“… {log_file}:")
        if not failures:
            print("  âœ… å¤±æ•—ã‚±ãƒ¼ã‚¹ãªã—")
            continue
            
        for failure in failures:
            print(f"  - {failure['case_id']}: `root_cause:{failure['root_cause']}` - {failure['description']}")
    
    # çµ±è¨ˆæƒ…å ±
    all_failures = []
    for failures in analysis_results.values():
        all_failures.extend(failures)
    
    if all_failures:
        root_cause_counts = {}
        for failure in all_failures:
            cause = failure['root_cause']
            root_cause_counts[cause] = root_cause_counts.get(cause, 0) + 1
        
        print(f"\nğŸ“Š Root Cause Distribution:")
        for cause, count in sorted(root_cause_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(all_failures)) * 100
            print(f"  - {cause}: {count}ä»¶ ({percentage:.1f}%)")

if __name__ == "__main__":
    update_observation_log_with_analysis()
