#!/usr/bin/env python3
"""
Golden Test Root Cause Analyzer
å¤±æ•—ç†ç”±ã®è‡ªå‹•åˆ†é¡ã¨ã‚¿ã‚°ä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import re
import argparse
from pathlib import Path
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Set, Any

class RootCause(Enum):
    """å¤±æ•—ç†ç”±ã®åˆ†é¡"""
    TOKENIZE = "TOKENIZE"      # ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²å•é¡Œ
    NORMALIZE = "NORMALIZE"    # æ­£è¦åŒ–è¾æ›¸ä¸è¶³
    PROMPT = "PROMPT"          # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆå•é¡Œ
    MODEL = "MODEL"            # ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ä¸å®‰å®š
    DATA_DRIFT = "DATA_DRIFT"  # ãƒ‡ãƒ¼ã‚¿å“è³ªåŠ£åŒ–
    FLAKY = "FLAKY"           # å†ç¾æ€§ã®ãªã„å¶ç™ºçš„å¤±æ•—
    INFRA = "INFRA"           # ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ç’°å¢ƒå•é¡Œ

class Freshness(Enum):
    """å¤±æ•—ã®æ–°è¦æ€§åˆ†é¡"""
    NEW = "NEW"        # æ–°è¦å¤±æ•—
    REPEAT = "REPEAT"  # å†ç™ºå¤±æ•—

def analyze_failure_root_cause(case_id: str, reference: str, prediction: str, score: float) -> RootCause:
    """å¤±æ•—ã‚±ãƒ¼ã‚¹ã®æ ¹æœ¬åŸå› ã‚’è‡ªå‹•åˆ†æ"""
    
    # ç©ºã®äºˆæ¸¬ â†’ ã‚¤ãƒ³ãƒ•ãƒ©å•é¡Œã®å¯èƒ½æ€§
    if not prediction.strip():
        return RootCause.INFRA
    
    ref_tokens = set(reference.split())
    pred_tokens = set(prediction.split())
    
    # å®Œå…¨ã«ç•°ãªã‚‹èªå½™ â†’ ãƒ¢ãƒ‡ãƒ«å•é¡Œ
    if len(ref_tokens & pred_tokens) == 0:
        return RootCause.MODEL
    
    # éƒ¨åˆ†ä¸€è‡´ãŒã‚ã‚‹ãŒä½ã‚¹ã‚³ã‚¢ â†’ æ­£è¦åŒ–å•é¡Œã®å¯èƒ½æ€§
    if 0 < score < 0.3:
        # é¡ä¼¼èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for ref_token in ref_tokens:
            for pred_token in pred_tokens:
                if _is_similar_token(ref_token, pred_token):
                    return RootCause.NORMALIZE
        return RootCause.TOKENIZE
    
    # ä¸­ç¨‹åº¦ã®ã‚¹ã‚³ã‚¢ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²å•é¡Œ
    if 0.3 <= score < 0.7:
        return RootCause.TOKENIZE
    
    # é«˜ã‚¹ã‚³ã‚¢ã ãŒä¸åˆæ ¼ â†’ Flakyï¼ˆå¢ƒç•Œç·šä¸Šï¼‰
    if score >= 0.7:
        return RootCause.FLAKY
    
    return RootCause.MODEL

def _is_similar_token(token1: str, token2: str) -> bool:
    """ãƒˆãƒ¼ã‚¯ãƒ³ã®é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯"""
    # éƒ¨åˆ†æ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
    if token1 in token2 or token2 in token1:
        return True
    
    # è‹±æ—¥æ··åœ¨ãƒã‚§ãƒƒã‚¯
    similar_pairs = [
        ("ai", "aiã‚·ã‚¹ãƒ†ãƒ "),
        ("å­¦ç¿’", "å­¦ç¿’æ”¹å–„"),
        ("åˆ†æ", "åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"),
        ("ci", "ciæ•´å‚™"),
    ]
    
    for pair in similar_pairs:
        if (token1.lower() in pair and token2.lower() in pair) or \
           (token2.lower() in pair and token1.lower() in pair):
            return True
    
    return False

def extract_failures_from_log(log_path: Path) -> List[Dict]:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’æŠ½å‡º"""
    failures = []
    
    if not log_path.exists():
        return failures
    
    with open(log_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    data = json.loads(line)
                    if not data.get('passed', True):  # å¤±æ•—ã‚±ãƒ¼ã‚¹ã®ã¿
                        failures.append(data)
                except json.JSONDecodeError:
                    continue
    
    return failures

def analyze_recent_failures() -> Dict[str, List[Dict]]:
    """ç›´è¿‘ã®ãƒ­ã‚°ã‹ã‚‰å¤±æ•—åˆ†æã‚’å®Ÿè¡Œ"""
    logs_dir = Path("tests/golden/logs")
    
    if not logs_dir.exists():
        return {}
    
    # æœ€æ–°2ã¤ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    log_files = sorted(logs_dir.glob("*.jsonl"), key=lambda x: x.stat().st_mtime, reverse=True)[:2]
    
    analysis_results = {}
    
    for log_file in log_files:
        failures = extract_failures_from_log(log_file)
        analyzed_failures = []
        
        for failure in failures:
            root_cause = analyze_failure_root_cause(
                failure.get('id', ''),
                failure.get('reference', ''),
                failure.get('prediction', ''),
                failure.get('score', 0.0)
            )
            
            analyzed_failures.append({
                'case_id': failure.get('id'),
                'root_cause': root_cause.value,
                'score': failure.get('score', 0.0),
                'reference': failure.get('reference', ''),
                'prediction': failure.get('prediction', ''),
                'description': _generate_description(root_cause, failure)
            })
        
        analysis_results[log_file.stem] = analyzed_failures
    
    return analysis_results

def _generate_description(root_cause: RootCause, failure: Dict) -> str:
    """å¤±æ•—ç†ç”±ã®èª¬æ˜æ–‡ç”Ÿæˆ"""
    case_id = failure.get('id', 'unknown')
    
    descriptions = {
        RootCause.TOKENIZE: f"è¤‡åˆèªãƒ»åˆ†å‰²å•é¡Œ",
        RootCause.NORMALIZE: f"æ­£è¦åŒ–è¾æ›¸ä¸è¶³",
        RootCause.PROMPT: f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆå•é¡Œ",
        RootCause.MODEL: f"ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ä¸å®‰å®š",
        RootCause.DATA_DRIFT: f"ãƒ‡ãƒ¼ã‚¿å“è³ªåŠ£åŒ–",
        RootCause.FLAKY: f"å†ç¾æ€§ãªã—ï¼ˆå¢ƒç•Œç·šä¸Šï¼‰",
        RootCause.INFRA: f"ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ç’°å¢ƒå•é¡Œ"
    }
    
    return descriptions.get(root_cause, "ä¸æ˜ãªåŸå› ")

def build_failure_history() -> Dict[str, Set[str]]:
    """éå»ã®å¤±æ•—å±¥æ­´ã‚’æ§‹ç¯‰ï¼ˆcase_id -> å¤±æ•—ã—ãŸæ—¥ä»˜ã®ã‚»ãƒƒãƒˆï¼‰"""
    logs_dir = Path("tests/golden/logs")
    failure_history = {}
    
    if not logs_dir.exists():
        return failure_history
    
    # å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ—¥ä»˜é †ã§ã‚¹ã‚­ãƒ£ãƒ³
    log_files = sorted(logs_dir.glob("*.jsonl"), key=lambda x: x.stem)
    
    for log_file in log_files:
        date_str = log_file.stem  # YYYYMMDD_HHMMSSå½¢å¼
        date_part = date_str.split('_')[0]  # YYYYMMDDéƒ¨åˆ†ã‚’å–å¾—
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        if not data.get('passed', True):  # å¤±æ•—ã‚±ãƒ¼ã‚¹
                            case_id = data.get('id', '')
                            if case_id:
                                if case_id not in failure_history:
                                    failure_history[case_id] = set()
                                failure_history[case_id].add(date_part)
                    except json.JSONDecodeError:
                        continue
    
    return failure_history

def infer_freshness(case_id: str, current_date: str, history: Dict[str, Set[str]]) -> Freshness:
    """å¤±æ•—ã®æ–°è¦æ€§ã‚’åˆ¤å®š"""
    if case_id not in history:
        return Freshness.NEW
    
    # å½“æ—¥ä»¥å‰ã«å¤±æ•—å±¥æ­´ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    past_failures = history[case_id]
    for past_date in past_failures:
        if past_date < current_date:  # æ–‡å­—åˆ—æ¯”è¼ƒã§YYYYMMDDé †åº
            return Freshness.REPEAT
    
    return Freshness.NEW

def apply_freshness_to_log(log_path: Path = None):
    """è¦³æ¸¬ãƒ­ã‚°ã«freshnessã‚¿ã‚°ã‚’è¿½è¨˜"""
    if log_path is None:
        log_path = Path("tests/golden/observation_log.md")
    
    if not log_path.exists():
        print(f"è¦³æ¸¬ãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {log_path}")
        return
    
    # å¤±æ•—å±¥æ­´ã‚’æ§‹ç¯‰
    failure_history = build_failure_history()
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(log_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å„å¤±æ•—ã‚±ãƒ¼ã‚¹è¡Œã‚’æ¤œç´¢ã—ã¦freshnessã‚’è¿½åŠ 
    lines = content.split('\n')
    updated_lines = []
    
    for line in lines:
        # å¤±æ•—åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¡Œã‚’æ¤œç´¢
        # å½¢å¼: - **case_id**: `root_cause:XXX` - èª¬æ˜
        match = re.match(r'^- \*\*([^*]+)\*\*: `root_cause:([^`]+)` - (.+)$', line)
        if match:
            case_id = match.group(1)
            root_cause = match.group(2)
            description = match.group(3)
            
            # æ—¥ä»˜ã‚’æ¨å®šï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ï¼‰
            current_date = "20250829"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆå®Ÿéš›ã¯å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—ï¼‰
            
            # freshnessã‚’åˆ¤å®š
            freshness = infer_freshness(case_id, current_date, failure_history)
            
            # freshnessãŒã¾ã å«ã¾ã‚Œã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
            if "freshness:" not in line:
                updated_line = f"- **{case_id}**: `root_cause:{root_cause}` | `freshness:{freshness.value}` - {description}"
                updated_lines.append(updated_line)
            else:
                updated_lines.append(line)
        else:
            updated_lines.append(line)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæˆ»ã—
    updated_content = '\n'.join(updated_lines)
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(updated_content)
    
    print(f"âœ… Freshness tags updated in {log_path}")

def update_observation_log_with_analysis():
    """è¦³æ¸¬ãƒ­ã‚°ã«åˆ†æçµæœã‚’è¿½è¨˜"""
    analysis_results = analyze_recent_failures()
    
    if not analysis_results:
        print("åˆ†æå¯¾è±¡ã®å¤±æ•—ã‚±ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print("ğŸ” Root Cause Analysis Results:")
    print("=" * 40)
    
    for log_file, failures in analysis_results.items():
        print(f"\nğŸ“… {log_file}:")
        if not failures:
            print("  âœ… å¤±æ•—ã‚±ãƒ¼ã‚¹ãªã—")
            continue
            
        for failure in failures:
            print(f"  - {failure['case_id']}: `root_cause:{failure['root_cause']}` - {failure['description']}")
    
    # çµ±è¨ˆæƒ…å ±
    all_failures = []
    for failures in analysis_results.values():
        all_failures.extend(failures)
    
    if all_failures:
        root_cause_counts = {}
        for failure in all_failures:
            cause = failure['root_cause']
            root_cause_counts[cause] = root_cause_counts.get(cause, 0) + 1
        
        print(f"\nğŸ“Š Root Cause Distribution:")
        for cause, count in sorted(root_cause_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(all_failures)) * 100
            print(f"  - {cause}: {count}ä»¶ ({percentage:.1f}%)")

def export_new_failures(output_path: str):
    """æ–°è¦å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’æŠ½å‡ºã—ã¦JSONã§å‡ºåŠ›"""
    logs_dir = Path("tests/golden/logs")
    if not logs_dir.exists():
        print("âŒ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # æœ€æ–°2ã¤ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–°è¦å¤±æ•—ã‚’æŠ½å‡º
    log_files = sorted(logs_dir.glob("*.jsonl"), key=lambda x: x.stat().st_mtime, reverse=True)[:2]
    
    new_failures = []
    failure_history = build_failure_history()
    
    for log_file in log_files:
        date_str = log_file.stem.split('_')[0]
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        if not data.get('passed', True):  # å¤±æ•—ã‚±ãƒ¼ã‚¹
                            case_id = data.get('id', '')
                            reference = data.get('reference', '')
                            prediction = data.get('prediction', '')
                            score = data.get('score', 0.0)
                            
                            # Freshnessåˆ¤å®š
                            freshness = infer_freshness(case_id, date_str, failure_history)
                            
                            if freshness == Freshness.NEW:
                                # Root Causeåˆ†æ
                                root_cause = analyze_failure_root_cause(case_id, reference, prediction, score)
                                
                                new_failures.append({
                                    "case_id": case_id,
                                    "reference": reference,
                                    "prediction": prediction,
                                    "score": score,
                                    "root_cause": root_cause.value,
                                    "date": date_str,
                                    "log_file": log_file.name,
                                    "diff_analysis": analyze_token_diff(reference, prediction)
                                })
                    except json.JSONDecodeError:
                        continue
    
    # çµæœä¿å­˜
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    export_data = {
        "export_timestamp": datetime.now().isoformat(),
        "total_new_failures": len(new_failures),
        "new_failures": new_failures
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ–°è¦å¤±æ•— {len(new_failures)} ä»¶ã‚’å‡ºåŠ›: {output_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    root_cause_counts = {}
    for failure in new_failures:
        cause = failure['root_cause']
        root_cause_counts[cause] = root_cause_counts.get(cause, 0) + 1
    
    print(f"ğŸ“Š Root Causeåˆ†å¸ƒ:")
    for cause, count in sorted(root_cause_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(new_failures)) * 100 if new_failures else 0
        print(f"  - {cause}: {count}ä»¶ ({percentage:.1f}%)")

def analyze_token_diff(reference: str, prediction: str) -> Dict[str, Any]:
    """å‚ç…§ã¨äºˆæ¸¬ã®å·®åˆ†ã‚’è©³ç´°åˆ†æ"""
    ref_tokens = set(reference.split()) if reference else set()
    pred_tokens = set(prediction.split()) if prediction else set()
    
    missing_tokens = ref_tokens - pred_tokens
    extra_tokens = pred_tokens - ref_tokens
    common_tokens = ref_tokens & pred_tokens
    
    # é¡ä¼¼ãƒˆãƒ¼ã‚¯ãƒ³ãƒšã‚¢ã‚’æ¤œå‡º
    similar_pairs = []
    for missing in missing_tokens:
        for extra in extra_tokens:
            if _is_potentially_similar(missing, extra):
                similar_pairs.append({"missing": missing, "extra": extra, "similarity": _calculate_similarity(missing, extra)})
    
    return {
        "missing_tokens": list(missing_tokens),
        "extra_tokens": list(extra_tokens),
        "common_tokens": list(common_tokens),
        "similar_pairs": similar_pairs,
        "jaccard_similarity": len(common_tokens) / len(ref_tokens | pred_tokens) if (ref_tokens | pred_tokens) else 0.0
    }

def _is_potentially_similar(token1: str, token2: str) -> bool:
    """ãƒˆãƒ¼ã‚¯ãƒ³ãŒé¡ä¼¼ã—ã¦ã„ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    # é•·ã•ãŒå¤§ããç•°ãªã‚‹å ´åˆã¯é™¤å¤–
    if abs(len(token1) - len(token2)) > max(len(token1), len(token2)) * 0.5:
        return False
    
    # éƒ¨åˆ†æ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
    if token1 in token2 or token2 in token1:
        return True
    
    # ç·¨é›†è·é›¢ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ï¼‰
    return _calculate_similarity(token1, token2) > 0.6

def _calculate_similarity(token1: str, token2: str) -> float:
    """ç°¡æ˜“é¡ä¼¼åº¦è¨ˆç®—ï¼ˆJaccardï¼‰"""
    set1 = set(token1.lower())
    set2 = set(token2.lower())
    
    if not set1 and not set2:
        return 1.0
    if not set1 or not set2:
        return 0.0
    
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    return intersection / union

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆCLIå¯¾å¿œï¼‰"""
    parser = argparse.ArgumentParser(description="Golden Test Root Cause Analyzer")
    parser.add_argument("--update-freshness", action="store_true", 
                       help="è¦³æ¸¬ãƒ­ã‚°ã«freshnessã‚¿ã‚°ã‚’è¿½åŠ ")
    parser.add_argument("--export-new-fails", type=str,
                       help="æ–°è¦å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’JSONã§å‡ºåŠ›ï¼ˆä¾‹: out/new_fails.jsonï¼‰")
    
    args = parser.parse_args()
    
    if args.update_freshness:
        apply_freshness_to_log()
    elif args.export_new_fails:
        export_new_failures(args.export_new_fails)
    else:
        update_observation_log_with_analysis()

if __name__ == "__main__":
    main()
