#!/usr/bin/env python3
"""
Golden Test Runner with Shadow Evaluation
ã—ãã„å€¤å½±éŸ¿äºˆæ¸¬ãƒ»ã‚·ãƒ£ãƒ‰ãƒ¼è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import argparse
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    # PR2: MODELèµ·å› å¤±æ•—ç”¨ã®å¼·åŒ–äºˆæ¸¬é–¢æ•°ã‚’ä½¿ç”¨
    from .run_golden_pr2 import predict, load_config
    from .evaluator import score
    from .root_cause_analyzer import analyze_failure_root_cause, Freshness
except ImportError:
    try:
        # çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ
        from run_golden_pr2 import predict, load_config
        from evaluator import score
        from root_cause_analyzer import analyze_failure_root_cause, Freshness
    except ImportError as e:
        print(f"Import error: {e}")
        print("Please run from tests/golden directory or ensure modules are accessible")
        sys.exit(1)

def load_weights_config(weights_file: str = None) -> Dict[str, Any]:
    """é‡ã¿ä»˜ã‘è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    if weights_file is None:
        weights_file = "tests/golden/weights_phase4.yaml"
    
    weights_path = Path(weights_file)
    if not weights_path.exists():
        print(f"âš ï¸ Weights file not found: {weights_path}, using default weights")
        return {"default_weight": 1.0, "weights": {}, "weight_multipliers": {}}
    
    try:
        with open(weights_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"âš ï¸ Error loading weights file: {e}, using default weights")
        return {"default_weight": 1.0, "weights": {}, "weight_multipliers": {}}

def calculate_case_weight(case_id: str, weights_config: Dict[str, Any], root_cause: str = None, freshness: str = None) -> float:
    """ã‚±ãƒ¼ã‚¹ã®é‡ã¿ã‚’è¨ˆç®—"""
    base_weight = weights_config.get("default_weight", 1.0)
    
    # å„ªå…ˆåº¦ã‚°ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹é‡ã¿
    weights = weights_config.get("weights", {})
    multipliers = weights_config.get("weight_multipliers", {})
    
    for priority_level, case_list in weights.items():
        if case_id in case_list:
            base_weight = multipliers.get(priority_level, 1.0)
            break
    
    # Phase4ç‰¹åˆ¥è¨­å®šã®é©ç”¨
    phase4_config = weights_config.get("phase4_config", {})
    
    # æ–°è¦å¤±æ•—ãƒœãƒ¼ãƒŠã‚¹
    if freshness == "NEW":
        base_weight += phase4_config.get("new_failure_bonus", 0.0)
    
    # FlakyãƒšãƒŠãƒ«ãƒ†ã‚£
    if root_cause == "FLAKY":
        base_weight += phase4_config.get("flaky_penalty", 0.0)
    
    # ãƒ¢ãƒ‡ãƒ«èµ·å› å¤±æ•—ãƒœãƒ¼ãƒŠã‚¹
    if root_cause == "MODEL":
        base_weight += phase4_config.get("model_failure_bonus", 0.0)
    
    return max(0.1, base_weight)  # æœ€å°é‡ã¿0.1ã‚’ä¿è¨¼

def run_shadow_evaluation(shadow_thresholds: str, report_path: str = None, weights_file: str = None) -> Dict[str, Any]:
    """Multi-Shadowè©•ä¾¡å®Ÿè¡Œï¼ˆè¤‡æ•°ã—ãã„å€¤ã§ã®äºˆæ¸¬è©•ä¾¡ï¼‰"""
    
    # ã—ãã„å€¤è§£æï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå¯¾å¿œï¼‰
    if isinstance(shadow_thresholds, str):
        threshold_list = [float(t.strip()) for t in shadow_thresholds.split(',')]
    else:
        threshold_list = [float(shadow_thresholds)]
    
    # ç¾åœ¨ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿
    config = load_config()
    current_threshold = config.get("threshold", 0.5)
    
    print(f"ğŸ” Multi-Shadow Evaluation: current={current_threshold} â†’ targets={threshold_list}")
    print(f"ğŸ“Š æœ¬ç•ªã—ãã„å€¤ã¯ {current_threshold} ã®ã¾ã¾ã€{threshold_list} ã§ã®äºˆæ¸¬è©•ä¾¡ã‚’å®Ÿè¡Œ")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
    cases_dir = Path("tests/golden/cases")
    if not cases_dir.exists():
        raise FileNotFoundError(f"Test cases directory not found: {cases_dir}")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ãƒ»äºˆæ¸¬å®Ÿè¡Œï¼ˆ1å›ã ã‘ï¼‰
    case_results = []
    total_cases = len(list(cases_dir.glob("*.json")))
    print(f"ğŸ“„ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {total_cases}")
    
    # å¤±æ•—å±¥æ­´ã‚’æ§‹ç¯‰ï¼ˆnew_fail_ratioè¨ˆç®—ç”¨ï¼‰
    failure_history = build_failure_history()
    current_date = datetime.now().strftime("%Y%m%d")
    
    for case_file in sorted(cases_dir.glob("*.json")):
        try:
            with open(case_file, 'r', encoding='utf-8') as f:
                case_data = json.load(f)
            
            case_id = case_data.get("id", case_file.stem)
            reference = case_data.get("reference", "")
            input_text = case_data.get("input", "")
            
            # äºˆæ¸¬å®Ÿè¡Œ
            prediction = predict(input_text)
            test_score = score(reference, prediction)
            
            case_results.append({
                "id": case_id,
                "input": input_text,
                "reference": reference,
                "prediction": prediction,
                "score": test_score
            })
        except Exception as e:
            print(f"âš ï¸ Error processing {case_file}: {e}")
            continue
    
    # å„ã—ãã„å€¤ã§ã®è©•ä¾¡
    threshold_reports = {}
    
    for shadow_threshold in threshold_list:
        print(f"\nğŸ¯ Evaluating threshold: {shadow_threshold}")
        
        shadow_passed = 0
        weighted_shadow_passed = 0.0
        total_weight = 0.0
        total_failures = 0
        new_failures = 0
        flaky_failures = 0
        root_cause_counts = {}
        failed_cases = []
        
        # é‡ã¿è¨­å®šã‚’èª­ã¿è¾¼ã¿
        weights_config = load_weights_config(weights_file)
        
        for case_result in case_results:
            case_id = case_result["id"]
            reference = case_result["reference"]
            prediction = case_result["prediction"]
            test_score = case_result["score"]
            
            # Root Causeåˆ†æã¨Freshnessåˆ¤å®šï¼ˆå…¨ã‚±ãƒ¼ã‚¹ã§å®Ÿè¡Œï¼‰
            root_cause = analyze_failure_root_cause(case_id, reference, prediction, test_score)
            root_cause_str = root_cause.value
            freshness = infer_freshness(case_id, current_date, failure_history)
            freshness_str = freshness.value
            
            # ã‚±ãƒ¼ã‚¹é‡ã¿ã‚’è¨ˆç®—
            case_weight = calculate_case_weight(case_id, weights_config, root_cause_str, freshness_str)
            total_weight += case_weight
            
            # ã‚·ãƒ£ãƒ‰ãƒ¼ã—ãã„å€¤ã§ã®çµæœ
            shadow_passed_case = test_score >= shadow_threshold
            if shadow_passed_case:
                shadow_passed += 1
                weighted_shadow_passed += case_weight
            else:
                total_failures += 1
                failed_cases.append(case_result.copy())
            
                root_cause_counts[root_cause_str] = root_cause_counts.get(root_cause_str, 0) + 1
                
                if freshness == Freshness.NEW:
                    new_failures += 1
                
                # Flakyåˆ¤å®š
                if test_score >= 0.7:
                    flaky_failures += 1
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        shadow_pass_rate = (shadow_passed / total_cases) * 100 if total_cases > 0 else 0
        weighted_pass_rate = (weighted_shadow_passed / total_weight) * 100 if total_weight > 0 else 0
        new_fail_ratio = new_failures / max(total_failures, 1)
        flaky_rate = flaky_failures / max(total_failures, 1)
        root_cause_top3 = sorted(root_cause_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # ã—ãã„å€¤åˆ¥ãƒ¬ãƒãƒ¼ãƒˆ
        threshold_reports[str(shadow_threshold)] = {
            "threshold": shadow_threshold,
            "total_cases": total_cases,
            "shadow_passed": shadow_passed,
            "shadow_pass_rate": shadow_pass_rate,
            "weighted_pass_rate": weighted_pass_rate,
            "total_weight": total_weight,
            "total_failures": total_failures,
            "new_failures": new_failures,
            "new_fail_ratio": new_fail_ratio,
            "flaky_failures": flaky_failures,
            "flaky_rate": flaky_rate,
            "root_cause_top3": root_cause_top3,
            "failed_cases": failed_cases
        }
        
        # çµæœè¡¨ç¤º
        print(f"  åˆæ ¼ç‡: {shadow_passed}/{total_cases} ({shadow_pass_rate:.1f}%)")
        print(f"  é‡ã¿ä»˜ãåˆæ ¼ç‡: {weighted_pass_rate:.1f}% (weight: {total_weight:.1f})")
        print(f"  Flakyç‡: {flaky_rate:.1%}")
        print(f"  æ–°è¦å¤±æ•—ç‡: {new_fail_ratio:.1%}")
        print(f"  Root Cause Top3: {root_cause_top3}")
    
    # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = {
        "multi_shadow_evaluation": {
            "current_threshold": current_threshold,
            "shadow_thresholds": threshold_list,
            "timestamp": datetime.now().isoformat(),
            "total_cases": total_cases,
            "thresholds": threshold_reports
        }
    }
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    if report_path:
        report_file = Path(report_path)
        report_file.parent.mkdir(parents=True, exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ Shadow evaluation report saved: {report_file}")
    
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nğŸ“Š Multi-Shadow Evaluation Summary:")
    for threshold in threshold_list:
        threshold_data = threshold_reports[str(threshold)]
        print(f"  Threshold {threshold}: {threshold_data['shadow_passed']}/{total_cases} ({threshold_data['shadow_pass_rate']:.1f}%)")
    
    return report

def build_failure_history() -> Dict[str, set]:
    """éå»ã®å¤±æ•—å±¥æ­´ã‚’æ§‹ç¯‰"""
    logs_dir = Path("tests/golden/logs")
    failure_history = {}
    
    if not logs_dir.exists():
        return failure_history
    
    for log_file in sorted(logs_dir.glob("*.jsonl")):
        date_str = log_file.stem.split('_')[0]  # YYYYMMDDéƒ¨åˆ†
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        if not data.get('passed', True):  # å¤±æ•—ã‚±ãƒ¼ã‚¹
                            case_id = data.get('id', '')
                            if case_id:
                                if case_id not in failure_history:
                                    failure_history[case_id] = set()
                                failure_history[case_id].add(date_str)
                    except json.JSONDecodeError:
                        continue
    
    return failure_history

def infer_freshness(case_id: str, current_date: str, history: Dict[str, set]) -> Freshness:
    """å¤±æ•—ã®æ–°è¦æ€§ã‚’åˆ¤å®š"""
    if case_id not in history:
        return Freshness.NEW
    
    past_failures = history[case_id]
    for past_date in past_failures:
        if past_date < current_date:
            return Freshness.REPEAT
    
    return Freshness.NEW

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Golden Test Runner with Shadow Evaluation")
    parser.add_argument("--threshold-shadow", type=str, 
                       help="ã‚·ãƒ£ãƒ‰ãƒ¼è©•ä¾¡ç”¨ã—ãã„å€¤ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå¯ã€ä¾‹: 0.7,0.85ï¼‰")
    parser.add_argument("--report", type=str, 
                       help="ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ‘ã‚¹ï¼ˆä¾‹: out/shadow_0_7.jsonï¼‰")
    parser.add_argument("--weights", type=str,
                       help="é‡ã¿ä»˜ã‘è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: tests/golden/weights_phase4.yamlï¼‰")
    
    args = parser.parse_args()
    
    if args.threshold_shadow:
        try:
            report = run_shadow_evaluation(args.threshold_shadow, args.report, args.weights)
            # é‡ã¿ä»˜ãè©•ä¾¡ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
            if "multi_shadow_evaluation" in report:
                thresholds = report["multi_shadow_evaluation"]["thresholds"]
                if "0.85" in thresholds:
                    weighted_rate = thresholds["0.85"].get("weighted_pass_rate", thresholds["0.85"].get("shadow_pass_rate", 0))
                    return weighted_rate >= 80
            return report.get("shadow_evaluation", {}).get("shadow_pass_rate", 0) >= 80  # æˆåŠŸåˆ¤å®š
        except Exception as e:
            print(f"âŒ Shadow evaluation failed: {e}")
            return False
    else:
        parser.print_help()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
