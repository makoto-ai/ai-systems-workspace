#!/usr/bin/env python3
"""
Real-time Analytics and Prediction System
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ»äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import streamlit as st
from typing import Dict, Any, List, Optional
import json
import logging
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import joblib
import os

class RealTimeAnalytics:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.setup_logging()
        self.data_stream = []
        self.predictions = []
        self.models = {}
        self.scalers = {}
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('real_time_analytics.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def add_data_point(self, data: Dict[str, Any]):
        """ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆè¿½åŠ """
        data['timestamp'] = datetime.now()
        self.data_stream.append(data)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’1000ä»¶ã«åˆ¶é™
        if len(self.data_stream) > 1000:
            self.data_stream.pop(0)
        
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆè¿½åŠ : {data}")
    
    def get_recent_data(self, hours: int = 24) -> List[Dict[str, Any]]:
        """æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [d for d in self.data_stream if d['timestamp'] >= cutoff_time]
    
    def calculate_metrics(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        if not data:
            return {}
        
        df = pd.DataFrame(data)
        
        metrics = {
            'total_operations': len(data),
            'avg_quality_score': df.get('quality_score', pd.Series([0])).mean(),
            'avg_generation_time': df.get('generation_time', pd.Series([0])).mean(),
            'success_rate': (df.get('success', pd.Series([True])).sum() / len(data)) * 100,
            'popular_styles': df.get('style', pd.Series(['unknown'])).value_counts().to_dict(),
            'avg_content_length': df.get('content_length', pd.Series([0])).mean()
        }
        
        return metrics
    
    def generate_predictions(self, target_metric: str = 'quality_score', hours_ahead: int = 6) -> Dict[str, Any]:
        """äºˆæ¸¬ç”Ÿæˆ"""
        if len(self.data_stream) < 10:
            return {"error": "ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"}
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df = pd.DataFrame(self.data_stream)
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        
        # ç‰¹å¾´é‡ä½œæˆ
        features = ['hour', 'day_of_week']
        if 'quality_score' in df.columns:
            features.append('quality_score')
        if 'generation_time' in df.columns:
            features.append('generation_time')
        
        X = df[features].fillna(0)
        y = df.get(target_metric, pd.Series([0]))
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X, y)
        
        # å°†æ¥ã®æ™‚é–“ãƒã‚¤ãƒ³ãƒˆã§äºˆæ¸¬
        future_hours = []
        current_time = datetime.now()
        for i in range(hours_ahead):
            future_time = current_time + timedelta(hours=i+1)
            future_hours.append({
                'hour': future_time.hour,
                'day_of_week': future_time.weekday()
            })
        
        future_df = pd.DataFrame(future_hours)
        predictions = model.predict(future_df[features])
        
        return {
            'predictions': predictions.tolist(),
            'future_hours': [h['hour'] for h in future_hours],
            'model_accuracy': model.score(X, y),
            'feature_importance': dict(zip(features, model.feature_importances_))
        }
    
    def create_realtime_dashboard(self) -> Dict[str, Any]:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ"""
        recent_data = self.get_recent_data(24)
        metrics = self.calculate_metrics(recent_data)
        predictions = self.generate_predictions()
        
        return {
            'metrics': metrics,
            'predictions': predictions,
            'data_points': len(recent_data),
            'last_update': datetime.now().isoformat()
        }
    
    def create_visualizations(self) -> Dict[str, go.Figure]:
        """å¯è¦–åŒ–ä½œæˆ"""
        if not self.data_stream:
            return {}
        
        df = pd.DataFrame(self.data_stream)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        figures = {}
        
        # æ™‚ç³»åˆ—å“è³ªã‚¹ã‚³ã‚¢
        if 'quality_score' in df.columns:
            fig_quality = go.Figure()
            fig_quality.add_trace(go.Scatter(
                x=df['timestamp'],
                y=df['quality_score'],
                mode='lines+markers',
                name='å“è³ªã‚¹ã‚³ã‚¢',
                line=dict(color='blue')
            ))
            fig_quality.update_layout(
                title='ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªã‚¹ã‚³ã‚¢æ¨ç§»',
                xaxis_title='æ™‚é–“',
                yaxis_title='å“è³ªã‚¹ã‚³ã‚¢',
                height=400
            )
            figures['quality_trend'] = fig_quality
        
        # ç”Ÿæˆæ™‚é–“åˆ†å¸ƒ
        if 'generation_time' in df.columns:
            fig_time = go.Figure()
            fig_time.add_trace(go.Histogram(
                x=df['generation_time'],
                nbinsx=20,
                name='ç”Ÿæˆæ™‚é–“åˆ†å¸ƒ'
            ))
            fig_time.update_layout(
                title='ç”Ÿæˆæ™‚é–“åˆ†å¸ƒ',
                xaxis_title='ç”Ÿæˆæ™‚é–“ï¼ˆç§’ï¼‰',
                yaxis_title='é »åº¦',
                height=400
            )
            figures['generation_time_dist'] = fig_time
        
        # ã‚¹ã‚¿ã‚¤ãƒ«åˆ¥ä½¿ç”¨é »åº¦
        if 'style' in df.columns:
            style_counts = df['style'].value_counts()
            fig_style = go.Figure(data=[go.Pie(
                labels=style_counts.index,
                values=style_counts.values,
                hole=0.3
            )])
            fig_style.update_layout(
                title='ã‚¹ã‚¿ã‚¤ãƒ«åˆ¥ä½¿ç”¨é »åº¦',
                height=400
            )
            figures['style_usage'] = fig_style
        
        return figures
    
    def detect_anomalies(self, threshold: float = 2.0) -> List[Dict[str, Any]]:
        """ç•°å¸¸æ¤œçŸ¥"""
        if len(self.data_stream) < 10:
            return []
        
        df = pd.DataFrame(self.data_stream)
        anomalies = []
        
        if 'quality_score' in df.columns:
            mean_score = df['quality_score'].mean()
            std_score = df['quality_score'].std()
            
            for idx, row in df.iterrows():
                z_score = abs((row['quality_score'] - mean_score) / std_score)
                if z_score > threshold:
                    anomalies.append({
                        'timestamp': row['timestamp'],
                        'metric': 'quality_score',
                        'value': row['quality_score'],
                        'z_score': z_score,
                        'severity': 'high' if z_score > 3 else 'medium'
                    })
        
        return anomalies
    
    def get_performance_insights(self) -> Dict[str, Any]:
        """æ€§èƒ½ã‚¤ãƒ³ã‚µã‚¤ãƒˆå–å¾—"""
        if not self.data_stream:
            return {}
        
        df = pd.DataFrame(self.data_stream)
        
        insights = {
            'total_operations': len(df),
            'avg_quality': df.get('quality_score', pd.Series([0])).mean(),
            'best_performing_style': df.get('style', pd.Series(['unknown'])).mode().iloc[0] if len(df) > 0 else 'unknown',
            'peak_usage_hour': df['timestamp'].dt.hour.mode().iloc[0] if len(df) > 0 else 0,
            'recent_trend': 'improving' if len(df) >= 2 and df['quality_score'].iloc[-1] > df['quality_score'].iloc[-2] else 'declining',
            'anomaly_count': len(self.detect_anomalies())
        }
        
        return insights

class PredictiveAnalytics:
    """äºˆæ¸¬åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.prediction_history = []
        
    def train_prediction_model(self, data: List[Dict[str, Any]], target: str):
        """äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        if len(data) < 20:
            return False
        
        df = pd.DataFrame(data)
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['month'] = df['timestamp'].dt.month
        
        features = ['hour', 'day_of_week', 'month']
        if 'quality_score' in df.columns:
            features.append('quality_score')
        if 'generation_time' in df.columns:
            features.append('generation_time')
        
        X = df[features].fillna(0)
        y = df[target].fillna(0)
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_scaled, y)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.models[target] = model
        self.scalers[target] = scaler
        
        return True
    
    def predict_future_values(self, target: str, hours_ahead: int = 24) -> List[float]:
        """å°†æ¥å€¤äºˆæ¸¬"""
        if target not in self.models:
            return []
        
        model = self.models[target]
        scaler = self.scalers[target]
        
        # å°†æ¥ã®æ™‚é–“ãƒã‚¤ãƒ³ãƒˆ
        future_times = []
        current_time = datetime.now()
        
        for i in range(hours_ahead):
            future_time = current_time + timedelta(hours=i+1)
            future_times.append({
                'hour': future_time.hour,
                'day_of_week': future_time.weekday(),
                'month': future_time.month
            })
        
        future_df = pd.DataFrame(future_times)
        X_future = scaler.transform(future_df)
        predictions = model.predict(X_future)
        
        return predictions.tolist()

def display_real_time_analytics():
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¡¨ç¤º"""
    st.header("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ»äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
    
    analytics = RealTimeAnalytics()
    predictive = PredictiveAnalytics()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.subheader("âš™ï¸ åˆ†æè¨­å®š")
    analysis_hours = st.sidebar.slider("åˆ†ææœŸé–“ï¼ˆæ™‚é–“ï¼‰", 1, 168, 24)
    prediction_hours = st.sidebar.slider("äºˆæ¸¬æœŸé–“ï¼ˆæ™‚é–“ï¼‰", 1, 72, 6)
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“ˆ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        if st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿æ›´æ–°"):
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            sample_data = {
                'quality_score': np.random.normal(0.8, 0.1),
                'generation_time': np.random.exponential(2.0),
                'style': np.random.choice(['popular', 'academic', 'business']),
                'content_length': np.random.randint(500, 2000),
                'success': True
            }
            analytics.add_data_point(sample_data)
            st.success("ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº†ï¼")
        
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º
        dashboard = analytics.create_realtime_dashboard()
        
        if dashboard:
            metrics = dashboard['metrics']
            
            st.metric("ç·æ“ä½œæ•°", metrics.get('total_operations', 0))
            st.metric("å¹³å‡å“è³ªã‚¹ã‚³ã‚¢", f"{metrics.get('avg_quality_score', 0):.3f}")
            st.metric("å¹³å‡ç”Ÿæˆæ™‚é–“", f"{metrics.get('avg_generation_time', 0):.2f}ç§’")
            st.metric("æˆåŠŸç‡", f"{metrics.get('success_rate', 0):.1f}%")
    
    with col2:
        st.subheader("ğŸ”® äºˆæ¸¬åˆ†æ")
        
        if st.button("ğŸ¯ äºˆæ¸¬å®Ÿè¡Œ"):
            predictions = analytics.generate_predictions(hours_ahead=prediction_hours)
            
            if 'predictions' in predictions:
                st.write("**å“è³ªã‚¹ã‚³ã‚¢äºˆæ¸¬:**")
                for i, pred in enumerate(predictions['predictions']):
                    st.write(f"æ™‚é–“ {i+1}: {pred:.3f}")
                
                if 'model_accuracy' in predictions:
                    st.metric("ãƒ¢ãƒ‡ãƒ«ç²¾åº¦", f"{predictions['model_accuracy']:.3f}")
    
    # å¯è¦–åŒ–
    st.subheader("ğŸ“Š å¯è¦–åŒ–")
    
    figures = analytics.create_visualizations()
    
    if figures:
        for name, fig in figures.items():
            st.plotly_chart(fig, use_container_width=True)
    
    # ç•°å¸¸æ¤œçŸ¥
    st.subheader("ğŸš¨ ç•°å¸¸æ¤œçŸ¥")
    
    anomalies = analytics.detect_anomalies()
    
    if anomalies:
        st.warning(f"ç•°å¸¸æ¤œçŸ¥: {len(anomalies)}ä»¶")
        for anomaly in anomalies[:5]:  # æœ€æ–°5ä»¶ã‚’è¡¨ç¤º
            st.write(f"- {anomaly['timestamp']}: {anomaly['metric']} = {anomaly['value']:.3f} (Z-score: {anomaly['z_score']:.2f})")
    else:
        st.success("ç•°å¸¸ã¯æ¤œçŸ¥ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # æ€§èƒ½ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    st.subheader("ğŸ’¡ æ€§èƒ½ã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
    
    insights = analytics.get_performance_insights()
    
    if insights:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ç·æ“ä½œæ•°", insights['total_operations'])
            st.metric("å¹³å‡å“è³ª", f"{insights['avg_quality']:.3f}")
        
        with col2:
            st.metric("æœ€é©ã‚¹ã‚¿ã‚¤ãƒ«", insights['best_performing_style'])
            st.metric("ãƒ”ãƒ¼ã‚¯æ™‚é–“", f"{insights['peak_usage_hour']}æ™‚")
        
        with col3:
            st.metric("ãƒˆãƒ¬ãƒ³ãƒ‰", insights['recent_trend'])
            st.metric("ç•°å¸¸æ•°", insights['anomaly_count'])

if __name__ == "__main__":
    display_real_time_analytics() 