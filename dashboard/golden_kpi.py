#!/usr/bin/env python3
"""
Golden Test KPI Dashboard
Streamlit-based visualization for Golden Test metrics
"""

import streamlit as st
import pandas as pd
import json
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
from datetime import datetime, timedelta
import re

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Golden Test KPI Dashboard",
    page_icon="ğŸ¯",
    layout="wide"
)

def load_golden_logs():
    """Golden Testã®ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿"""
    logs_dir = Path("tests/golden/logs")
    if not logs_dir.exists():
        return pd.DataFrame()
    
    all_data = []
    for log_file in logs_dir.glob("*.jsonl"):
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥æ™‚ã‚’æŠ½å‡º
                        date_str = log_file.stem  # 20250829_211416
                        try:
                            date_obj = datetime.strptime(date_str, "%Y%m%d_%H%M%S")
                            data['timestamp'] = date_obj
                            data['date'] = date_obj.date()
                            all_data.append(data)
                        except ValueError:
                            continue
        except Exception as e:
            st.error(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {log_file}: {e}")
            continue
    
    return pd.DataFrame(all_data)

def load_observation_log():
    """è¦³æ¸¬ãƒ­ã‚°ã‹ã‚‰é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    log_file = Path("tests/golden/observation_log.md")
    if not log_file.exists():
        return pd.DataFrame()
    
    weekly_data = []
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # é€±æ¬¡è¦³æ¸¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        pattern = r'## (\d{4}-\d{2}-\d{2}) - é€±æ¬¡è¦³æ¸¬.*?åˆæ ¼ç‡.*?(\d+)/(\d+) \((\d+)%\)'
        matches = re.findall(pattern, content, re.DOTALL)
        
        for match in matches:
            date_str, passed, total, percentage = match
            
            # è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰freshnessæƒ…å ±ã‚’æŠ½å‡º
            section_pattern = rf'## {re.escape(date_str)} - é€±æ¬¡è¦³æ¸¬(.*?)(?=## |\Z)'
            section_match = re.search(section_pattern, content, re.DOTALL)
            
            new_failures = 0
            total_failures = 0
            
            if section_match:
                section_content = section_match.group(1)
                # å¤±æ•—åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œç´¢
                failure_matches = re.findall(r'- \*\*([^*]+)\*\*: `root_cause:([^`]+)`(?:\s*\|\s*`freshness:([^`]+)`)?', section_content)
                for case_id, root_cause, freshness in failure_matches:
                    total_failures += 1
                    if freshness == "NEW":
                        new_failures += 1
            
            new_fail_ratio = new_failures / max(total_failures, 1) if total_failures > 0 else 0.0
            
            weekly_data.append({
                'date': datetime.strptime(date_str, "%Y-%m-%d").date(),
                'passed': int(passed),
                'total': int(total),
                'pass_rate': int(percentage),
                'total_failures': total_failures,
                'new_failures': new_failures,
                'new_fail_ratio': new_fail_ratio
            })
    
    except Exception as e:
        st.error(f"è¦³æ¸¬ãƒ­ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    return pd.DataFrame(weekly_data)

def analyze_failure_reasons(df):
    """å¤±æ•—ç†ç”±ã®åˆ†æï¼ˆRoot Causeåˆ†æå«ã‚€ï¼‰"""
    failed_cases = df[df['passed'] == False]
    if failed_cases.empty:
        return pd.DataFrame()
    
    failure_analysis = []
    for _, case in failed_cases.iterrows():
        ref_words = set(case['reference'].split())
        pred_words = set(case['prediction'].split()) if case['prediction'] else set()
        
        missing_words = ref_words - pred_words
        extra_words = pred_words - ref_words
        
        # Root Causeåˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
        root_cause = analyze_root_cause(case['score'], missing_words, pred_words)
        
        failure_analysis.append({
            'case_id': case['id'],
            'score': case['score'],
            'missing_words': ', '.join(missing_words) if missing_words else 'ãªã—',
            'extra_words': ', '.join(extra_words) if extra_words else 'ãªã—',
            'missing_count': len(missing_words),
            'root_cause': root_cause,
            'date': case.get('date', 'Unknown')
        })
    
    return pd.DataFrame(failure_analysis)

def analyze_root_cause(score, missing_words, pred_words):
    """Root Causeç°¡æ˜“åˆ†æ"""
    if not pred_words:
        return "INFRA"
    elif score == 0:
        return "MODEL"
    elif score < 0.3 and missing_words:
        return "NORMALIZE"
    elif score < 0.7:
        return "TOKENIZE"
    else:
        return "FLAKY"

def calculate_flaky_rate(df):
    """Flakyç‡ã®è¨ˆç®—"""
    if df.empty:
        return 0.0
    
    failed_cases = df[df['passed'] == False]
    if failed_cases.empty:
        return 0.0
    
    # ã‚¹ã‚³ã‚¢0.7ä»¥ä¸Šã®å¤±æ•—ã‚’Flakyã¨åˆ¤å®š
    flaky_cases = failed_cases[failed_cases['score'] >= 0.7]
    return len(flaky_cases) / len(failed_cases) * 100

def calculate_model_efficiency(df):
    """ãƒ¢ãƒ‡ãƒ«åˆ¥åŠ¹ç‡æ€§ã®è¨ˆç®—ï¼ˆåˆæ ¼1ä»¶ã‚ãŸã‚Šã®è©¦è¡Œå›æ•°ï¼‰"""
    # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã¯è©¦è¡Œå›æ•°ã®æƒ…å ±ãŒãªã„ãŸã‚ã€
    # åˆæ ¼ç‡ã‹ã‚‰æ¨å®šåŠ¹ç‡ã‚’è¨ˆç®—
    model_stats = []
    
    for date in df['date'].unique():
        day_data = df[df['date'] == date]
        if day_data.empty:
            continue
            
        total_cases = len(day_data)
        passed_cases = len(day_data[day_data['passed'] == True])
        pass_rate = passed_cases / total_cases if total_cases > 0 else 0
        
        # åŠ¹ç‡æ€§ã®æ¨å®šï¼ˆåˆæ ¼1ä»¶ã‚ãŸã‚Šã®æƒ³å®šè©¦è¡Œå›æ•°ï¼‰
        efficiency = 1 / pass_rate if pass_rate > 0 else float('inf')
        
        model_stats.append({
            'date': date,
            'model': 'Groq (llama-3.3-70b)',  # ç¾åœ¨å›ºå®š
            'total_cases': total_cases,
            'passed_cases': passed_cases,
            'pass_rate': pass_rate,
            'trials_per_success': efficiency
        })
    
    return pd.DataFrame(model_stats)

# ãƒ¡ã‚¤ãƒ³ç”»é¢
st.title("ğŸ¯ Golden Test KPI Dashboard")
st.markdown("AIãƒ¢ãƒ‡ãƒ«å‡ºåŠ›å“è³ªã®ç¶™ç¶šç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
with st.spinner("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­..."):
    df = load_golden_logs()
    weekly_df = load_observation_log()

if df.empty and weekly_df.empty:
    st.warning("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Golden Testã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
st.sidebar.header("ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
if not df.empty:
    date_range = st.sidebar.date_input(
        "æœŸé–“é¸æŠ",
        value=(df['date'].min(), df['date'].max()),
        min_value=df['date'].min(),
        max_value=df['date'].max()
    )
    
    # æœŸé–“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if len(date_range) == 2:
        start_date, end_date = date_range
        df_filtered = df[(df['date'] >= start_date) & (df['date'] <= end_date)]
    else:
        df_filtered = df
else:
    df_filtered = df

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
col1, col2, col3, col4, col5, col6 = st.columns(6)

if not df_filtered.empty:
    total_cases = len(df_filtered)
    passed_cases = len(df_filtered[df_filtered['passed'] == True])
    pass_rate = passed_cases / total_cases * 100 if total_cases > 0 else 0
    avg_score = df_filtered['score'].mean()
    flaky_rate = calculate_flaky_rate(df_filtered)
    
    # New Fail Ratioè¨ˆç®—ï¼ˆé€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
    if not weekly_df.empty and 'new_fail_ratio' in weekly_df.columns:
        latest_new_fail_ratio = weekly_df.iloc[-1]['new_fail_ratio'] * 100
    else:
        latest_new_fail_ratio = 0.0
    
    col1.metric("ç·ãƒ†ã‚¹ãƒˆæ•°", total_cases)
    col2.metric("åˆæ ¼æ•°", passed_cases)
    col3.metric("åˆæ ¼ç‡", f"{pass_rate:.1f}%")
    col4.metric("å¹³å‡ã‚¹ã‚³ã‚¢", f"{avg_score:.3f}")
    col5.metric("Flakyç‡", f"{flaky_rate:.1f}%")
    col6.metric("æ–°è¦å¤±æ•—ç‡", f"{latest_new_fail_ratio:.1f}%")

# 1. é€±æ¬¡åˆæ ¼ç‡ï¼ˆãƒ©ã‚¤ãƒ³ãƒãƒ£ãƒ¼ãƒˆï¼‰
st.header("ğŸ“ˆ é€±æ¬¡åˆæ ¼ç‡ãƒˆãƒ¬ãƒ³ãƒ‰")

if not weekly_df.empty:
    fig_weekly = px.line(
        weekly_df, 
        x='date', 
        y='pass_rate',
        title='é€±æ¬¡åˆæ ¼ç‡ã®æ¨ç§»',
        labels={'pass_rate': 'åˆæ ¼ç‡ (%)', 'date': 'æ—¥ä»˜'},
        markers=True
    )
    
    # ã—ãã„å€¤ãƒ©ã‚¤ãƒ³ã‚’è¿½åŠ 
    fig_weekly.add_hline(y=90, line_dash="dash", line_color="green", 
                        annotation_text="ç›®æ¨™: 90%")
    fig_weekly.add_hline(y=80, line_dash="dash", line_color="orange", 
                        annotation_text="è­¦å‘Š: 80%")
    
    fig_weekly.update_layout(height=400)
    st.plotly_chart(fig_weekly, use_container_width=True)
    
    # æ–°è¦å¤±æ•—ç‡ãƒˆãƒ¬ãƒ³ãƒ‰
    if 'new_fail_ratio' in weekly_df.columns:
        st.subheader("ğŸ“Š æ–°è¦å¤±æ•—ç‡ãƒˆãƒ¬ãƒ³ãƒ‰")
        fig_new_fail = px.line(
            weekly_df,
            x='date',
            y='new_fail_ratio',
            title='é€±æ¬¡æ–°è¦å¤±æ•—ç‡ã®æ¨ç§»',
            labels={'new_fail_ratio': 'æ–°è¦å¤±æ•—ç‡', 'date': 'æ—¥ä»˜'},
            markers=True
        )
        fig_new_fail.update_traces(line_color='red')
        fig_new_fail.update_layout(
            height=300,
            yaxis=dict(tickformat=".1%")
        )
        st.plotly_chart(fig_new_fail, use_container_width=True)
else:
    st.info("é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è¦³æ¸¬ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# 2. å¤±æ•—ç†ç”±ã®ä¸Šä½ï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
st.header("ğŸ” å¤±æ•—ç†ç”±åˆ†æ")

if not df_filtered.empty:
    failure_df = analyze_failure_reasons(df_filtered)
    
    if not failure_df.empty:
        # Root Cause Top3ã®è¡¨ç¤º
        st.subheader("ğŸ“Š Root Cause Top3")
        root_cause_counts = failure_df['root_cause'].value_counts().head(3)
        
        if not root_cause_counts.empty:
            col1, col2, col3 = st.columns(3)
            
            for i, (cause, count) in enumerate(root_cause_counts.items()):
                percentage = (count / len(failure_df)) * 100
                with [col1, col2, col3][i]:
                    st.metric(
                        f"#{i+1} {cause}",
                        f"{count}ä»¶",
                        f"{percentage:.1f}%"
                    )
        
        # ä¸è¶³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é »åº¦åˆ†æ
        st.subheader("ğŸ”¤ ä¸è¶³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½10")
        missing_words_flat = []
        for words_str in failure_df['missing_words']:
            if words_str != 'ãªã—':
                missing_words_flat.extend(words_str.split(', '))
        
        if missing_words_flat:
            missing_freq = pd.Series(missing_words_flat).value_counts().head(10)
            
            fig_failures = px.bar(
                x=missing_freq.values,
                y=missing_freq.index,
                orientation='h',
                title='ä¸è¶³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦',
                labels={'x': 'å‡ºç¾å›æ•°', 'y': 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'}
            )
            fig_failures.update_layout(height=400)
            st.plotly_chart(fig_failures, use_container_width=True)
        else:
            st.success("åˆ†ææœŸé–“ä¸­ã«å¤±æ•—ã‚±ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ï¼")
    else:
        st.success("åˆ†ææœŸé–“ä¸­ã«å¤±æ•—ã‚±ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ï¼")
else:
    st.info("åˆ†æã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# 3. ãƒ¢ãƒ‡ãƒ«åˆ¥åŠ¹ç‡æ€§ï¼ˆè¡¨å½¢å¼ï¼‰
st.header("âš¡ ãƒ¢ãƒ‡ãƒ«åŠ¹ç‡æ€§")

if not df_filtered.empty:
    efficiency_df = calculate_model_efficiency(df_filtered)
    
    if not efficiency_df.empty:
        # åŠ¹ç‡æ€§ãƒ†ãƒ¼ãƒ–ãƒ«
        st.subheader("åˆæ ¼1ä»¶ã‚ãŸã‚Šã®æ¨å®šè©¦è¡Œå›æ•°")
        
        display_df = efficiency_df[['date', 'model', 'total_cases', 'passed_cases', 'pass_rate', 'trials_per_success']].copy()
        display_df['pass_rate'] = display_df['pass_rate'].apply(lambda x: f"{x*100:.1f}%")
        display_df['trials_per_success'] = display_df['trials_per_success'].apply(
            lambda x: f"{x:.2f}" if x != float('inf') else "âˆ"
        )
        
        display_df.columns = ['æ—¥ä»˜', 'ãƒ¢ãƒ‡ãƒ«', 'ç·ã‚±ãƒ¼ã‚¹æ•°', 'åˆæ ¼æ•°', 'åˆæ ¼ç‡', 'è©¦è¡Œå›æ•°/åˆæ ¼']
        st.dataframe(display_df, use_container_width=True)
    else:
        st.info("åŠ¹ç‡æ€§ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# è©³ç´°ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
with st.expander("ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿"):
    if not df_filtered.empty:
        st.subheader("ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è©³ç´°")
        detail_df = df_filtered[['id', 'date', 'score', 'passed', 'reference', 'prediction']].copy()
        detail_df = detail_df.sort_values('date', ascending=False)
        st.dataframe(detail_df, use_container_width=True)
    
    if not weekly_df.empty:
        st.subheader("é€±æ¬¡è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(weekly_df, use_container_width=True)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("ğŸ¯ **Golden Test Dashboard** - AIãƒ¢ãƒ‡ãƒ«å“è³ªã®ç¶™ç¶šçš„ç›£è¦–")
st.markdown(f"æœ€çµ‚æ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
