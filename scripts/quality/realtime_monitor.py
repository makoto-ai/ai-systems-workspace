#!/usr/bin/env python3
"""
ğŸš€ Phase 3: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–ã‚¨ãƒ³ã‚¸ãƒ³
=====================================

ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’ç›£è¦–ã—ã€å³åº§ã«å“è³ªè©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 

ä¸»è¦æ©Ÿèƒ½:
- ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã®å³åº§æ¤œçŸ¥
- å“è³ªã‚¹ã‚³ã‚¢ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ˆç®—
- å“è³ªåŠ£åŒ–ã®è‡ªå‹•æ¤œçŸ¥
- å³åº§ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆ
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿çµ±åˆ
"""

import os
import sys
import json
import time
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import threading
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class QualityScore:
    """å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_dir = Path(".")
        self.learning_data = self._load_learning_data()
    
    def _load_learning_data(self) -> Dict[str, Any]:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists("out/continuous_learning.json"):
                with open("out/continuous_learning.json") as f:
                    return json.load(f)
        except:
            pass
        return {"patterns": {}, "thresholds": {}, "weights": {}}
    
    def calculate_file_score(self, file_path: str) -> Dict[str, Any]:
        """å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score_data = {
            "file": file_path,
            "timestamp": datetime.now().isoformat(),
            "scores": {},
            "overall_score": 0.0,
            "risk_level": "unknown",
            "issues": []
        }
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥åˆ¤å®š
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext == '.yml' or file_ext == '.yaml':
                score_data = self._score_yaml_file(file_path, score_data)
            elif file_ext == '.py':
                score_data = self._score_python_file(file_path, score_data)
            elif file_ext in ['.js', '.ts']:
                score_data = self._score_js_file(file_path, score_data)
            else:
                score_data["scores"]["general"] = 0.8  # ä¸€èˆ¬ãƒ•ã‚¡ã‚¤ãƒ«
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            if score_data["scores"]:
                score_data["overall_score"] = sum(score_data["scores"].values()) / len(score_data["scores"])
            
            # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¤å®š
            score_data["risk_level"] = self._determine_risk_level(score_data["overall_score"])
            
        except Exception as e:
            score_data["issues"].append(f"Scoring error: {e}")
            score_data["overall_score"] = 0.5
        
        return score_data
    
    def _score_yaml_file(self, file_path: str, score_data: Dict) -> Dict:
        """YAMLãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªã‚¹ã‚³ã‚¢"""
        try:
            # yamllint ãƒã‚§ãƒƒã‚¯
            result = subprocess.run(
                ["yamllint", "-f", "parsable", file_path],
                capture_output=True, text=True
            )
            
            issues = result.stdout.strip().split('\n') if result.stdout.strip() else []
            warning_count = len([i for i in issues if 'warning' in i])
            error_count = len([i for i in issues if 'error' in i])
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆã‚¨ãƒ©ãƒ¼é‡è¦–ï¼‰
            base_score = 1.0
            base_score -= error_count * 0.3
            base_score -= warning_count * 0.1
            base_score = max(0.0, min(1.0, base_score))
            
            score_data["scores"]["yaml_syntax"] = base_score
            score_data["scores"]["yaml_warnings"] = max(0.0, 1.0 - warning_count * 0.05)
            
            if issues:
                score_data["issues"].extend(issues[:5])  # æœ€å¤§5å€‹ã¾ã§
            
        except FileNotFoundError:
            score_data["scores"]["yaml_syntax"] = 0.7  # yamllintæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        
        return score_data
    
    def _score_python_file(self, file_path: str, score_data: Dict) -> Dict:
        """Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªã‚¹ã‚³ã‚¢"""
        try:
            # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
            with open(file_path, 'r') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            line_count = len(lines)
            empty_lines = sum(1 for line in lines if not line.strip())
            comment_lines = sum(1 for line in lines if line.strip().startswith('#'))
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            readability_score = min(1.0, comment_lines / max(1, line_count * 0.1))
            structure_score = 0.8 if line_count < 500 else (0.6 if line_count < 1000 else 0.4)
            
            score_data["scores"]["readability"] = readability_score
            score_data["scores"]["structure"] = structure_score
            
        except Exception as e:
            score_data["issues"].append(f"Python analysis error: {e}")
        
        return score_data
    
    def _score_js_file(self, file_path: str, score_data: Dict) -> Dict:
        """JavaScript/TypeScriptãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªã‚¹ã‚³ã‚¢"""
        try:
            with open(file_path, 'r') as f:
                content = f.read()
            
            lines = content.split('\n')
            line_count = len(lines)
            
            # åŸºæœ¬æ§‹é€ ãƒã‚§ãƒƒã‚¯
            structure_score = 0.8 if line_count < 300 else (0.6 if line_count < 600 else 0.4)
            
            score_data["scores"]["structure"] = structure_score
            
        except Exception as e:
            score_data["issues"].append(f"JS analysis error: {e}")
        
        return score_data
    
    def _determine_risk_level(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¤å®š"""
        if score >= 0.8:
            return "low"
        elif score >= 0.6:
            return "medium"
        else:
            return "high"


class RealtimeQualityHandler(FileSystemEventHandler):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.quality_scorer = QualityScore()
        self.recent_changes = []
        self.quality_history = []
        self.debounce_time = 1.0  # 1ç§’ã®ãƒ‡ãƒã‚¦ãƒ³ã‚¹
        self.last_event_time = {}
        
        # ç›£è¦–å¯¾è±¡æ‹¡å¼µå­
        self.monitored_extensions = {'.py', '.yml', '.yaml', '.js', '.ts', '.md', '.json'}
        
        # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.excluded_dirs = {'.git', 'node_modules', '__pycache__', '.pytest_cache', 'dist', 'build'}
    
    def should_monitor(self, file_path: str) -> bool:
        """ç›£è¦–å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯"""
        path = Path(file_path)
        
        # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒã‚§ãƒƒã‚¯
        for part in path.parts:
            if part in self.excluded_dirs:
                return False
        
        # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        if path.suffix.lower() in self.monitored_extensions:
            return True
        
        return False
    
    def on_modified(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†"""
        if event.is_directory:
            return
        
        file_path = event.src_path
        
        if not self.should_monitor(file_path):
            return
        
        # ãƒ‡ãƒã‚¦ãƒ³ã‚¹å‡¦ç†
        current_time = time.time()
        if file_path in self.last_event_time:
            if current_time - self.last_event_time[file_path] < self.debounce_time:
                return
        
        self.last_event_time[file_path] = current_time
        
        # å“è³ªè©•ä¾¡ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        threading.Thread(target=self._evaluate_quality, args=(file_path,), daemon=True).start()
    
    def _evaluate_quality(self, file_path: str):
        """å“è³ªè©•ä¾¡å®Ÿè¡Œ"""
        try:
            print(f"ğŸ“Š Analyzing: {file_path}")
            
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            score_data = self.quality_scorer.calculate_file_score(file_path)
            
            # å±¥æ­´ã«è¿½åŠ 
            self.quality_history.append(score_data)
            
            # æœ€æ–°100ä»¶ã¾ã§ä¿æŒ
            if len(self.quality_history) > 100:
                self.quality_history = self.quality_history[-100:]
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¡¨ç¤º
            self._display_feedback(score_data)
            
            # å“è³ªåŠ£åŒ–ãƒã‚§ãƒƒã‚¯
            self._check_quality_degradation(score_data)
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            self._save_quality_data()
            
        except Exception as e:
            print(f"âŒ Quality evaluation error for {file_path}: {e}")
    
    def _display_feedback(self, score_data: Dict):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¡¨ç¤º"""
        file_name = Path(score_data["file"]).name
        score = score_data["overall_score"]
        risk = score_data["risk_level"]
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¥ã®è¡¨ç¤º
        if risk == "high":
            print(f"ğŸš¨ HIGH RISK: {file_name} (Score: {score:.2f})")
            for issue in score_data["issues"][:3]:  # æœ€å¤§3å€‹
                print(f"   âš ï¸  {issue}")
        elif risk == "medium":
            print(f"ğŸ”¶ MEDIUM RISK: {file_name} (Score: {score:.2f})")
        else:
            print(f"âœ… LOW RISK: {file_name} (Score: {score:.2f})")
        
        # å…·ä½“çš„ãªã‚¹ã‚³ã‚¢è¡¨ç¤º
        if len(score_data["scores"]) > 1:
            print(f"   ğŸ“ˆ Details: {', '.join([f'{k}:{v:.2f}' for k, v in score_data['scores'].items()])}")
    
    def _check_quality_degradation(self, current_score: Dict):
        """å“è³ªåŠ£åŒ–ãƒã‚§ãƒƒã‚¯"""
        if len(self.quality_history) < 2:
            return
        
        # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã®éå»ã®ã‚¹ã‚³ã‚¢ã¨æ¯”è¼ƒ
        file_path = current_score["file"]
        previous_scores = [
            h for h in self.quality_history[-10:]  # ç›´è¿‘10å€‹
            if h["file"] == file_path and h["timestamp"] != current_score["timestamp"]
        ]
        
        if not previous_scores:
            return
        
        latest_previous = previous_scores[-1]
        current = current_score["overall_score"]
        previous = latest_previous["overall_score"]
        
        # å“è³ªåŠ£åŒ–é–¾å€¤: 0.2ä»¥ä¸Šã®ä½ä¸‹
        if current < previous - 0.2:
            print(f"ğŸ“‰ QUALITY DEGRADATION DETECTED!")
            print(f"   File: {Path(file_path).name}")
            print(f"   Score: {previous:.2f} â†’ {current:.2f} (Î”{current-previous:.2f})")
            print(f"   ğŸ”§ Consider reviewing recent changes")
    
    def _save_quality_data(self):
        """å“è³ªãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            os.makedirs("out", exist_ok=True)
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªãƒ‡ãƒ¼ã‚¿ä¿å­˜
            quality_summary = {
                "last_updated": datetime.now().isoformat(),
                "total_evaluations": len(self.quality_history),
                "recent_scores": self.quality_history[-10:],
                "risk_distribution": self._calculate_risk_distribution(),
                "average_score": self._calculate_average_score()
            }
            
            with open("out/realtime_quality.json", "w") as f:
                json.dump(quality_summary, f, indent=2)
                
        except Exception as e:
            print(f"âŒ Save error: {e}")
    
    def _calculate_risk_distribution(self) -> Dict[str, int]:
        """ãƒªã‚¹ã‚¯åˆ†å¸ƒè¨ˆç®—"""
        distribution = {"high": 0, "medium": 0, "low": 0}
        
        for score_data in self.quality_history[-20:]:  # ç›´è¿‘20å€‹
            risk = score_data.get("risk_level", "unknown")
            if risk in distribution:
                distribution[risk] += 1
        
        return distribution
    
    def _calculate_average_score(self) -> float:
        """å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not self.quality_history:
            return 0.0
        
        recent_scores = [s["overall_score"] for s in self.quality_history[-20:]]
        return sum(recent_scores) / len(recent_scores) if recent_scores else 0.0


class RealtimeQualityMonitor:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, watch_path: str = "."):
        self.watch_path = watch_path
        self.observer = Observer()
        self.handler = RealtimeQualityHandler()
        self.is_running = False
    
    def start(self):
        """ç›£è¦–é–‹å§‹"""
        print("ğŸš€ Realtime Quality Monitor Starting...")
        print(f"ğŸ“‚ Watching: {os.path.abspath(self.watch_path)}")
        print("âš¡ File changes will be analyzed instantly")
        print("ğŸ›‘ Press Ctrl+C to stop")
        print("-" * 50)
        
        self.observer.schedule(self.handler, self.watch_path, recursive=True)
        self.observer.start()
        self.is_running = True
        
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            self.stop()
    
    def stop(self):
        """ç›£è¦–åœæ­¢"""
        if self.is_running:
            print("\nğŸ›‘ Stopping Realtime Quality Monitor...")
            self.observer.stop()
            self.observer.join()
            self.is_running = False
            
            # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
            self._display_final_stats()
    
    def _display_final_stats(self):
        """æœ€çµ‚çµ±è¨ˆè¡¨ç¤º"""
        history = self.handler.quality_history
        
        if not history:
            print("ğŸ“Š No quality evaluations performed")
            return
        
        print(f"\nğŸ“Š Final Statistics:")
        print(f"   Total evaluations: {len(history)}")
        print(f"   Average score: {self.handler._calculate_average_score():.2f}")
        
        risk_dist = self.handler._calculate_risk_distribution()
        print(f"   Risk distribution: High:{risk_dist['high']}, Medium:{risk_dist['medium']}, Low:{risk_dist['low']}")
        
        print(f"ğŸ“„ Quality report saved: out/realtime_quality.json")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ğŸš€ Realtime Quality Monitor")
    parser.add_argument("--path", "-p", default=".", help="Path to watch (default: current directory)")
    parser.add_argument("--test", action="store_true", help="Run test mode (analyze current state and exit)")
    
    args = parser.parse_args()
    
    if args.test:
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ç¾åœ¨ã®çŠ¶æ…‹ã‚’åˆ†æã—ã¦çµ‚äº†
        print("ğŸ§ª Test Mode: Analyzing current state...")
        handler = RealtimeQualityHandler()
        
        # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚¹ãƒˆåˆ†æ
        test_files = [
            ".github/workflows/pr-quality-check.yml",
            "tests/golden/evaluator.py",
            "scripts/quality/tag_failures.py"
        ]
        
        for file_path in test_files:
            if os.path.exists(file_path):
                score_data = handler.quality_scorer.calculate_file_score(file_path)
                handler._display_feedback(score_data)
        
        handler._save_quality_data()
        print("âœ… Test analysis complete")
    else:
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–é–‹å§‹
        monitor = RealtimeQualityMonitor(args.path)
        monitor.start()


if __name__ == "__main__":
    main()



