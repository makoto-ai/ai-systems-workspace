#!/usr/bin/env python3
"""
ğŸ”® Issue Predictor - å•é¡Œäºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³
Phase 1ã§è“„ç©ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å°†æ¥ã®å•é¡Œã‚’äºˆæ¸¬
"""

import json
import os
import glob
import re
import subprocess
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import yaml

class IssuePredictionEngine:
    def __init__(self):
        self.prediction_confidence = {}
        self.risk_patterns = {}
        self.historical_data = {}
        self.phase1_insights = {}
        
        # Phase 1ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        self._load_phase1_data()
        self._load_pattern_rules()
        self._analyze_historical_trends()
        
    def predict_future_issues(self, target_files=None, prediction_days=7):
        """å°†æ¥ã®å•é¡Œã‚’äºˆæ¸¬"""
        print("ğŸ”® Issue Prediction Engine")
        print("=" * 40)
        
        predictions = {
            "timestamp": datetime.now().isoformat(),
            "prediction_horizon_days": prediction_days,
            "confidence_threshold": 0.7,
            "predicted_issues": [],
            "risk_assessment": {},
            "preventive_actions": []
        }
        
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®äºˆæ¸¬
        file_risks = self._predict_file_risks(target_files)
        
        # 2. æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®äºˆæ¸¬  
        temporal_risks = self._predict_temporal_risks(prediction_days)
        
        # 3. ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã«ã‚ˆã‚‹äºˆæ¸¬
        pattern_risks = self._predict_pattern_risks()
        
        # 4. çµ±åˆãƒªã‚¹ã‚¯è©•ä¾¡
        integrated_risks = self._integrate_risk_assessments(
            file_risks, temporal_risks, pattern_risks
        )
        
        predictions["predicted_issues"] = integrated_risks["high_risk_issues"]
        predictions["risk_assessment"] = integrated_risks["risk_summary"] 
        predictions["preventive_actions"] = self._generate_preventive_actions(
            integrated_risks
        )
        
        return predictions
    
    def _load_phase1_data(self):
        """Phase 1ã®è“„ç©ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # Learning insights ãƒ‡ãƒ¼ã‚¿
        insights_file = "out/learning_insights.json"
        if os.path.exists(insights_file):
            with open(insights_file, 'r') as f:
                self.phase1_insights = json.load(f)
        
        # System health ãƒ‡ãƒ¼ã‚¿
        health_file = "out/system_health.json"
        if os.path.exists(health_file):
            with open(health_file, 'r') as f:
                self.phase1_insights["system_health"] = json.load(f)
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
        json_files = glob.glob("out/*.json")
        self.historical_data = {}
        
        for file in json_files[-20:]:  # æœ€æ–°20ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
            try:
                with open(file, 'r') as f:
                    data = json.load(f)
                    file_key = os.path.basename(file)
                    self.historical_data[file_key] = data
            except:
                continue
    
    def _load_pattern_rules(self):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ãƒ«ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿"""
        rules_files = [
            "scripts/quality/tag_rules.yaml",
            "scripts/quality/yaml_tag_rules.yaml"
        ]
        
        self.risk_patterns = {
            "known_failures": [],
            "warning_signs": [],
            "success_patterns": []
        }
        
        for rules_file in rules_files:
            if os.path.exists(rules_file):
                try:
                    with open(rules_file, 'r') as f:
                        rules = yaml.safe_load(f)
                        for rule in rules.get('rules', []):
                            self.risk_patterns["known_failures"].append({
                                "tag": rule.get("tag"),
                                "pattern": rule.get("pattern"),
                                "confidence": 0.8
                            })
                except:
                    continue
    
    def _analyze_historical_trends(self):
        """éå»ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        # ã‚³ãƒŸãƒƒãƒˆå±¥æ­´ã‹ã‚‰ã®å‚¾å‘åˆ†æ
        try:
            result = subprocess.run([
                "git", "log", "--oneline", "--since=30 days ago"
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                commits = result.stdout.strip().split('\n')
                
                # å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
                fix_commits = [c for c in commits if 'fix' in c.lower()]
                error_commits = [c for c in commits if any(
                    word in c.lower() for word in ['error', 'warning', 'fail', 'bug']
                )]
                
                self.historical_data["commit_analysis"] = {
                    "total_commits": len([c for c in commits if c.strip()]),
                    "fix_commits": len(fix_commits),
                    "error_commits": len(error_commits),
                    "fix_frequency": len(fix_commits) / max(1, len([c for c in commits if c.strip()]))
                }
        except:
            pass
    
    def _predict_file_risks(self, target_files):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ãƒªã‚¹ã‚¯äºˆæ¸¬"""
        file_risks = {}
        
        # GitHub Actions ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å¤‰æ›´ãƒªã‚¹ã‚¯
        workflow_files = glob.glob(".github/workflows/*.yml")
        
        for workflow in workflow_files:
            risk_score = 0.0
            risk_factors = []
            
            try:
                with open(workflow, 'r') as f:
                    content = f.read()
                    
                # é«˜ãƒªã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
                if 'secrets.' in content:
                    risk_score += 0.3
                    risk_factors.append("secrets context usage")
                
                if len(content.split('\n')) > 200:
                    risk_score += 0.2
                    risk_factors.append("complex workflow (200+ lines)")
                
                if content.count('run:') > 10:
                    risk_score += 0.2
                    risk_factors.append("many run steps")
                
                if 'python' in content and 'pip install' in content:
                    risk_score += 0.1
                    risk_factors.append("dependency management")
                    
                # Phase 1ã§æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®ç…§åˆ
                for pattern in self.risk_patterns["known_failures"]:
                    if pattern["pattern"] and re.search(pattern["pattern"], content, re.IGNORECASE):
                        risk_score += 0.4
                        risk_factors.append(f"known failure pattern: {pattern['tag']}")
                
                file_risks[workflow] = {
                    "risk_score": min(risk_score, 1.0),
                    "risk_factors": risk_factors,
                    "confidence": 0.8 if risk_factors else 0.3
                }
                
            except:
                continue
        
        return file_risks
    
    def _predict_temporal_risks(self, prediction_days):
        """æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®äºˆæ¸¬"""
        temporal_risks = {
            "time_based_risks": [],
            "seasonal_patterns": {},
            "workday_patterns": {}
        }
        
        # Phase 1ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        if "fix_success_rates" in self.phase1_insights:
            fix_data = self.phase1_insights["fix_success_rates"]
            total_attempts = fix_data.get("total_attempts", 0)
            
            # ä¿®æ­£é »åº¦ã‹ã‚‰å°†æ¥ã®å•é¡Œç™ºç”Ÿã‚’äºˆæ¸¬
            if total_attempts > 0:
                daily_avg_fixes = total_attempts / 14  # 2é€±é–“ã®ãƒ‡ãƒ¼ã‚¿ã¨ä»®å®š
                predicted_issues_count = int(daily_avg_fixes * prediction_days)
                
                temporal_risks["time_based_risks"].append({
                    "type": "routine_maintenance",
                    "predicted_count": predicted_issues_count,
                    "confidence": 0.7,
                    "timeframe": f"next {prediction_days} days"
                })
        
        # é€±æœ«ãƒ»å¹³æ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        current_day = datetime.now().weekday()
        if current_day >= 5:  # åœŸæ—¥
            temporal_risks["workday_patterns"]["weekend_risk"] = {
                "risk_level": "low",
                "reason": "reduced development activity"
            }
        else:
            temporal_risks["workday_patterns"]["weekday_risk"] = {
                "risk_level": "medium",
                "reason": "active development period"
            }
        
        return temporal_risks
    
    def _predict_pattern_risks(self):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã«ã‚ˆã‚‹äºˆæ¸¬"""
        pattern_risks = {
            "high_probability": [],
            "medium_probability": [],
            "emerging_patterns": []
        }
        
        # Phase 1ã§å­¦ç¿’ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        if "pattern_recognition" in self.phase1_insights:
            pattern_data = self.phase1_insights["pattern_recognition"]
            pattern_count = pattern_data.get("recognized_patterns", 0)
            
            if pattern_count < 10:
                pattern_risks["emerging_patterns"].append({
                    "type": "pattern_insufficiency",
                    "description": "Low pattern recognition coverage",
                    "risk_level": "medium",
                    "mitigation": "Expand tag_rules.yaml"
                })
        
        # æˆåŠŸç‡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ãƒªã‚¹ã‚¯äºˆæ¸¬
        if "fix_success_rates" in self.phase1_insights:
            fix_rates = self.phase1_insights["fix_success_rates"]
            
            # YAMLä¿®æ­£ã®é »åº¦ã‹ã‚‰å°†æ¥ã®YAMLå•é¡Œã‚’äºˆæ¸¬
            yaml_fixes = fix_rates.get("yaml_fixes", 0)
            if yaml_fixes > 2:
                pattern_risks["high_probability"].append({
                    "type": "yaml_syntax_issues",
                    "description": "Frequent YAML fixes indicate ongoing syntax challenges",
                    "probability": 0.75,
                    "impact": "medium"
                })
            
            # Contextè­¦å‘Šã®é »åº¦
            context_warnings = fix_rates.get("context_warnings", 0)
            if context_warnings > 5:
                pattern_risks["high_probability"].append({
                    "type": "github_actions_context_issues", 
                    "description": "High context warning frequency suggests template issues",
                    "probability": 0.85,
                    "impact": "low"
                })
        
        return pattern_risks
    
    def _integrate_risk_assessments(self, file_risks, temporal_risks, pattern_risks):
        """ãƒªã‚¹ã‚¯è©•ä¾¡ã®çµ±åˆ"""
        integrated = {
            "high_risk_issues": [],
            "medium_risk_issues": [],
            "low_risk_issues": [],
            "risk_summary": {}
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ã‚¯ã®çµ±åˆ
        for file, risk_data in file_risks.items():
            risk_level = "high" if risk_data["risk_score"] > 0.7 else \
                        "medium" if risk_data["risk_score"] > 0.4 else "low"
            
            issue = {
                "type": "file_risk",
                "target": file,
                "risk_score": risk_data["risk_score"],
                "factors": risk_data["risk_factors"],
                "confidence": risk_data["confidence"]
            }
            
            integrated[f"{risk_level}_risk_issues"].append(issue)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ã‚¯ã®çµ±åˆ
        for risk in pattern_risks["high_probability"]:
            integrated["high_risk_issues"].append({
                "type": "pattern_risk",
                "description": risk["description"],
                "probability": risk["probability"],
                "impact": risk["impact"]
            })
        
        # æ™‚ç³»åˆ—ãƒªã‚¹ã‚¯ã®çµ±åˆ
        for risk in temporal_risks["time_based_risks"]:
            integrated["medium_risk_issues"].append({
                "type": "temporal_risk",
                "description": f"Predicted {risk['predicted_count']} issues in {risk['timeframe']}",
                "confidence": risk["confidence"]
            })
        
        # ã‚µãƒãƒªãƒ¼ä½œæˆ
        integrated["risk_summary"] = {
            "total_high_risk": len(integrated["high_risk_issues"]),
            "total_medium_risk": len(integrated["medium_risk_issues"]),
            "total_low_risk": len(integrated["low_risk_issues"]),
            "overall_risk_level": self._calculate_overall_risk(integrated)
        }
        
        return integrated
    
    def _calculate_overall_risk(self, integrated):
        """å…¨ä½“ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—"""
        high_count = len(integrated["high_risk_issues"])
        medium_count = len(integrated["medium_risk_issues"])
        
        if high_count > 3:
            return "high"
        elif high_count > 0 or medium_count > 5:
            return "medium"
        else:
            return "low"
    
    def _generate_preventive_actions(self, integrated_risks):
        """äºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç”Ÿæˆ"""
        actions = []
        
        # é«˜ãƒªã‚¹ã‚¯å•é¡Œã¸ã®å¯¾å¿œ
        for issue in integrated_risks["high_risk_issues"]:
            if issue["type"] == "file_risk":
                actions.append({
                    "priority": "high",
                    "action": f"Review and refactor {issue['target']}",
                    "reason": ", ".join(issue["factors"]),
                    "estimated_effort": "30 minutes"
                })
            
            elif issue["type"] == "pattern_risk":
                if "yaml" in issue["description"].lower():
                    actions.append({
                        "priority": "high", 
                        "action": "Implement YAML validation in pre-commit hooks",
                        "reason": issue["description"],
                        "estimated_effort": "45 minutes"
                    })
                
                elif "context" in issue["description"].lower():
                    actions.append({
                        "priority": "medium",
                        "action": "Create GitHub Actions context usage guidelines",
                        "reason": issue["description"],
                        "estimated_effort": "20 minutes"
                    })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸è¶³ã¸ã®å¯¾å¿œ
        if integrated_risks["risk_summary"]["total_high_risk"] + integrated_risks["risk_summary"]["total_medium_risk"] < 3:
            actions.append({
                "priority": "low",
                "action": "Expand pattern recognition rules",
                "reason": "Low risk diversity suggests insufficient pattern coverage",
                "estimated_effort": "15 minutes"
            })
        
        return actions
    
    def generate_prediction_report(self, target_files=None, prediction_days=7):
        """äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        predictions = self.predict_future_issues(target_files, prediction_days)
        
        print(f"\nğŸ“… Prediction Horizon: {prediction_days} days")
        print(f"ğŸ¯ Analysis Timestamp: {predictions['timestamp']}")
        print()
        
        # ãƒªã‚¹ã‚¯ã‚µãƒãƒªãƒ¼
        risk_summary = predictions["risk_assessment"]
        overall_risk = risk_summary["overall_risk_level"]
        
        risk_icon = "ğŸ”´" if overall_risk == "high" else "ğŸŸ¡" if overall_risk == "medium" else "ğŸŸ¢"
        print(f"ğŸ¯ Overall Risk Level: {risk_icon} {overall_risk.upper()}")
        print()
        
        print(f"ğŸ“Š Risk Distribution:")
        print(f"  High Risk Issues: {risk_summary['total_high_risk']}")
        print(f"  Medium Risk Issues: {risk_summary['total_medium_risk']}")  
        print(f"  Low Risk Issues: {risk_summary['total_low_risk']}")
        print()
        
        # ä¸»è¦ãªäºˆæ¸¬å•é¡Œ
        if predictions["predicted_issues"]:
            print("ğŸš¨ High Priority Predictions:")
            print("-" * 35)
            for i, issue in enumerate(predictions["predicted_issues"][:3], 1):
                print(f"{i}. {issue.get('description', issue.get('target', 'Unknown issue'))}")
                if 'factors' in issue:
                    print(f"   Factors: {', '.join(issue['factors'][:2])}")
                if 'probability' in issue:
                    print(f"   Probability: {issue['probability']:.1%}")
                print()
        
        # æ¨å¥¨äºˆé˜²ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if predictions["preventive_actions"]:
            print("ğŸ›¡ï¸ Recommended Preventive Actions:")
            print("-" * 38)
            for i, action in enumerate(predictions["preventive_actions"][:3], 1):
                priority_icon = "ğŸ”´" if action["priority"] == "high" else "ğŸŸ¡" if action["priority"] == "medium" else "ğŸŸ¢"
                print(f"{i}. {priority_icon} {action['action']}")
                print(f"   Effort: {action['estimated_effort']}")
                print(f"   Reason: {action['reason']}")
                print()
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        os.makedirs("out", exist_ok=True)
        with open("out/issue_predictions.json", 'w') as f:
            json.dump(predictions, f, indent=2)
        
        print(f"ğŸ“„ Detailed predictions saved: out/issue_predictions.json")
        
        return predictions

def main():
    predictor = IssuePredictionEngine()
    predictor.generate_prediction_report(prediction_days=7)

if __name__ == "__main__":
    main()


