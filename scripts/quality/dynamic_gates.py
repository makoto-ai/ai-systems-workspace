#!/usr/bin/env python3
"""
ğŸ›ï¸ Phase 3: å‹•çš„å“è³ªã‚²ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
===============================

å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å‹•çš„ã«å“è³ªåŸºæº–ã‚’èª¿æ•´ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 

ä¸»è¦æ©Ÿèƒ½:
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å‹•çš„é–¾å€¤èª¿æ•´
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¾å­˜ã®å“è³ªåŸºæº–
- ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ãƒ»é‡è¦åº¦åˆ¥ã®é©å¿œçš„ã‚²ãƒ¼ãƒˆ
- æ®µéšçš„å“è³ªãƒã‚§ãƒƒã‚¯
- ç¶™ç¶šå­¦ç¿’ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š
"""

import os
import sys
import json
import statistics
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import re


@dataclass
class QualityGate:
    """å“è³ªã‚²ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    min_score: float
    warning_score: float
    context: str
    file_pattern: str
    importance_level: int  # 1-5 (5ãŒæœ€é‡è¦)
    learning_weight: float
    last_updated: datetime


class ContextAnalyzer:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.project_patterns = {
            "critical": [
                r"\.github/workflows/",
                r"scripts/quality/",
                r"tests/golden/",
                r"package\.json",
                r"requirements\.txt",
                r"Dockerfile",
                r"docker-compose\.yml"
            ],
            "important": [
                r"src/",
                r"lib/",
                r"components/",
                r"services/",
                r"api/",
                r"\.py$",
                r"\.ts$",
                r"\.js$"
            ],
            "standard": [
                r"docs/",
                r"examples/",
                r"\.md$",
                r"\.txt$"
            ],
            "experimental": [
                r"experimental/",
                r"test/",
                r"demo/",
                r"sandbox/"
            ]
        }
    
    def analyze_file_context(self, file_path: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"""
        path = Path(file_path)
        
        # é‡è¦åº¦ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        importance = self._determine_importance(file_path)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥åˆ¤å®š
        file_type = self._determine_file_type(path)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ®µéšåˆ¤å®š
        project_phase = self._determine_project_phase()
        
        # å¤‰æ›´å±¥æ­´åˆ†æ
        change_frequency = self._analyze_change_frequency(file_path)
        
        return {
            "file_path": file_path,
            "importance_level": importance,
            "file_type": file_type,
            "project_phase": project_phase,
            "change_frequency": change_frequency,
            "size_category": self._categorize_file_size(path),
            "complexity_hints": self._analyze_complexity_hints(path)
        }
    
    def _determine_importance(self, file_path: str) -> int:
        """é‡è¦åº¦ãƒ¬ãƒ™ãƒ«åˆ¤å®š (1-5)"""
        for level, patterns in [
            (5, self.project_patterns["critical"]),
            (4, self.project_patterns["important"]), 
            (3, self.project_patterns["standard"]),
            (2, self.project_patterns["experimental"])
        ]:
            for pattern in patterns:
                if re.search(pattern, file_path):
                    return level
        return 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _determine_file_type(self, path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥åˆ¤å®š"""
        ext = path.suffix.lower()
        
        type_mapping = {
            ".py": "python",
            ".js": "javascript", 
            ".ts": "typescript",
            ".yml": "yaml", ".yaml": "yaml",
            ".json": "json",
            ".md": "markdown",
            ".sh": "shell",
            ".dockerfile": "docker"
        }
        
        return type_mapping.get(ext, "other")
    
    def _determine_project_phase(self) -> str:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ®µéšåˆ¤å®š"""
        # ç°¡å˜ãªåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€ãƒ–ãƒ©ãƒ³ãƒåã‚„ã‚¿ã‚°ãªã©ã‹ã‚‰åˆ¤å®š
        
        if os.path.exists(".github/workflows/deploy.yml"):
            return "production"
        elif os.path.exists(".github/workflows/pr-quality-check.yml"):
            return "staging"
        else:
            return "development"
    
    def _analyze_change_frequency(self, file_path: str) -> str:
        """å¤‰æ›´é »åº¦åˆ†æ"""
        # ç°¡æ˜“å®Ÿè£…: å®Ÿéš›ã¯git logã‚’åˆ†æ
        try:
            import git
            repo = git.Repo(".")
            commits = list(repo.iter_commits(paths=file_path, max_count=10))
            
            if len(commits) >= 5:
                return "high"
            elif len(commits) >= 2:
                return "medium"
            else:
                return "low"
        except:
            return "unknown"
    
    def _categorize_file_size(self, path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚«ãƒ†ã‚´ãƒª"""
        try:
            size = path.stat().st_size
            
            if size < 1024:  # 1KB
                return "tiny"
            elif size < 10240:  # 10KB
                return "small"
            elif size < 51200:  # 50KB
                return "medium"
            elif size < 102400:  # 100KB
                return "large"
            else:
                return "very_large"
        except:
            return "unknown"
    
    def _analyze_complexity_hints(self, path: Path) -> List[str]:
        """è¤‡é›‘æ€§ãƒ’ãƒ³ãƒˆåˆ†æ"""
        hints = []
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            lines = content.split('\n')
            
            # è¡Œæ•°ãƒã‚§ãƒƒã‚¯
            if len(lines) > 500:
                hints.append("long_file")
            
            # ãƒã‚¹ãƒˆæ·±åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ï¼‰
            max_indent = 0
            for line in lines:
                if line.strip():
                    indent = len(line) - len(line.lstrip())
                    max_indent = max(max_indent, indent)
            
            if max_indent > 16:  # 4ãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒã‚¹ãƒˆ
                hints.append("deep_nesting")
            
            # è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            if re.search(r'for.*for.*for', content):
                hints.append("nested_loops")
            
            if content.count('if') > 10:
                hints.append("many_conditions")
                
        except:
            pass
        
        return hints


class LearningEngine:
    """å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.learning_data = self._load_learning_data()
        self.performance_history = self._load_performance_history()
    
    def _load_learning_data(self) -> Dict[str, Any]:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists("out/gate_learning.json"):
                with open("out/gate_learning.json") as f:
                    return json.load(f)
        except:
            pass
        
        return {
            "file_type_stats": {},
            "importance_stats": {},
            "size_stats": {},
            "phase_stats": {},
            "threshold_adjustments": [],
            "performance_metrics": []
        }
    
    def _load_performance_history(self) -> List[Dict]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´èª­ã¿è¾¼ã¿"""
        history = []
        
        try:
            if os.path.exists("out/feedback_history.json"):
                with open("out/feedback_history.json") as f:
                    for line in f:
                        if line.strip():
                            history.append(json.loads(line))
        except:
            pass
        
        return history
    
    def calculate_dynamic_thresholds(self, context: Dict[str, Any]) -> Dict[str, float]:
        """å‹•çš„é–¾å€¤è¨ˆç®—"""
        base_thresholds = {
            "critical": 0.3,
            "warning": 0.6,
            "target": 0.8
        }
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®èª¿æ•´
        adjustments = self._calculate_context_adjustments(context)
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´
        learning_adjustments = self._calculate_learning_adjustments(context)

        # ãƒ‰ãƒªãƒ•ãƒˆè€æ€§ï¼ˆåˆ†ä½ç‚¹ï¼‹EWMAï¼‰èª¿æ•´
        drift_adjustments = self._calculate_drift_adjustments(context, base_thresholds)
        
        # æœ€çµ‚é–¾å€¤è¨ˆç®—
        final_thresholds = {}
        for key, base in base_thresholds.items():
            adjustment = (
                adjustments.get(key, 0)
                + learning_adjustments.get(key, 0)
                + drift_adjustments.get(key, 0)
            )
            final_thresholds[key] = max(0.0, min(1.0, base + adjustment))
        
        return final_thresholds
    
    def _calculate_context_adjustments(self, context: Dict[str, Any]) -> Dict[str, float]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹èª¿æ•´è¨ˆç®—"""
        adjustments = {"critical": 0, "warning": 0, "target": 0}
        
        # é‡è¦åº¦ãƒ¬ãƒ™ãƒ«èª¿æ•´
        importance = context["importance_level"]
        if importance >= 5:  # æœ€é‡è¦
            adjustments["critical"] += 0.1
            adjustments["warning"] += 0.1
            adjustments["target"] += 0.1
        elif importance <= 2:  # ä½é‡è¦åº¦
            adjustments["critical"] -= 0.1
            adjustments["warning"] -= 0.1
            
        # å½±éŸ¿åŠå¾„ï¼ˆimpact radiusï¼‰: é‡è¦åº¦Ã—å¤‰æ›´é »åº¦ã§é‡ã¿ä»˜ã‘
        impact_radius = 0.0
        freq = context.get("change_frequency", "unknown")
        if freq == "high":
            impact_radius += 0.05
        elif freq == "medium":
            impact_radius += 0.02
        if context["importance_level"] >= 4:
            impact_radius += 0.05
        adjustments["warning"] += impact_radius
        adjustments["target"] += impact_radius / 2

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ®µéšèª¿æ•´
        phase = context["project_phase"]
        if phase == "production":
            adjustments["critical"] += 0.15
            adjustments["warning"] += 0.15
            adjustments["target"] += 0.1
        elif phase == "development":
            adjustments["critical"] -= 0.05
            adjustments["warning"] -= 0.05
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºèª¿æ•´
        size = context["size_category"]
        if size in ["large", "very_large"]:
            adjustments["target"] += 0.05  # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã¯é«˜å“è³ªã‚’æ±‚ã‚ã‚‹
        elif size == "tiny":
            adjustments["warning"] -= 0.1  # å°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã¯ç·©å’Œ
            
        # è¤‡é›‘æ€§èª¿æ•´
        complexity_hints = context["complexity_hints"]
        if "long_file" in complexity_hints:
            adjustments["target"] += 0.1
        if "deep_nesting" in complexity_hints:
            adjustments["warning"] += 0.05
            
        return adjustments

    def _calculate_drift_adjustments(self, context: Dict[str, Any], base_thresholds: Dict[str, float]) -> Dict[str, float]:
        """ãƒ‰ãƒªãƒ•ãƒˆè€æ€§ã®ãŸã‚ã®åˆ†ä½ç‚¹ï¼‹EWMAã«ã‚ˆã‚‹è‡ªå‹•èª¿æ•´
        - é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›´è¿‘ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‹ã‚‰åˆ†ä½ç‚¹ï¼ˆq10, q40, q75ï¼‰ã‚’ç®—å‡º
        - ãã‚Œã‚’ã‚¢ãƒ³ã‚«ãƒ¼ã«ã—ã€(anchor - base) ã‚’èª¿æ•´é‡ã¨ã™ã‚‹
        - å‰å›èª¿æ•´ï¼ˆåŒã‚¿ã‚¤ãƒ—ï¼‰ã¨ã®EWMAã§å¹³æ»‘åŒ–
        """
        try:
            similar = self._find_similar_files(context)
            if not similar:
                return {"critical": 0.0, "warning": 0.0, "target": 0.0}

            scores = sorted([r.get("score", 0.0) for r in similar])
            if len(scores) < 5:
                return {"critical": 0.0, "warning": 0.0, "target": 0.0}

            def quantile(arr, q):
                idx = max(0, min(len(arr) - 1, int(q * (len(arr) - 1))))
                return float(arr[idx])

            q10 = quantile(scores, 0.10)
            q40 = quantile(scores, 0.40)
            q75 = quantile(scores, 0.75)

            # ã‚¢ãƒ³ã‚«ãƒ¼ï¼ˆéå»å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ï¼‰
            anchors = {
                "critical": max(0.05, min(0.7, q10 + 0.05)),
                "warning": max(0.2, min(0.85, q40)),
                "target": max(0.5, min(0.95, q75)),
            }

            raw_adjustments = {
                key: anchors[key] - base_thresholds[key] for key in anchors
            }

            # EWMAå¹³æ»‘åŒ–ï¼ˆåŒãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã®ç›´è¿‘èª¿æ•´ã¨ãƒ–ãƒ¬ãƒ³ãƒ‰ï¼‰
            ft = context.get("file_type", "other")
            recent = [rec for rec in self.learning_data.get("threshold_adjustments", [])
                      if rec.get("context", {}).get("file_type") == ft]
            if recent:
                last = recent[-1].get("thresholds", {})
                smoothed = {}
                alpha = 0.5
                for k in ["critical", "warning", "target"]:
                    prev_delta = last.get(k, base_thresholds[k]) - base_thresholds[k]
                    smoothed[k] = alpha * raw_adjustments[k] + (1 - alpha) * prev_delta
                return smoothed

            return raw_adjustments

        except Exception:
            return {"critical": 0.0, "warning": 0.0, "target": 0.0}
    
    def _calculate_learning_adjustments(self, context: Dict[str, Any]) -> Dict[str, float]:
        """å­¦ç¿’ãƒ™ãƒ¼ã‚¹èª¿æ•´è¨ˆç®—"""
        adjustments = {"critical": 0, "warning": 0, "target": 0}
        
        if not self.performance_history:
            return adjustments
        
        # é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«ã®éå»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        similar_files = self._find_similar_files(context)
        
        if similar_files:
            scores = [f["score"] for f in similar_files]
            avg_score = statistics.mean(scores)
            
            # å¹³å‡ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆã¯é–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼ˆç¾å®Ÿçš„ãªåŸºæº–ã«ï¼‰
            if avg_score < 0.6:
                adjustments["critical"] -= 0.05
                adjustments["warning"] -= 0.05
            elif avg_score > 0.8:
                adjustments["target"] += 0.05
                
        return adjustments
    
    def _find_similar_files(self, context: Dict[str, Any]) -> List[Dict]:
        """é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        similar = []
        
        for record in self.performance_history[-50:]:  # æœ€æ–°50ä»¶
            similarity_score = 0
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ãŒåŒã˜
            if self._get_file_type(record.get("file", "")) == context["file_type"]:
                similarity_score += 2
            
            # é‡è¦åº¦ãŒè¿‘ã„
            record_importance = self._estimate_importance(record.get("file", ""))
            if abs(record_importance - context["importance_level"]) <= 1:
                similarity_score += 1
            
            # é–¾å€¤ä»¥ä¸Šã§é¡ä¼¼ã¨ã¿ãªã™
            if similarity_score >= 2:
                similar.append(record)
                
        return similar[-10:]  # æœ€æ–°10ä»¶ã¾ã§
    
    def _get_file_type(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å–å¾—ï¼ˆå±¥æ­´ãƒ¬ã‚³ãƒ¼ãƒ‰ç”¨ï¼‰"""
        ext = Path(file_path).suffix.lower()
        return ext
    
    def _estimate_importance(self, file_path: str) -> int:
        """é‡è¦åº¦æ¨å®šï¼ˆå±¥æ­´ãƒ¬ã‚³ãƒ¼ãƒ‰ç”¨ï¼‰"""
        analyzer = ContextAnalyzer()
        return analyzer._determine_importance(file_path)
    
    def update_learning_data(self, context: Dict, thresholds: Dict, actual_score: float, gate_result: Dict):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ›´æ–°"""
        learning_record = {
            "timestamp": datetime.now().isoformat(),
            "context": context,
            "thresholds": thresholds,
            "actual_score": actual_score,
            "gate_result": gate_result
        }
        
        self.learning_data["threshold_adjustments"].append(learning_record)
        
        # æœ€æ–°1000ä»¶ã¾ã§ä¿æŒ
        if len(self.learning_data["threshold_adjustments"]) > 1000:
            self.learning_data["threshold_adjustments"] = self.learning_data["threshold_adjustments"][-1000:]
        
        # çµ±è¨ˆæ›´æ–°
        self._update_statistics(context, actual_score)
        
        # ä¿å­˜
        self._save_learning_data()
    
    def _update_statistics(self, context: Dict, score: float):
        """çµ±è¨ˆæƒ…å ±æ›´æ–°"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—çµ±è¨ˆ
        file_type = context["file_type"]
        if file_type not in self.learning_data["file_type_stats"]:
            self.learning_data["file_type_stats"][file_type] = {"count": 0, "total_score": 0}
        
        stats = self.learning_data["file_type_stats"][file_type]
        stats["count"] += 1
        stats["total_score"] += score
        stats["avg_score"] = stats["total_score"] / stats["count"]
    
    def _save_learning_data(self):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            os.makedirs("out", exist_ok=True)
            with open("out/gate_learning.json", "w") as f:
                json.dump(self.learning_data, f, indent=2)
        except Exception as e:
            print(f"Learning data save error: {e}")


class DynamicQualityGate:
    """å‹•çš„å“è³ªã‚²ãƒ¼ãƒˆãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.context_analyzer = ContextAnalyzer()
        self.learning_engine = LearningEngine()
        self.gates = self._initialize_gates()
    
    def _initialize_gates(self) -> List[QualityGate]:
        """åˆæœŸã‚²ãƒ¼ãƒˆè¨­å®š"""
        return [
            QualityGate(
                name="critical_files",
                min_score=0.4,
                warning_score=0.7,
                context="critical",
                file_pattern=r"\.github/|scripts/quality/",
                importance_level=5,
                learning_weight=0.8,
                last_updated=datetime.now()
            ),
            QualityGate(
                name="source_code",
                min_score=0.3,
                warning_score=0.6,
                context="source",
                file_pattern=r"\.(py|js|ts)$",
                importance_level=4,
                learning_weight=0.7,
                last_updated=datetime.now()
            ),
            QualityGate(
                name="configuration",
                min_score=0.5,
                warning_score=0.8,
                context="config",
                file_pattern=r"\.(yml|yaml|json)$",
                importance_level=4,
                learning_weight=0.6,
                last_updated=datetime.now()
            ),
            QualityGate(
                name="documentation",
                min_score=0.2,
                warning_score=0.5,
                context="docs",
                file_pattern=r"\.md$",
                importance_level=2,
                learning_weight=0.3,
                last_updated=datetime.now()
            )
        ]
    
    def evaluate_file(self, file_path: str, quality_score: float) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«è©•ä¾¡"""
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
        context = self.context_analyzer.analyze_file_context(file_path)
        
        # å‹•çš„é–¾å€¤è¨ˆç®—
        thresholds = self.learning_engine.calculate_dynamic_thresholds(context)
        
        # é©ç”¨å¯èƒ½ã‚²ãƒ¼ãƒˆæ¤œç´¢
        applicable_gate = self._find_applicable_gate(file_path)
        
        # ã‚²ãƒ¼ãƒˆè©•ä¾¡
        gate_result = self._evaluate_against_gate(
            quality_score, thresholds, applicable_gate, context
        )
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        self.learning_engine.update_learning_data(
            context, thresholds, quality_score, gate_result
        )
        
        return gate_result
    
    def _find_applicable_gate(self, file_path: str) -> Optional[QualityGate]:
        """é©ç”¨å¯èƒ½ã‚²ãƒ¼ãƒˆæ¤œç´¢"""
        for gate in self.gates:
            if re.search(gate.file_pattern, file_path):
                return gate
        return None
    
    def _evaluate_against_gate(self, score: float, thresholds: Dict, 
                              gate: Optional[QualityGate], context: Dict) -> Dict[str, Any]:
        """ã‚²ãƒ¼ãƒˆè©•ä¾¡å®Ÿè¡Œ"""
        result = {
            "file": context["file_path"],
            "score": score,
            "context": context,
            "thresholds": thresholds,
            "gate_name": gate.name if gate else "default",
            "timestamp": datetime.now().isoformat()
        }
        
        # ã‚²ãƒ¼ãƒˆåˆ¤å®š
        if score < thresholds["critical"]:
            result["status"] = "blocked"
            result["level"] = "critical" 
            result["message"] = f"Critical quality issue (score: {score:.2f} < {thresholds['critical']:.2f})"
            result["action"] = "immediate_fix_required"
            
        elif score < thresholds["warning"]:
            result["status"] = "warning"
            result["level"] = "warning"
            result["message"] = f"Quality warning (score: {score:.2f} < {thresholds['warning']:.2f})"
            result["action"] = "improvement_recommended"
            
        elif score < thresholds["target"]:
            result["status"] = "passed_with_notes"
            result["level"] = "info"
            result["message"] = f"Acceptable quality (score: {score:.2f} < target: {thresholds['target']:.2f})"
            result["action"] = "monitoring"
            
        else:
            result["status"] = "passed"
            result["level"] = "success"
            result["message"] = f"High quality (score: {score:.2f} >= {thresholds['target']:.2f})"
            result["action"] = "none"
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        result["recommendations"] = self._generate_recommendations(result)
        
        return result
    
    def _generate_recommendations(self, result: Dict) -> List[str]:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        recommendations = []
        
        context = result["context"]
        score = result["score"]
        level = result["level"]
        
        if level == "critical":
            recommendations.append("ğŸš¨ Immediate attention required")
            recommendations.append("Consider rollback if this is a regression")
            
            if context["importance_level"] >= 4:
                recommendations.append("This is a critical file - review carefully")
            
        elif level == "warning":
            recommendations.append("âš ï¸ Quality improvement needed")
            
            complexity_hints = context["complexity_hints"]
            if "long_file" in complexity_hints:
                recommendations.append("Consider breaking down this large file")
            if "deep_nesting" in complexity_hints:
                recommendations.append("Reduce nesting complexity")
                
        elif level == "info":
            recommendations.append("â„¹ï¸ Minor improvements possible")
            
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥æ¨å¥¨
        file_type = context["file_type"]
        if file_type == "yaml" and score < 0.8:
            recommendations.append("Run yamllint for specific issues")
        elif file_type == "python" and score < 0.7:
            recommendations.append("Consider adding docstrings and comments")
            
        return recommendations[:3]  # æœ€å¤§3ã¤ã¾ã§
    
    def get_gate_summary(self) -> Dict[str, Any]:
        """ã‚²ãƒ¼ãƒˆã‚µãƒãƒªãƒ¼å–å¾—"""
        return {
            "total_gates": len(self.gates),
            "learning_data_points": len(self.learning_engine.learning_data.get("threshold_adjustments", [])),
            "file_type_coverage": len(self.learning_engine.learning_data.get("file_type_stats", {})),
            "last_updated": max([g.last_updated for g in self.gates]).isoformat(),
            "gate_names": [g.name for g in self.gates]
        }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ğŸ›ï¸ Dynamic Quality Gates")
    parser.add_argument("--file", "-f", help="File to evaluate")
    parser.add_argument("--score", "-s", type=float, default=0.75, help="Quality score")
    parser.add_argument("--summary", action="store_true", help="Show gate summary")
    parser.add_argument("--test", action="store_true", help="Run test mode")
    
    args = parser.parse_args()
    
    gate_system = DynamicQualityGate()
    
    if args.summary:
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        summary = gate_system.get_gate_summary()
        print("ğŸ›ï¸ Dynamic Quality Gates Summary")
        print("=" * 40)
        for key, value in summary.items():
            print(f"{key}: {value}")
            
    elif args.test:
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰
        print("ğŸ§ª Dynamic Quality Gates Test")
        print("=" * 35)
        
        test_files = [
            {"file": ".github/workflows/pr-quality-check.yml", "score": 0.9},
            {"file": "scripts/quality/tag_failures.py", "score": 0.6},
            {"file": "README.md", "score": 0.4},
            {"file": "experimental/test.py", "score": 0.3}
        ]
        
        for test_case in test_files:
            print(f"\\nTesting: {test_case['file']}")
            result = gate_system.evaluate_file(test_case["file"], test_case["score"])
            
            print(f"Status: {result['status']} ({result['level']})")
            print(f"Message: {result['message']}")
            print(f"Action: {result['action']}")
            
            if result["recommendations"]:
                print("Recommendations:")
                for rec in result["recommendations"]:
                    print(f"  â€¢ {rec}")
                    
    elif args.file:
        # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«è©•ä¾¡
        result = gate_system.evaluate_file(args.file, args.score)
        
        print(f"ğŸ›ï¸ Quality Gate Result for {args.file}")
        print("=" * 50)
        print(f"Score: {result['score']:.2f}")
        print(f"Status: {result['status']} ({result['level']})")
        print(f"Gate: {result['gate_name']}")
        print(f"Message: {result['message']}")
        print(f"Action: {result['action']}")
        
        if result["recommendations"]:
            print("\\nRecommendations:")
            for rec in result["recommendations"]:
                print(f"  â€¢ {rec}")
        
        # é–¾å€¤æƒ…å ±
        thresholds = result["thresholds"]
        print(f"\\nDynamic Thresholds:")
        print(f"  Critical: {thresholds['critical']:.2f}")
        print(f"  Warning:  {thresholds['warning']:.2f}")
        print(f"  Target:   {thresholds['target']:.2f}")
        
    else:
        print("âŒ Please specify --file, --summary, or --test")
        parser.print_help()


if __name__ == "__main__":
    main()




