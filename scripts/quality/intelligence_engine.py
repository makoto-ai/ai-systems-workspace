#!/usr/bin/env python3
"""
ğŸ¤– Phase 4: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿çµ±åˆçŸ¥èƒ½åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
===================================

å…¨Phaseã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆAIåˆ†æã—ã€ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬ã‚’è¡Œã„ã€
ç¶™ç¶šå­¦ç¿’ã«ã‚ˆã‚‹è‡ªå·±é€²åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’æä¾›ã™ã‚‹ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ãƒ»ã‚¨ãƒ³ã‚¸ãƒ³

ä¸»è¦æ©Ÿèƒ½:
- å…¨ãƒ•ã‚§ãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿çµ±åˆåˆ†æ
- AIãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³
- äºˆæ¸¬ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- ç¶™ç¶šå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
- è‡ªå·±é€²åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
"""

import os
import sys
import json
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import statistics
import re
import pickle
from collections import defaultdict, Counter


@dataclass
class LearningPattern:
    """å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    pattern_id: str
    pattern_type: str  # "improvement", "regression", "cyclical", "anomaly"
    confidence_score: float
    frequency: int
    impact_score: float
    conditions: Dict[str, Any]
    outcomes: Dict[str, Any]
    discovered_at: datetime
    last_seen: datetime


@dataclass  
class IntelligenceInsight:
    """çŸ¥èƒ½åˆ†ææ´å¯Ÿãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    insight_id: str
    insight_type: str  # "prediction", "recommendation", "warning", "opportunity"
    title: str
    description: str
    evidence: List[str]
    confidence: float
    actionable_items: List[str]
    expected_impact: float
    time_sensitivity: str  # "immediate", "short_term", "medium_term", "long_term"
    generated_at: datetime


class DataUnificationEngine:
    """ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.data_sources = {
            "phase1": [
                "out/learning_insights.json",
                "out/system_health.json"
            ],
            "phase2": [
                "out/issue_predictions.json", 
                "out/preventive_fixes.json"
            ],
            "phase3": [
                "out/realtime_quality.json",
                "out/feedback_history.json",
                "out/gate_learning.json",
                "out/auto_guard_learning.json"
            ],
            "phase4": [
                "out/intelligent_optimization.json",
                "out/adaptive_guidance.json",
                "out/project_optimization.json"
            ]
        }
        
        self.unified_schema = {
            "timestamp": "datetime",
            "source_phase": "string",
            "data_type": "string",
            "quality_metrics": "dict",
            "improvement_actions": "list",
            "success_indicators": "dict",
            "failure_patterns": "list",
            "context_metadata": "dict"
        }
    
    def unify_all_data(self) -> Dict[str, Any]:
        """å…¨ãƒ‡ãƒ¼ã‚¿çµ±åˆ"""
        print("ğŸ¤– å…¨ãƒ•ã‚§ãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        unified_data = {
            "metadata": {
                "unification_timestamp": datetime.now().isoformat(),
                "total_sources": 0,
                "successful_loads": 0,
                "data_quality_score": 0.0
            },
            "phase1_data": [],
            "phase2_data": [],
            "phase3_data": [],
            "phase4_data": [],
            "time_series": [],
            "quality_metrics_timeline": [],
            "improvement_actions_history": [],
            "pattern_indicators": {}
        }
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
        for phase, sources in self.data_sources.items():
            phase_data = []
            for source in sources:
                data = self._load_and_normalize_data(source, phase)
                if data:
                    phase_data.extend(data if isinstance(data, list) else [data])
                    unified_data["metadata"]["successful_loads"] += 1
                unified_data["metadata"]["total_sources"] += 1
            
            unified_data[f"{phase}_data"] = phase_data
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        unified_data["time_series"] = self._build_time_series(unified_data)
        unified_data["quality_metrics_timeline"] = self._extract_quality_timeline(unified_data)
        unified_data["improvement_actions_history"] = self._extract_actions_history(unified_data)
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
        unified_data["metadata"]["data_quality_score"] = self._assess_data_quality(unified_data)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {unified_data['metadata']['successful_loads']}/{unified_data['metadata']['total_sources']} ã‚½ãƒ¼ã‚¹æˆåŠŸ")
        return unified_data
    
    def _load_and_normalize_data(self, source_path: str, phase: str) -> Optional[List[Dict]]:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»æ­£è¦åŒ–"""
        try:
            if not os.path.exists(source_path):
                return None
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ãŸèª­ã¿è¾¼ã¿
            if source_path.endswith('.json'):
                with open(source_path) as f:
                    data = json.load(f)
            elif source_path.endswith('.jsonl'):
                data = []
                with open(source_path) as f:
                    for line in f:
                        if line.strip():
                            data.append(json.loads(line))
            else:
                return None
            
            # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
            normalized = self._normalize_to_schema(data, phase, source_path)
            return normalized
            
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {source_path}: {e}")
            return None
    
    def _normalize_to_schema(self, data: Any, phase: str, source_path: str) -> List[Dict]:
        """ã‚¹ã‚­ãƒ¼ãƒæ­£è¦åŒ–"""
        normalized = []
        
        if isinstance(data, dict):
            data = [data]
        elif not isinstance(data, list):
            return []
        
        for item in data:
            if not isinstance(item, dict):
                continue
                
            normalized_item = {
                "timestamp": self._extract_timestamp(item),
                "source_phase": phase,
                "data_type": Path(source_path).stem,
                "quality_metrics": self._extract_quality_metrics(item),
                "improvement_actions": self._extract_improvement_actions(item),
                "success_indicators": self._extract_success_indicators(item),
                "failure_patterns": self._extract_failure_patterns(item),
                "context_metadata": self._extract_context_metadata(item),
                "raw_data": item  # å…ƒãƒ‡ãƒ¼ã‚¿ã‚‚ä¿æŒ
            }
            
            normalized.append(normalized_item)
        
        return normalized
    
    def _extract_timestamp(self, item: Dict) -> str:
        """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º"""
        timestamp_fields = ["timestamp", "created_at", "last_updated", "analysis_timestamp"]
        
        for field in timestamp_fields:
            if field in item and item[field]:
                return str(item[field])
        
        return datetime.now().isoformat()
    
    def _extract_quality_metrics(self, item: Dict) -> Dict[str, float]:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º"""
        metrics = {}
        
        # æ§˜ã€…ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹å½¢å¼ã«å¯¾å¿œ
        if "score" in item:
            metrics["overall_score"] = float(item["score"])
        if "quality_score" in item:
            metrics["quality_score"] = float(item["quality_score"])
        if "pass_rate" in item:
            metrics["pass_rate"] = float(item["pass_rate"])
        if "improvement" in item:
            metrics["improvement"] = float(item["improvement"])
        
        return metrics
    
    def _extract_improvement_actions(self, item: Dict) -> List[str]:
        """æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º"""
        actions = []
        
        action_fields = ["actions", "recommendations", "fixes", "steps", "action"]
        for field in action_fields:
            if field in item and item[field]:
                if isinstance(item[field], list):
                    actions.extend(item[field])
                elif isinstance(item[field], str):
                    actions.append(item[field])
        
        return actions
    
    def _extract_success_indicators(self, item: Dict) -> Dict[str, Any]:
        """æˆåŠŸæŒ‡æ¨™æŠ½å‡º"""
        indicators = {}
        
        if "success" in item:
            indicators["success"] = item["success"]
        if "status" in item:
            indicators["status"] = item["status"]
        if "final_state" in item:
            indicators["final_state"] = item["final_state"]
        
        return indicators
    
    def _extract_failure_patterns(self, item: Dict) -> List[str]:
        """å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        patterns = []
        
        if "errors" in item and isinstance(item["errors"], list):
            patterns.extend(item["errors"])
        if "issues" in item and isinstance(item["issues"], list):
            patterns.extend(item["issues"])
        if "severity" in item and item["severity"] in ["error", "critical"]:
            patterns.append(f"severity_{item['severity']}")
        
        return patterns
    
    def _extract_context_metadata(self, item: Dict) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        metadata = {}
        
        context_fields = ["file", "project_id", "developer_id", "project_type", "environment"]
        for field in context_fields:
            if field in item:
                metadata[field] = item[field]
        
        return metadata
    
    def _build_time_series(self, unified_data: Dict) -> List[Dict]:
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰"""
        time_series = []
        
        for phase in ["phase1", "phase2", "phase3", "phase4"]:
            phase_data = unified_data.get(f"{phase}_data", [])
            
            for item in phase_data:
                time_point = {
                    "timestamp": item["timestamp"],
                    "phase": item["source_phase"],
                    "data_type": item["data_type"],
                    "metrics": item["quality_metrics"]
                }
                time_series.append(time_point)
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
        time_series.sort(key=lambda x: x["timestamp"])
        return time_series
    
    def _extract_quality_timeline(self, unified_data: Dict) -> List[Dict]:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹æ™‚ç³»åˆ—æŠ½å‡º"""
        timeline = []
        
        for item in unified_data["time_series"]:
            if item["metrics"]:
                timeline_point = {
                    "timestamp": item["timestamp"],
                    "phase": item["phase"],
                    "overall_score": item["metrics"].get("overall_score", 0),
                    "quality_score": item["metrics"].get("quality_score", 0)
                }
                timeline.append(timeline_point)
        
        return timeline
    
    def _extract_actions_history(self, unified_data: Dict) -> List[Dict]:
        """æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´æŠ½å‡º"""
        history = []
        
        for phase in ["phase1", "phase2", "phase3", "phase4"]:
            phase_data = unified_data.get(f"{phase}_data", [])
            
            for item in phase_data:
                if item["improvement_actions"]:
                    history_item = {
                        "timestamp": item["timestamp"],
                        "phase": item["source_phase"],
                        "actions": item["improvement_actions"],
                        "success_indicators": item["success_indicators"]
                    }
                    history.append(history_item)
        
        return history
    
    def _assess_data_quality(self, unified_data: Dict) -> float:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡"""
        total_items = 0
        quality_score = 0
        
        for phase in ["phase1", "phase2", "phase3", "phase4"]:
            phase_data = unified_data.get(f"{phase}_data", [])
            
            for item in phase_data:
                total_items += 1
                item_quality = 0
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å­˜åœ¨
                if item["timestamp"]:
                    item_quality += 0.2
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å­˜åœ¨
                if item["quality_metrics"]:
                    item_quality += 0.3
                
                # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å­˜åœ¨
                if item["improvement_actions"]:
                    item_quality += 0.2
                
                # æˆåŠŸæŒ‡æ¨™ã®å­˜åœ¨
                if item["success_indicators"]:
                    item_quality += 0.15
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨
                if item["context_metadata"]:
                    item_quality += 0.15
                
                quality_score += item_quality
        
        return quality_score / max(1, total_items)


class PatternRecognitionEngine:
    """ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.known_patterns = self._load_known_patterns()
        self.pattern_threshold = 0.7
    
    def _load_known_patterns(self) -> Dict[str, Any]:
        """æ—¢çŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿"""
        return {
            "improvement_patterns": [
                {"name": "gradual_improvement", "indicators": ["increasing_scores", "consistent_actions"]},
                {"name": "breakthrough", "indicators": ["sudden_jump", "new_methodology"]},
                {"name": "plateau", "indicators": ["stable_scores", "no_major_changes"]}
            ],
            "failure_patterns": [
                {"name": "recurring_failure", "indicators": ["repeated_errors", "same_root_cause"]},
                {"name": "cascading_failure", "indicators": ["multiple_failures", "dependency_issues"]},
                {"name": "degradation", "indicators": ["declining_scores", "increasing_complexity"]}
            ],
            "cyclical_patterns": [
                {"name": "weekly_cycle", "period": 7, "indicators": ["day_of_week_variance"]},
                {"name": "deployment_cycle", "period": 14, "indicators": ["release_related_changes"]}
            ]
        }
    
    def recognize_patterns(self, unified_data: Dict) -> List[LearningPattern]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜å®Ÿè¡Œ"""
        print("ğŸ” AIãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜é–‹å§‹...")
        
        patterns = []
        
        # æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        improvement_patterns = self._recognize_improvement_patterns(unified_data)
        patterns.extend(improvement_patterns)
        
        # å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        failure_patterns = self._recognize_failure_patterns(unified_data)
        patterns.extend(failure_patterns)
        
        # å‘¨æœŸãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        cyclical_patterns = self._recognize_cyclical_patterns(unified_data)
        patterns.extend(cyclical_patterns)
        
        # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        anomaly_patterns = self._recognize_anomaly_patterns(unified_data)
        patterns.extend(anomaly_patterns)
        
        print(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜å®Œäº†: {len(patterns)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹")
        return patterns
    
    def _recognize_improvement_patterns(self, unified_data: Dict) -> List[LearningPattern]:
        """æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜"""
        patterns = []
        
        timeline = unified_data["quality_metrics_timeline"]
        if len(timeline) < 3:
            return patterns
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        scores = [item.get("overall_score", 0) for item in timeline[-10:]]  # ç›´è¿‘10ä»¶
        
        if len(scores) >= 3:
            # æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
            if self._is_improving_trend(scores):
                pattern = LearningPattern(
                    pattern_id="improvement_trend_1",
                    pattern_type="improvement",
                    confidence_score=0.8,
                    frequency=len([s for s in scores if s > 0]),
                    impact_score=max(scores) - min(scores),
                    conditions={"trend": "improving", "data_points": len(scores)},
                    outcomes={"score_improvement": max(scores) - scores[0]},
                    discovered_at=datetime.now(),
                    last_seen=datetime.now()
                )
                patterns.append(pattern)
        
        return patterns
    
    def _recognize_failure_patterns(self, unified_data: Dict) -> List[LearningPattern]:
        """å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜"""
        patterns = []
        
        # å¤±æ•—å±¥æ­´åˆ†æ
        failure_count = 0
        recurring_issues = []
        
        for phase in ["phase1", "phase2", "phase3", "phase4"]:
            phase_data = unified_data.get(f"{phase}_data", [])
            
            for item in phase_data:
                failure_patterns = item.get("failure_patterns", [])
                if failure_patterns:
                    failure_count += 1
                    recurring_issues.extend(failure_patterns)
        
        # ç¹°ã‚Šè¿”ã—å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³
        if failure_count >= 3:
            common_issues = [issue for issue, count in Counter(recurring_issues).most_common(3)]
            
            pattern = LearningPattern(
                pattern_id="recurring_failure_1",
                pattern_type="regression",
                confidence_score=0.7,
                frequency=failure_count,
                impact_score=failure_count / 10.0,
                conditions={"failure_count": failure_count, "common_issues": common_issues},
                outcomes={"identified_patterns": common_issues},
                discovered_at=datetime.now(),
                last_seen=datetime.now()
            )
            patterns.append(pattern)
        
        return patterns
    
    def _recognize_cyclical_patterns(self, unified_data: Dict) -> List[LearningPattern]:
        """å‘¨æœŸãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜"""
        patterns = []
        
        timeline = unified_data["quality_metrics_timeline"]
        if len(timeline) < 14:  # æœ€ä½2é€±é–“ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            return patterns
        
        # ç°¡æ˜“çš„ãªå‘¨æœŸæ€§æ¤œå‡º
        scores = [item.get("overall_score", 0) for item in timeline]
        
        # é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ7æ—¥å‘¨æœŸï¼‰ã®æ¤œå‡º
        if self._detect_weekly_pattern(scores):
            pattern = LearningPattern(
                pattern_id="weekly_cycle_1",
                pattern_type="cyclical",
                confidence_score=0.6,
                frequency=7,
                impact_score=0.3,
                conditions={"period": 7, "pattern_type": "weekly"},
                outcomes={"detected_cycle": "weekly"},
                discovered_at=datetime.now(),
                last_seen=datetime.now()
            )
            patterns.append(pattern)
        
        return patterns
    
    def _recognize_anomaly_patterns(self, unified_data: Dict) -> List[LearningPattern]:
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜"""
        patterns = []
        
        timeline = unified_data["quality_metrics_timeline"]
        if len(timeline) < 5:
            return patterns
        
        scores = [item.get("overall_score", 0) for item in timeline[-20:]]  # ç›´è¿‘20ä»¶
        
        if scores:
            mean_score = statistics.mean(scores)
            std_dev = statistics.stdev(scores) if len(scores) > 1 else 0
            
            # ç•°å¸¸å€¤æ¤œå‡ºï¼ˆæ¨™æº–åå·®ã®2å€ä»¥ä¸Šï¼‰
            anomalies = [s for s in scores if abs(s - mean_score) > 2 * std_dev]
            
            if anomalies:
                pattern = LearningPattern(
                    pattern_id="anomaly_1",
                    pattern_type="anomaly",
                    confidence_score=0.9,
                    frequency=len(anomalies),
                    impact_score=max(abs(a - mean_score) for a in anomalies),
                    conditions={"mean": mean_score, "std_dev": std_dev, "threshold": 2 * std_dev},
                    outcomes={"anomaly_count": len(anomalies), "anomaly_values": anomalies},
                    discovered_at=datetime.now(),
                    last_seen=datetime.now()
                )
                patterns.append(pattern)
        
        return patterns
    
    def _is_improving_trend(self, scores: List[float]) -> bool:
        """æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š"""
        if len(scores) < 3:
            return False
        
        # ç›´è¿‘ã®å€¤ãŒéå»ã‚ˆã‚Šé«˜ã„ã‹ãƒã‚§ãƒƒã‚¯
        recent_avg = statistics.mean(scores[-3:])
        earlier_avg = statistics.mean(scores[:-3]) if len(scores) > 3 else scores[0]
        
        return recent_avg > earlier_avg * 1.05  # 5%ä»¥ä¸Šã®æ”¹å–„
    
    def _detect_weekly_pattern(self, scores: List[float]) -> bool:
        """é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        if len(scores) < 14:
            return False
        
        # ç°¡æ˜“çš„ãªé€±æ¬¡ç›¸é–¢ãƒã‚§ãƒƒã‚¯
        weekly_scores = []
        for i in range(0, len(scores) - 7, 7):
            week_score = statistics.mean(scores[i:i+7])
            weekly_scores.append(week_score)
        
        if len(weekly_scores) >= 2:
            # é€±ã”ã¨ã®ã‚¹ã‚³ã‚¢å¤‰å‹•ã‚’ç¢ºèª
            variance = statistics.variance(weekly_scores)
            return variance < 0.1  # å¤‰å‹•ãŒå°ã•ã‘ã‚Œã°å‘¨æœŸæ€§ã‚ã‚Š
        
        return False


class PredictiveTrendEngine:
    """äºˆæ¸¬ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.prediction_horizon = 30  # days
    
    def predict_trends(self, unified_data: Dict, patterns: List[LearningPattern]) -> List[IntelligenceInsight]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬"""
        print("ğŸ”® äºˆæ¸¬ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æé–‹å§‹...")
        
        insights = []
        
        # å“è³ªã‚¹ã‚³ã‚¢äºˆæ¸¬
        quality_predictions = self._predict_quality_trends(unified_data)
        insights.extend(quality_predictions)
        
        # æ”¹å–„åŠ¹æœäºˆæ¸¬
        improvement_predictions = self._predict_improvement_effects(unified_data, patterns)
        insights.extend(improvement_predictions)
        
        # ãƒªã‚¹ã‚¯äºˆæ¸¬
        risk_predictions = self._predict_risk_areas(unified_data, patterns)
        insights.extend(risk_predictions)
        
        # æ©Ÿä¼šäºˆæ¸¬
        opportunity_predictions = self._predict_opportunities(unified_data, patterns)
        insights.extend(opportunity_predictions)
        
        print(f"âœ… äºˆæ¸¬åˆ†æå®Œäº†: {len(insights)}å€‹ã®æ´å¯Ÿã‚’ç”Ÿæˆ")
        return insights
    
    def _predict_quality_trends(self, unified_data: Dict) -> List[IntelligenceInsight]:
        """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬"""
        insights = []
        
        timeline = unified_data["quality_metrics_timeline"]
        if len(timeline) < 5:
            return insights
        
        recent_scores = [item.get("overall_score", 0) for item in timeline[-10:]]
        
        if recent_scores:
            trend_slope = self._calculate_trend_slope(recent_scores)
            
            if trend_slope > 0.01:  # æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰
                insight = IntelligenceInsight(
                    insight_id="quality_trend_positive",
                    insight_type="prediction",
                    title="å“è³ªã‚¹ã‚³ã‚¢æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šäºˆæ¸¬",
                    description=f"ç¾åœ¨ã®æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆ{trend_slope:.3f}/æœŸé–“ï¼‰ãŒç¶™ç¶šã™ã‚Œã°ã€30æ—¥å¾Œã«{recent_scores[-1] + trend_slope * 30:.2f}ã®ã‚¹ã‚³ã‚¢é”æˆäºˆæ¸¬",
                    evidence=[f"ç›´è¿‘10å›ã®å¹³å‡æ”¹å–„ç‡: {trend_slope:.3f}", f"ç¾åœ¨ã‚¹ã‚³ã‚¢: {recent_scores[-1]:.2f}"],
                    confidence=0.7,
                    actionable_items=[
                        "ç¾åœ¨ã®æ”¹å–„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¶™ç¶š",
                        "æˆåŠŸè¦å› ã®æ–‡æ›¸åŒ–ãƒ»æ¨™æº–åŒ–",
                        "æ”¹å–„åŠ¹æœã®å®šæœŸæ¸¬å®š"
                    ],
                    expected_impact=trend_slope * 30,
                    time_sensitivity="medium_term",
                    generated_at=datetime.now()
                )
                insights.append(insight)
            elif trend_slope < -0.01:  # æ‚ªåŒ–ãƒˆãƒ¬ãƒ³ãƒ‰
                insight = IntelligenceInsight(
                    insight_id="quality_trend_negative",
                    insight_type="warning",
                    title="å“è³ªã‚¹ã‚³ã‚¢æ‚ªåŒ–ãƒˆãƒ¬ãƒ³ãƒ‰è­¦å‘Š",
                    description=f"å“è³ªã‚¹ã‚³ã‚¢ãŒæ‚ªåŒ–å‚¾å‘ï¼ˆ{trend_slope:.3f}/æœŸé–“ï¼‰ã€‚å¯¾ç­–ãªã—ã§ã¯30æ—¥å¾Œã«{recent_scores[-1] + trend_slope * 30:.2f}ã¾ã§ä½ä¸‹äºˆæ¸¬",
                    evidence=[f"ç›´è¿‘10å›ã®å¹³å‡æ‚ªåŒ–ç‡: {trend_slope:.3f}", f"ç¾åœ¨ã‚¹ã‚³ã‚¢: {recent_scores[-1]:.2f}"],
                    confidence=0.8,
                    actionable_items=[
                        "ç·Šæ€¥å“è³ªæ”¹å–„è¨ˆç”»ã®ç­–å®š",
                        "æ‚ªåŒ–è¦å› ã®æ ¹æœ¬åˆ†æ",
                        "è¿½åŠ å“è³ªãƒã‚§ãƒƒã‚¯å°å…¥"
                    ],
                    expected_impact=abs(trend_slope * 30),
                    time_sensitivity="immediate",
                    generated_at=datetime.now()
                )
                insights.append(insight)
        
        return insights
    
    def _predict_improvement_effects(self, unified_data: Dict, patterns: List[LearningPattern]) -> List[IntelligenceInsight]:
        """æ”¹å–„åŠ¹æœäºˆæ¸¬"""
        insights = []
        
        improvement_patterns = [p for p in patterns if p.pattern_type == "improvement"]
        
        for pattern in improvement_patterns:
            if pattern.confidence_score >= 0.7:
                insight = IntelligenceInsight(
                    insight_id=f"improvement_effect_{pattern.pattern_id}",
                    insight_type="recommendation",
                    title="ç¶™ç¶šçš„æ”¹å–„åŠ¹æœã®æœ€å¤§åŒ–",
                    description=f"ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern.pattern_id}' ã®ç¶™ç¶šã«ã‚ˆã‚Šã€{pattern.impact_score:.2f}ãƒã‚¤ãƒ³ãƒˆã®è¿½åŠ æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã‚‹",
                    evidence=[f"ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿¡é ¼åº¦: {pattern.confidence_score:.2f}", f"éå»ã®æ”¹å–„å®Ÿç¸¾: {pattern.impact_score:.2f}"],
                    confidence=pattern.confidence_score,
                    actionable_items=[
                        "æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®å†ç¾ãƒ»æ¨™æº–åŒ–",
                        "åŒæ§˜ã®æ¡ä»¶ã§ã®å±•é–‹æ¤œè¨",
                        "åŠ¹æœæ¸¬å®šã®ç¶™ç¶šå®Ÿæ–½"
                    ],
                    expected_impact=pattern.impact_score * 1.5,  # æœ€å¤§åŒ–åŠ¹æœ
                    time_sensitivity="short_term",
                    generated_at=datetime.now()
                )
                insights.append(insight)
        
        return insights
    
    def _predict_risk_areas(self, unified_data: Dict, patterns: List[LearningPattern]) -> List[IntelligenceInsight]:
        """ãƒªã‚¹ã‚¯é ˜åŸŸäºˆæ¸¬"""
        insights = []
        
        failure_patterns = [p for p in patterns if p.pattern_type in ["regression", "anomaly"]]
        
        for pattern in failure_patterns:
            if pattern.confidence_score >= 0.6:
                insight = IntelligenceInsight(
                    insight_id=f"risk_prediction_{pattern.pattern_id}",
                    insight_type="warning",
                    title="æ½œåœ¨çš„ãƒªã‚¹ã‚¯é ˜åŸŸã®ç‰¹å®š",
                    description=f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern.pattern_id}' ã«åŸºã¥ãã€{pattern.frequency}å›ã®é¡ä¼¼å•é¡Œç™ºç”Ÿãƒªã‚¹ã‚¯ã‚ã‚Š",
                    evidence=[f"éå»ã®ç™ºç”Ÿé »åº¦: {pattern.frequency}å›", f"å½±éŸ¿åº¦: {pattern.impact_score:.2f}"],
                    confidence=pattern.confidence_score,
                    actionable_items=[
                        "äºˆé˜²çš„å¯¾ç­–ã®å®Ÿæ–½",
                        "ç›£è¦–å¼·åŒ–ã«ã‚ˆã‚‹æ—©æœŸç™ºè¦‹",
                        "ã‚³ãƒ³ãƒ†ã‚£ãƒ³ã‚¸ã‚§ãƒ³ã‚·ãƒ¼ãƒ—ãƒ©ãƒ³ã®ç­–å®š"
                    ],
                    expected_impact=pattern.impact_score,
                    time_sensitivity="short_term" if pattern.confidence_score >= 0.8 else "medium_term",
                    generated_at=datetime.now()
                )
                insights.append(insight)
        
        return insights
    
    def _predict_opportunities(self, unified_data: Dict, patterns: List[LearningPattern]) -> List[IntelligenceInsight]:
        """æ©Ÿä¼šäºˆæ¸¬"""
        insights = []
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†æã‹ã‚‰æ©Ÿä¼šã‚’ç™ºè¦‹
        actions_history = unified_data["improvement_actions_history"]
        
        if actions_history:
            # æˆåŠŸç‡ã®é«˜ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã‚’ç‰¹å®š
            successful_actions = []
            for item in actions_history:
                success_indicators = item.get("success_indicators", {})
                if success_indicators.get("success", False) or success_indicators.get("status") == "passed":
                    successful_actions.extend(item.get("actions", []))
            
            if successful_actions:
                action_counts = Counter(successful_actions)
                top_actions = action_counts.most_common(3)
                
                insight = IntelligenceInsight(
                    insight_id="opportunity_successful_actions",
                    insight_type="opportunity",
                    title="é«˜æˆåŠŸç‡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ‹¡å¤§æ©Ÿä¼š",
                    description=f"æˆåŠŸç‡ã®é«˜ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ '{top_actions[0][0]}' ã‚’ä»–ã®é ˜åŸŸã«ã‚‚é©ç”¨ã™ã‚‹ã“ã¨ã§è¿½åŠ æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã‚‹",
                    evidence=[f"æˆåŠŸå®Ÿç¸¾: {top_actions[0][1]}å›", "é¡ä¼¼æ¡ä»¶ã§ã®é©ç”¨å¯èƒ½æ€§"],
                    confidence=0.6,
                    actionable_items=[
                        "æˆåŠŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ä»–é ˜åŸŸé©ç”¨æ¤œè¨",
                        "é©ç”¨æ¡ä»¶ã®åˆ†æãƒ»æ–‡æ›¸åŒ–",
                        "æ®µéšçš„å±•é–‹è¨ˆç”»ã®ç­–å®š"
                    ],
                    expected_impact=0.3,
                    time_sensitivity="medium_term",
                    generated_at=datetime.now()
                )
                insights.append(insight)
        
        return insights
    
    def _calculate_trend_slope(self, scores: List[float]) -> float:
        """ãƒˆãƒ¬ãƒ³ãƒ‰å‚¾ãã®è¨ˆç®—"""
        if len(scores) < 2:
            return 0
        
        # ç°¡å˜ãªç·šå½¢å›å¸°ã®å‚¾ã
        n = len(scores)
        x_values = list(range(n))
        x_mean = statistics.mean(x_values)
        y_mean = statistics.mean(scores)
        
        numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(x_values, scores))
        denominator = sum((x - x_mean) ** 2 for x in x_values)
        
        if denominator == 0:
            return 0
        
        return numerator / denominator


class ContinuousLearningEngine:
    """ç¶™ç¶šå­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.learning_models = {}
        self.feedback_history = []
    
    def update_learning_models(self, unified_data: Dict, patterns: List[LearningPattern], insights: List[IntelligenceInsight]) -> Dict[str, Any]:
        """å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        print("ğŸ§  ç¶™ç¶šå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°é–‹å§‹...")
        
        learning_updates = {
            "model_updates": 0,
            "new_patterns": len(patterns),
            "insight_accuracy": 0.0,
            "learning_effectiveness": 0.0,
            "next_learning_goals": []
        }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ¢ãƒ‡ãƒ«æ›´æ–°
        self._update_pattern_models(patterns)
        learning_updates["model_updates"] += 1
        
        # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ›´æ–°
        self._update_prediction_models(insights)
        learning_updates["model_updates"] += 1
        
        # å­¦ç¿’åŠ¹æœæ¸¬å®š
        effectiveness = self._measure_learning_effectiveness(unified_data)
        learning_updates["learning_effectiveness"] = effectiveness
        
        # æ¬¡ã®å­¦ç¿’ç›®æ¨™è¨­å®š
        goals = self._set_next_learning_goals(patterns, insights)
        learning_updates["next_learning_goals"] = goals
        
        print(f"âœ… ç¶™ç¶šå­¦ç¿’æ›´æ–°å®Œäº†: åŠ¹æœåº¦{effectiveness:.2f}")
        return learning_updates
    
    def _update_pattern_models(self, patterns: List[LearningPattern]):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        for pattern in patterns:
            model_key = f"pattern_{pattern.pattern_type}"
            
            if model_key not in self.learning_models:
                self.learning_models[model_key] = {
                    "instances": [],
                    "accuracy": 0.0,
                    "confidence_threshold": 0.7
                }
            
            self.learning_models[model_key]["instances"].append({
                "pattern_id": pattern.pattern_id,
                "confidence": pattern.confidence_score,
                "impact": pattern.impact_score,
                "conditions": pattern.conditions,
                "timestamp": pattern.discovered_at
            })
            
            # æœ€æ–°50å€‹ã¾ã§ä¿æŒ
            if len(self.learning_models[model_key]["instances"]) > 50:
                self.learning_models[model_key]["instances"] = self.learning_models[model_key]["instances"][-50:]
    
    def _update_prediction_models(self, insights: List[IntelligenceInsight]):
        """äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        for insight in insights:
            model_key = f"prediction_{insight.insight_type}"
            
            if model_key not in self.learning_models:
                self.learning_models[model_key] = {
                    "predictions": [],
                    "accuracy_history": [],
                    "confidence_calibration": 1.0
                }
            
            self.learning_models[model_key]["predictions"].append({
                "insight_id": insight.insight_id,
                "confidence": insight.confidence,
                "expected_impact": insight.expected_impact,
                "time_sensitivity": insight.time_sensitivity,
                "timestamp": insight.generated_at
            })
    
    def _measure_learning_effectiveness(self, unified_data: Dict) -> float:
        """å­¦ç¿’åŠ¹æœæ¸¬å®š"""
        # ç°¡æ˜“çš„ãªåŠ¹æœæ¸¬å®š
        timeline = unified_data["quality_metrics_timeline"]
        
        if len(timeline) < 10:
            return 0.5
        
        recent_scores = [item.get("overall_score", 0) for item in timeline[-10:]]
        earlier_scores = [item.get("overall_score", 0) for item in timeline[-20:-10]] if len(timeline) >= 20 else recent_scores
        
        if recent_scores and earlier_scores:
            recent_avg = statistics.mean(recent_scores)
            earlier_avg = statistics.mean(earlier_scores)
            
            improvement = (recent_avg - earlier_avg) / max(0.01, earlier_avg)  # æ­£è¦åŒ–ã•ã‚ŒãŸæ”¹å–„ç‡
            return max(0.0, min(1.0, improvement + 0.5))  # 0-1ã«æ­£è¦åŒ–
        
        return 0.5
    
    def _set_next_learning_goals(self, patterns: List[LearningPattern], insights: List[IntelligenceInsight]) -> List[str]:
        """æ¬¡ã®å­¦ç¿’ç›®æ¨™è¨­å®š"""
        goals = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®ç›®æ¨™
        low_confidence_patterns = [p for p in patterns if p.confidence_score < 0.7]
        if low_confidence_patterns:
            goals.append(f"ä½ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç²¾åº¦å‘ä¸Š ({len(low_confidence_patterns)}å€‹)")
        
        # äºˆæ¸¬ç²¾åº¦æ”¹å–„ç›®æ¨™
        warning_insights = [i for i in insights if i.insight_type == "warning"]
        if warning_insights:
            goals.append(f"è­¦å‘Šäºˆæ¸¬ã®ç²¾åº¦å‘ä¸Š ({len(warning_insights)}å€‹)")
        
        # ä¸€èˆ¬çš„ãªæ”¹å–„ç›®æ¨™
        goals.extend([
            "ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¶™ç¶šå‘ä¸Š",
            "æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹ã®ä¿ƒé€²",
            "äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦å‘ä¸Š"
        ])
        
        return goals[:5]  # ä¸Šä½5å€‹ã¾ã§


class IntelligenceEngineSystem:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ãƒ»ã‚¨ãƒ³ã‚¸ãƒ³ãƒ»ã‚·ã‚¹ãƒ†ãƒ  ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.data_engine = DataUnificationEngine()
        self.pattern_engine = PatternRecognitionEngine()
        self.prediction_engine = PredictiveTrendEngine()
        self.learning_engine = ContinuousLearningEngine()
    
    def run_full_intelligence_analysis(self) -> Dict[str, Any]:
        """å®Œå…¨ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹åˆ†æå®Ÿè¡Œ"""
        print("ğŸ¤– ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ãƒ»ã‚¨ãƒ³ã‚¸ãƒ³ãƒ»ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹...")
        
        # Step 1: ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        unified_data = self.data_engine.unify_all_data()
        
        # Step 2: ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        patterns = self.pattern_engine.recognize_patterns(unified_data)
        
        # Step 3: äºˆæ¸¬ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        insights = self.prediction_engine.predict_trends(unified_data, patterns)
        
        # Step 4: ç¶™ç¶šå­¦ç¿’æ›´æ–°
        learning_updates = self.learning_engine.update_learning_models(unified_data, patterns, insights)
        
        # çµæœçµ±åˆ
        result = {
            "analysis_metadata": {
                "execution_timestamp": datetime.now().isoformat(),
                "data_quality_score": unified_data["metadata"]["data_quality_score"],
                "total_data_sources": unified_data["metadata"]["total_sources"],
                "successful_loads": unified_data["metadata"]["successful_loads"]
            },
            "pattern_analysis": {
                "total_patterns": len(patterns),
                "pattern_types": list(set(p.pattern_type for p in patterns)),
                "high_confidence_patterns": len([p for p in patterns if p.confidence_score >= 0.8]),
                "patterns_summary": [self._pattern_to_dict(p) for p in patterns[:10]]
            },
            "predictive_insights": {
                "total_insights": len(insights),
                "insight_types": list(set(i.insight_type for i in insights)),
                "immediate_actions": len([i for i in insights if i.time_sensitivity == "immediate"]),
                "insights_summary": [self._insight_to_dict(i) for i in insights[:10]]
            },
            "learning_updates": learning_updates,
            "intelligence_score": self._calculate_intelligence_score(patterns, insights, learning_updates),
            "next_recommendations": self._generate_next_recommendations(insights[:5])
        }
        
        # çµæœä¿å­˜
        self._save_intelligence_results(result)
        
        print(f"âœ… ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹åˆ†æå®Œäº†: çŸ¥èƒ½ã‚¹ã‚³ã‚¢ {result['intelligence_score']:.2f}")
        return result
    
    def _pattern_to_dict(self, pattern: LearningPattern) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "id": pattern.pattern_id,
            "type": pattern.pattern_type,
            "confidence": round(pattern.confidence_score, 3),
            "frequency": pattern.frequency,
            "impact": round(pattern.impact_score, 3),
            "conditions": pattern.conditions
        }
    
    def _insight_to_dict(self, insight: IntelligenceInsight) -> Dict[str, Any]:
        """æ´å¯Ÿã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "id": insight.insight_id,
            "type": insight.insight_type,
            "title": insight.title,
            "confidence": round(insight.confidence, 3),
            "expected_impact": round(insight.expected_impact, 3),
            "time_sensitivity": insight.time_sensitivity,
            "actionable_items": insight.actionable_items[:3]  # ä¸Šä½3å€‹ã¾ã§
        }
    
    def _calculate_intelligence_score(self, patterns: List[LearningPattern], insights: List[IntelligenceInsight], learning_updates: Dict) -> float:
        """çŸ¥èƒ½ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.0
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚¹ã‚³ã‚¢ (0.4)
        if patterns:
            avg_pattern_confidence = statistics.mean([p.confidence_score for p in patterns])
            pattern_diversity = len(set(p.pattern_type for p in patterns)) / 4.0  # 4ç¨®é¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            score += (avg_pattern_confidence * 0.7 + pattern_diversity * 0.3) * 0.4
        
        # æ´å¯Ÿç”Ÿæˆã‚¹ã‚³ã‚¢ (0.3)
        if insights:
            avg_insight_confidence = statistics.mean([i.confidence for i in insights])
            insight_diversity = len(set(i.insight_type for i in insights)) / 4.0  # 4ç¨®é¡ã®æ´å¯Ÿ
            score += (avg_insight_confidence * 0.8 + insight_diversity * 0.2) * 0.3
        
        # å­¦ç¿’åŠ¹æœã‚¹ã‚³ã‚¢ (0.3)
        learning_effectiveness = learning_updates.get("learning_effectiveness", 0.5)
        score += learning_effectiveness * 0.3
        
        return min(1.0, score)
    
    def _generate_next_recommendations(self, top_insights: List[IntelligenceInsight]) -> List[str]:
        """æ¬¡ã®æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        recommendations = []
        
        for insight in top_insights:
            if insight.time_sensitivity == "immediate":
                recommendations.append(f"ğŸš¨ ç·Šæ€¥: {insight.actionable_items[0] if insight.actionable_items else insight.title}")
            elif insight.insight_type == "opportunity":
                recommendations.append(f"ğŸ’¡ æ©Ÿä¼š: {insight.actionable_items[0] if insight.actionable_items else insight.title}")
            elif insight.insight_type == "warning":
                recommendations.append(f"âš ï¸ æ³¨æ„: {insight.actionable_items[0] if insight.actionable_items else insight.title}")
            else:
                recommendations.append(f"ğŸ“Š åˆ†æ: {insight.actionable_items[0] if insight.actionable_items else insight.title}")
        
        return recommendations[:10]
    
    def _save_intelligence_results(self, result: Dict[str, Any]):
        """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹çµæœä¿å­˜"""
        try:
            os.makedirs("out", exist_ok=True)
            with open("out/intelligence_analysis.json", "w") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def display_intelligence_results(self, result: Dict[str, Any]):
        """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹çµæœè¡¨ç¤º"""
        print("\nğŸ¤– ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ãƒ»ã‚¨ãƒ³ã‚¸ãƒ³åˆ†æçµæœ")
        print("=" * 60)
        
        metadata = result["analysis_metadata"]
        pattern_analysis = result["pattern_analysis"]
        insights = result["predictive_insights"]
        learning = result["learning_updates"]
        
        print(f"ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ:")
        print(f"   ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢: {metadata['data_quality_score']:.2f}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {metadata['successful_loads']}/{metadata['total_data_sources']}")
        print(f"   çŸ¥èƒ½ã‚¹ã‚³ã‚¢: {result['intelligence_score']:.2f}")
        
        print(f"\nğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜çµæœ:")
        print(f"   ç™ºè¦‹ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern_analysis['total_patterns']}å€‹")
        print(f"   é«˜ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern_analysis['high_confidence_patterns']}å€‹")
        print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³ç¨®åˆ¥: {', '.join(pattern_analysis['pattern_types'])}")
        
        print(f"\nğŸ”® äºˆæ¸¬æ´å¯Ÿçµæœ:")
        print(f"   ç”Ÿæˆæ´å¯Ÿ: {insights['total_insights']}å€‹")
        print(f"   ç·Šæ€¥å¯¾å¿œé …ç›®: {insights['immediate_actions']}å€‹")
        print(f"   æ´å¯Ÿç¨®åˆ¥: {', '.join(insights['insight_types'])}")
        
        print(f"\nğŸ§  å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ :")
        print(f"   å­¦ç¿’åŠ¹æœ: {learning['learning_effectiveness']:.2f}")
        print(f"   ãƒ¢ãƒ‡ãƒ«æ›´æ–°: {learning['model_updates']}å€‹")
        print(f"   æ¬¡ã®å­¦ç¿’ç›®æ¨™: {len(learning['next_learning_goals'])}å€‹")
        
        print(f"\nğŸš€ æ¬¡ã®æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        for rec in result["next_recommendations"][:5]:
            print(f"   â€¢ {rec}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ğŸ¤– Intelligence Engine System")
    parser.add_argument("--full-analysis", action="store_true", help="Run full intelligence analysis")
    parser.add_argument("--display-only", action="store_true", help="Display last intelligence results")
    
    args = parser.parse_args()
    
    system = IntelligenceEngineSystem()
    
    if args.display_only:
        try:
            with open("out/intelligence_analysis.json") as f:
                result = json.load(f)
                system.display_intelligence_results(result)
        except:
            print("âŒ ä¿å­˜ã•ã‚ŒãŸã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    elif args.full_analysis:
        result = system.run_full_intelligence_analysis()
        system.display_intelligence_results(result)
    else:
        result = system.run_full_intelligence_analysis()
        system.display_intelligence_results(result)


if __name__ == "__main__":
    main()


