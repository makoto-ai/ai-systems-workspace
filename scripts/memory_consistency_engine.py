#!/usr/bin/env python3
"""
ğŸ§  Memory Consistency Engine - è¨˜æ†¶ä¸€è²«æ€§ã‚¨ãƒ³ã‚¸ãƒ³
AIã®ç™ºè¨€ãƒ»è©•ä¾¡ã®ä¸€è²«æ€§ã‚’ä¿æŒã—ã€æ”¹ç«„ã‚’é˜²æ­¢ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import json
import datetime
import hashlib
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

class MemoryConsistencyEngine:
    """è¨˜æ†¶ä¸€è²«æ€§ã‚¨ãƒ³ã‚¸ãƒ³ - AIç™ºè¨€ã®ä¸€è²«æ€§ä¿æŒ"""
    
    def __init__(self, base_dir: str = None):
        if base_dir is None:
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿéš›ã®å ´æ‰€ã‚’å–å¾—
            self.base_dir = Path(__file__).parent
        else:
            self.base_dir = Path(base_dir)
        self.consistency_dir = self.base_dir / "consistency_memory"
        self.consistency_dir.mkdir(exist_ok=True)
        
        # ä¸€è²«æ€§è¨˜éŒ²ãƒ•ã‚¡ã‚¤ãƒ«
        self.evaluations_file = self.consistency_dir / "evaluations_history.json"
        self.statements_file = self.consistency_dir / "statements_history.json"
        self.contradictions_file = self.consistency_dir / "contradictions_log.json"
        
        # è©•ä¾¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.evaluation_patterns = {
            "percentage": r'(\d+)(?:%|ï¼…|ç‚¹|ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ)',
            "completion_claims": [
                "å®Œæˆ", "å®Œäº†", "é”æˆ", "æˆåŠŸ", "100%", "100ï¼…", 
                "å®Œç’§", "perfect", "complete", "finished"
            ],
            "quality_levels": [
                "å„ªç§€", "ç´ æ™´ã‚‰ã—ã„", "å®Œç’§", "æœ€é«˜", "æœ€é©",
                "poor", "æ‚ªã„", "ä¸ååˆ†", "failed", "å¤±æ•—"
            ]
        }
    
    def record_evaluation(self, context: str, evaluation: str, score: Optional[float] = None) -> str:
        """è©•ä¾¡è¨˜éŒ² - ã‚·ã‚¹ãƒ†ãƒ ã‚„ã‚¿ã‚¹ã‚¯ã®è©•ä¾¡ã‚’è¨˜éŒ²"""
        
        timestamp = datetime.datetime.now()
        evaluation_id = hashlib.sha256(f"{timestamp.isoformat()}{context}".encode()).hexdigest()[:8]
        
        # æ•°å€¤è©•ä¾¡ã‚’æŠ½å‡º
        if score is None:
            score = self._extract_score(evaluation)
        
        evaluation_record = {
            "id": evaluation_id,
            "timestamp": timestamp.isoformat(),
            "context": context,
            "evaluation": evaluation,
            "score": score,
            "confidence_level": self._assess_confidence(evaluation),
            "claim_type": self._classify_claim_type(evaluation)
        }
        
        # æ—¢å­˜è¨˜éŒ²ã®èª­ã¿è¾¼ã¿
        evaluations = self._load_evaluations()
        evaluations.append(evaluation_record)
        
        # ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        consistency_issues = self._check_evaluation_consistency(evaluation_record, evaluations)
        if consistency_issues:
            self._log_contradiction(evaluation_record, consistency_issues)
        
        # è¨˜éŒ²ä¿å­˜
        self._save_evaluations(evaluations)
        
        return evaluation_id
    
    def record_statement(self, statement: str, category: str = "general") -> str:
        """ç™ºè¨€è¨˜éŒ² - AIç™ºè¨€ã®è¿½è·¡"""
        
        timestamp = datetime.datetime.now()
        statement_id = hashlib.sha256(f"{timestamp.isoformat()}{statement}".encode()).hexdigest()[:8]
        
        statement_record = {
            "id": statement_id,
            "timestamp": timestamp.isoformat(),
            "statement": statement,
            "category": category,
            "certainty_level": self._assess_certainty(statement),
            "claim_strength": self._assess_claim_strength(statement)
        }
        
        # æ—¢å­˜ç™ºè¨€ã¨ã®çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
        statements = self._load_statements()
        contradictions = self._check_statement_consistency(statement_record, statements)
        if contradictions:
            self._log_contradiction(statement_record, contradictions)
        
        statements.append(statement_record)
        self._save_statements(statements)
        
        return statement_id
    
    def check_before_evaluation(self, context: str, new_evaluation: str) -> Dict[str, Any]:
        """è©•ä¾¡å‰ãƒã‚§ãƒƒã‚¯ - æ—¢å­˜è©•ä¾¡ã¨ã®ä¸€è²«æ€§ç¢ºèª"""
        
        evaluations = self._load_evaluations()
        related_evaluations = [
            e for e in evaluations 
            if self._is_related_context(context, e["context"])
        ]
        
        if not related_evaluations:
            return {"can_proceed": True, "warnings": []}
        
        new_score = self._extract_score(new_evaluation)
        warnings = []
        
        for prev_eval in related_evaluations[-3:]:  # æœ€è¿‘3ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
            time_diff = self._calculate_time_diff(prev_eval["timestamp"])
            score_diff = abs(new_score - (prev_eval["score"] or 0)) if new_score and prev_eval["score"] else 0
            
            # çŸ­æ™‚é–“ã§ã®å¤§å¹…è©•ä¾¡å¤‰æ›´ã‚’è­¦å‘Š
            if time_diff < 30 and score_diff > 20:  # 30åˆ†ä»¥å†…ã«20ç‚¹ä»¥ä¸Šã®å¤‰æ›´
                warnings.append({
                    "type": "rapid_score_change",
                    "message": f"âš ï¸ è¨˜æ†¶æ”¹ç«„è­¦å‘Š: {time_diff}åˆ†å‰ã«{prev_eval['score']}ç‚¹è©•ä¾¡ â†’ ä»Šå›{new_score}ç‚¹ï¼ˆ{score_diff}ç‚¹å·®ï¼‰",
                    "previous_evaluation": prev_eval,
                    "risk_level": "high" if score_diff > 30 else "medium"
                })
        
        return {
            "can_proceed": len([w for w in warnings if w["risk_level"] == "high"]) == 0,
            "warnings": warnings,
            "related_evaluations": related_evaluations
        }
    
    def _extract_score(self, text: str) -> Optional[float]:
        """æ•°å€¤è©•ä¾¡æŠ½å‡º"""
        
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸æŠ½å‡º
        percentage_matches = re.findall(r'(\d+)(?:%|ï¼…|ç‚¹|ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ)', text)
        if percentage_matches:
            return float(percentage_matches[0])
        
        # ã€ŒXç‚¹æº€ç‚¹ä¸­Yç‚¹ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
        score_matches = re.findall(r'(\d+)ç‚¹.*?(\d+)ç‚¹', text)
        if score_matches:
            return float(score_matches[0][1])  # Yç‚¹ã‚’è¿”ã™
        
        return None
    
    def _assess_confidence(self, evaluation: str) -> str:
        """ç¢ºä¿¡åº¦è©•ä¾¡"""
        
        high_confidence = ["å®Œç’§", "ç¢ºå®Ÿ", "çµ¶å¯¾", "é–“é•ã„ãªã", "100%"]
        medium_confidence = ["æ€ã„ã¾ã™", "ã¨æ€ã†", "ã‹ã‚‚ã—ã‚Œ", "ãŠãã‚‰ã"]
        low_confidence = ["ã‚ã‹ã‚‰ãªã„", "ä¸æ˜", "ç¢ºèªãŒå¿…è¦", "ãƒ†ã‚¹ãƒˆå¿…è¦"]
        
        evaluation_lower = evaluation.lower()
        
        if any(word in evaluation_lower for word in high_confidence):
            return "high"
        elif any(word in evaluation_lower for word in low_confidence):
            return "low"
        elif any(word in evaluation_lower for word in medium_confidence):
            return "medium"
        else:
            return "medium"
    
    def _classify_claim_type(self, evaluation: str) -> str:
        """ä¸»å¼µã‚¿ã‚¤ãƒ—åˆ†é¡"""
        
        if any(word in evaluation for word in self.evaluation_patterns["completion_claims"]):
            return "completion_claim"
        elif re.search(self.evaluation_patterns["percentage"], evaluation):
            return "percentage_evaluation"
        elif any(word in evaluation for word in self.evaluation_patterns["quality_levels"]):
            return "quality_assessment"
        else:
            return "general_evaluation"
    
    def _check_evaluation_consistency(self, new_evaluation: Dict, history: List[Dict]) -> List[str]:
        """è©•ä¾¡ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯"""
        
        issues = []
        context = new_evaluation["context"]
        new_score = new_evaluation["score"]
        
        # åŒä¸€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æœ€è¿‘ã®è©•ä¾¡ã‚’ãƒã‚§ãƒƒã‚¯
        recent_evaluations = [
            e for e in history[-10:]  # æœ€è¿‘10ä»¶
            if self._is_related_context(context, e["context"])
            and e["id"] != new_evaluation["id"]
        ]
        
        for prev_eval in recent_evaluations:
            time_diff = self._calculate_time_diff(prev_eval["timestamp"])
            
            if time_diff < 60 and prev_eval["score"] and new_score:  # 1æ™‚é–“ä»¥å†…
                score_diff = abs(new_score - prev_eval["score"])
                
                if score_diff > 25:  # 25ç‚¹ä»¥ä¸Šã®å·®
                    issues.append(f"å¤§å¹…è©•ä¾¡å¤‰æ›´: {prev_eval['score']}ç‚¹ â†’ {new_score}ç‚¹ï¼ˆ{time_diff}åˆ†é–“ï¼‰")
                
                # å®Œæˆå®£è¨€ã®çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
                if (prev_eval["claim_type"] == "completion_claim" and 
                    new_evaluation["claim_type"] != "completion_claim" and 
                    new_score < 90):
                    issues.append(f"å®Œæˆå®£è¨€çŸ›ç›¾: ç›´å‰ã«å®Œæˆå®£è¨€ â†’ ä»Šå›{new_score}ç‚¹è©•ä¾¡")
        
        return issues
    
    def _is_related_context(self, context1: str, context2: str) -> bool:
        """é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®š"""
        
        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        keywords1 = set(re.findall(r'\w+', context1.lower()))
        keywords2 = set(re.findall(r'\w+', context2.lower()))
        
        overlap = len(keywords1 & keywords2)
        return overlap >= 2  # 2èªä»¥ä¸Šé‡è¤‡ã§é–¢é€£ã¨ã¿ãªã™
    
    def _calculate_time_diff(self, timestamp_str: str) -> int:
        """æ™‚é–“å·®è¨ˆç®—ï¼ˆåˆ†ï¼‰"""
        
        past_time = datetime.datetime.fromisoformat(timestamp_str)
        now = datetime.datetime.now()
        return int((now - past_time).total_seconds() / 60)
    
    def _log_contradiction(self, record: Dict, issues: List[str]) -> None:
        """çŸ›ç›¾ãƒ­ã‚°è¨˜éŒ²"""
        
        contradiction_log = {
            "timestamp": datetime.datetime.now().isoformat(),
            "record": record,
            "issues": issues,
            "severity": "high" if len(issues) > 2 else "medium"
        }
        
        # æ—¢å­˜ãƒ­ã‚°èª­ã¿è¾¼ã¿
        contradictions = []
        if self.contradictions_file.exists():
            with open(self.contradictions_file, 'r', encoding='utf-8') as f:
                contradictions = json.load(f)
        
        contradictions.append(contradiction_log)
        
        # ä¿å­˜
        with open(self.contradictions_file, 'w', encoding='utf-8') as f:
            json.dump(contradictions, f, ensure_ascii=False, indent=2)
    
    def _load_evaluations(self) -> List[Dict]:
        """è©•ä¾¡å±¥æ­´èª­ã¿è¾¼ã¿"""
        if self.evaluations_file.exists():
            with open(self.evaluations_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def _save_evaluations(self, evaluations: List[Dict]) -> None:
        """è©•ä¾¡å±¥æ­´ä¿å­˜"""
        with open(self.evaluations_file, 'w', encoding='utf-8') as f:
            json.dump(evaluations, f, ensure_ascii=False, indent=2)
    
    def _load_statements(self) -> List[Dict]:
        """ç™ºè¨€å±¥æ­´èª­ã¿è¾¼ã¿"""
        if self.statements_file.exists():
            with open(self.statements_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def _save_statements(self, statements: List[Dict]) -> None:
        """ç™ºè¨€å±¥æ­´ä¿å­˜"""
        with open(self.statements_file, 'w', encoding='utf-8') as f:
            json.dump(statements, f, ensure_ascii=False, indent=2)
    
    def get_consistency_report(self) -> Dict[str, Any]:
        """ä¸€è²«æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        evaluations = self._load_evaluations()
        statements = self._load_statements()
        
        # çŸ›ç›¾ãƒ­ã‚°èª­ã¿è¾¼ã¿
        contradictions = []
        if self.contradictions_file.exists():
            with open(self.contradictions_file, 'r', encoding='utf-8') as f:
                contradictions = json.load(f)
        
        recent_contradictions = [
            c for c in contradictions
            if self._calculate_time_diff(c["timestamp"]) < 1440  # 24æ™‚é–“ä»¥å†…
        ]
        
        return {
            "total_evaluations": len(evaluations),
            "total_statements": len(statements),
            "recent_contradictions": len(recent_contradictions),
            "consistency_score": max(0, 100 - len(recent_contradictions) * 10),
            "reliability_level": self._assess_reliability(recent_contradictions),
            "latest_contradictions": recent_contradictions[-5:] if recent_contradictions else []
        }
    
    def _assess_reliability(self, contradictions: List[Dict]) -> str:
        """ä¿¡é ¼æ€§ãƒ¬ãƒ™ãƒ«è©•ä¾¡"""
        
        if len(contradictions) == 0:
            return "high"
        elif len(contradictions) <= 2:
            return "medium"
        else:
            return "low"

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    engine = MemoryConsistencyEngine()
    
    # ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡è¨˜éŒ²
    engine.record_evaluation(
        "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡", 
        "çœŸã®100%é”æˆå®Œäº†ï¼å®Œç’§ãªã‚·ã‚¹ãƒ†ãƒ ã§ã™", 
        100.0
    )
    
    # çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
    check_result = engine.check_before_evaluation(
        "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡",
        "å®Ÿéš›ã¯70-80%ç¨‹åº¦ã®ã‚·ã‚¹ãƒ†ãƒ "
    )
    
    print("ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯çµæœ:")
    print(json.dumps(check_result, ensure_ascii=False, indent=2))
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = engine.get_consistency_report()
    print("\nä¸€è²«æ€§ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(json.dumps(report, ensure_ascii=False, indent=2))