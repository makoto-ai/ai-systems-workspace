#!/usr/bin/env python3
"""
ğŸ§  Cursor Enhanced Memory System - é«˜ç²¾åº¦ç‰ˆè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ 
ç¾çŠ¶ã®åŸºæœ¬ç‰ˆã‹ã‚‰å¤§å¹…ã«ç²¾åº¦å‘ä¸Šã—ãŸè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import json
import datetime
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import hashlib
try:
    from cursor_memory_system import CursorMemorySystem
except ImportError:
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    from cursor_memory_system import CursorMemorySystem

class CursorEnhancedMemory(CursorMemorySystem):
    """é«˜ç²¾åº¦ç‰ˆCursorè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, obsidian_vault_path: str = "docs/obsidian-knowledge"):
        super().__init__(obsidian_vault_path)
        
        # æ‹¡å¼µæ©Ÿèƒ½ç”¨ãƒ•ã‚©ãƒ«ãƒ€
        self.enhanced_dir = self.memory_dir / "enhanced"
        self.enhanced_dir.mkdir(exist_ok=True)
        
        # è©³ç´°è¨˜éŒ²ç”¨
        self.detailed_conversations = self.enhanced_dir / "detailed_conversations"
        self.conversation_patterns = self.enhanced_dir / "conversation_patterns"
        self.auto_summaries = self.enhanced_dir / "auto_summaries"
        self.workflow_memory = self.enhanced_dir / "workflow_memory"
        
        for folder in [self.detailed_conversations, self.conversation_patterns, self.auto_summaries, self.workflow_memory]:
            folder.mkdir(exist_ok=True)
        
        # é‡è¦åº¦åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.importance_keywords = {
            "critical": [
                "è«–æ–‡æ¤œç´¢", "ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³", "äº‹å®Ÿç¢ºèª", "YouTubeåŸç¨¿", 
                "å‰Šé™¤ç¦æ­¢", "å®‰å…¨ç¢ºèª", "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼", "å¿…é ˆãƒ•ãƒ­ãƒ¼"
            ],
            "high": [
                "å‰Šé™¤", "delete", "remove", "æ§‹ç¯‰", "build", "create",
                "å¤‰æ›´", "change", "modify", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", "backup",
                "ã‚¨ãƒ©ãƒ¼", "error", "å•é¡Œ", "issue", "è§£æ±º", "solve",
                "å®Œæˆ", "complete", "å®Ÿè£…", "implement"
            ],
            "medium": [
                "ç¢ºèª", "check", "ãƒ†ã‚¹ãƒˆ", "test", "å®Ÿè¡Œ", "run",
                "è¨­å®š", "config", "è¿½åŠ ", "add", "æ›´æ–°", "update",
                "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", "install", "èµ·å‹•", "start"
            ],
            "contextual": [
                "å‰å›", "ç¶šã", "å¼•ãç¶™ã", "è¨˜æ†¶", "è¦šãˆã¦", "æ€ã„å‡º",
                "ãªãœ", "ã©ã†ã—ã¦", "ç†ç”±", "åŸå› ", "ç›®çš„", "æ„å›³",
                "æ¬¡ã«", "ä»Šå¾Œ", "å°†æ¥", "è¨ˆç”»", "äºˆå®š"
            ]
        }
    
    def record_enhanced_conversation(self, user_message: str, ai_response: str, 
                                   context_before: str = "", context_after: str = "") -> str:
        """æ‹¡å¼µç‰ˆä¼šè©±è¨˜éŒ² - æ–‡è„ˆã¨é‡è¦åº¦ã‚’è©³ç´°ã«è¨˜éŒ²"""
        
        timestamp = datetime.datetime.now()
        conversation_id = hashlib.sha256(f"{timestamp.isoformat()}{user_message[:50]}".encode()).hexdigest()[:8]
        
        # é‡è¦åº¦åˆ¤å®š
        importance_level = self._analyze_importance(user_message, ai_response)
        
        # æ–‡è„ˆåˆ†æ
        context_analysis = self._analyze_context(user_message, ai_response, context_before, context_after)
        
        # æ„Ÿæƒ…ãƒ»ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹åˆ†æ
        emotional_context = self._analyze_emotional_context(user_message, ai_response)
        
        # è©³ç´°è¨˜éŒ²
        detailed_record = {
            "conversation_id": conversation_id,
            "timestamp": timestamp.isoformat(),
            "importance_level": importance_level,
            "context_analysis": context_analysis,
            "emotional_context": emotional_context,
            "conversation": {
                "user_message": user_message,
                "ai_response": ai_response,
                "context_before": context_before,
                "context_after": context_after
            },
            "extracted_entities": self._extract_entities(user_message, ai_response),
            "action_items": self._extract_action_items(user_message, ai_response),
            "questions_raised": self._extract_questions(user_message, ai_response),
            "decisions_made": self._extract_decisions(user_message, ai_response)
        }
        
        # ä¿å­˜
        detailed_file = self.detailed_conversations / f"{conversation_id}_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_record, f, ensure_ascii=False, indent=2)
        
        # åŸºæœ¬è¨˜éŒ²ã‚‚æ›´æ–°
        super().record_conversation(user_message, ai_response, "enhanced")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
        self._update_conversation_patterns(detailed_record)
        
        return conversation_id
    
    def _analyze_importance(self, user_message: str, ai_response: str) -> str:
        """é‡è¦åº¦åˆ†æ"""
        
        text = f"{user_message} {ai_response}".lower()
        
        # é‡è¦åº¦ãƒã‚§ãƒƒã‚¯
        critical_count = sum(1 for keyword in self.importance_keywords["critical"] if keyword in text)
        high_count = sum(1 for keyword in self.importance_keywords["high"] if keyword in text)
        medium_count = sum(1 for keyword in self.importance_keywords["medium"] if keyword in text)
        contextual_count = sum(1 for keyword in self.importance_keywords["contextual"] if keyword in text)
        
        if critical_count >= 1:
            return "critical"
        elif high_count >= 2:
            return "critical"
        elif high_count >= 1:
            return "high"
        elif medium_count >= 2 or contextual_count >= 1:
            return "medium"
        else:
            return "low"
    
    def _analyze_context(self, user_message: str, ai_response: str, 
                        context_before: str, context_after: str) -> Dict[str, Any]:
        """æ–‡è„ˆåˆ†æ"""
        
        return {
            "continuation_detected": any(word in user_message.lower() for word in ["ç¶šã", "å‰å›", "å¼•ãç¶™ã"]),
            "question_type": self._classify_question_type(user_message),
            "project_reference": self._extract_project_references(user_message, ai_response),
            "technical_content": self._extract_technical_content(user_message, ai_response),
            "time_references": self._extract_time_references(user_message, ai_response),
            "context_quality": {
                "before_available": bool(context_before.strip()),
                "after_available": bool(context_after.strip()),
                "context_richness": len(context_before.split()) + len(context_after.split())
            }
        }
    
    def _analyze_emotional_context(self, user_message: str, ai_response: str) -> Dict[str, Any]:
        """æ„Ÿæƒ…ãƒ»ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹åˆ†æ"""
        
        user_tone = self._detect_tone(user_message)
        ai_tone = self._detect_tone(ai_response)
        
        return {
            "user_tone": user_tone,
            "ai_tone": ai_tone,
            "uncertainty_detected": any(word in user_message.lower() for word in ["ã‚ã‹ã‚‰ãªã„", "ä¸æ˜", "ï¼Ÿ", "ç¢ºèª"]),
            "satisfaction_indicators": any(word in user_message.lower() for word in ["ã‚ã‚ŠãŒã¨ã†", "åŠ©ã‹ã‚‹", "å®Œç’§"]),
            "frustration_indicators": any(word in user_message.lower() for word in ["å‹˜å¼", "å›°ã‚‹", "æ€–ã„", "å¿ƒé…"]),
            "confidence_level": self._assess_confidence_level(user_message, ai_response)
        }
    
    def _extract_entities(self, user_message: str, ai_response: str) -> Dict[str, List[str]]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã€ã‚·ã‚¹ãƒ†ãƒ åã€æŠ€è¡“åãªã©ï¼‰"""
        
        text = f"{user_message} {ai_response}"
        
        entities = {
            "file_names": re.findall(r'[\w\-\_]+\.(py|md|json|sh|sql|txt)', text),
            "system_names": re.findall(r'([A-Z][a-z]+(?:[A-Z][a-z]+)*(?:ã‚·ã‚¹ãƒ†ãƒ |System))', text),
            "commands": re.findall(r'`([^`]+)`', text),
            "technologies": re.findall(r'(Python|JavaScript|SQL|Git|Docker|API|REST|JSON)', text),
            "project_names": re.findall(r'(voice-roleplay|paper-research|monitoring)', text)
        }
        
        return {k: list(set(v)) for k, v in entities.items() if v}
    
    def _extract_action_items(self, user_message: str, ai_response: str) -> List[str]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡º"""
        
        action_items = []
        
        # ã€Œã€œã—ã¦ãã ã•ã„ã€ã€Œã€œã—ãŸã„ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
        action_patterns = [
            r'(.+?)ã—ã¦ãã ã•ã„',
            r'(.+?)ã—ãŸã„',
            r'(.+?)ã™ã‚‹å¿…è¦ãŒã‚',
            r'(.+?)ã‚’å®Ÿè¡Œ',
            r'(.+?)ã‚’ãƒ†ã‚¹ãƒˆ'
        ]
        
        text = f"{user_message} {ai_response}"
        for pattern in action_patterns:
            matches = re.findall(pattern, text)
            action_items.extend([match.strip() for match in matches if len(match.strip()) > 5])
        
        return list(set(action_items))
    
    def _extract_questions(self, user_message: str, ai_response: str) -> List[str]:
        """è³ªå•æŠ½å‡º"""
        
        questions = []
        
        # è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³
        if 'ï¼Ÿ' in user_message or '?' in user_message:
            questions.append(user_message.strip())
        
        # ã€Œãªãœã€ã€Œã©ã†ã—ã¦ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
        question_starters = ['ãªãœ', 'ã©ã†ã—ã¦', 'ã©ã®ã‚ˆã†ã«', 'ã„ã¤', 'ã©ã“ã§', 'ã ã‚ŒãŒ']
        for starter in question_starters:
            if starter in user_message.lower():
                questions.append(user_message.strip())
                break
        
        return questions
    
    def _extract_decisions(self, user_message: str, ai_response: str) -> List[str]:
        """æ±ºå®šäº‹é …æŠ½å‡º"""
        
        decisions = []
        
        # æ±ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³
        decision_patterns = [
            r'(.+?)ã«æ±ºå®š',
            r'(.+?)ã™ã‚‹ã“ã¨ã«ã—',
            r'(.+?)ã‚’æ¡ç”¨',
            r'(.+?)ã§ç¢ºå®š'
        ]
        
        text = f"{user_message} {ai_response}"
        for pattern in decision_patterns:
            matches = re.findall(pattern, text)
            decisions.extend([match.strip() for match in matches if len(match.strip()) > 5])
        
        return decisions
    
    def generate_auto_summary(self, timeframe_hours: int = 24) -> str:
        """è‡ªå‹•è¦ç´„ç”Ÿæˆ"""
        
        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=timeframe_hours)
        
        # æœŸé–“å†…ã®ä¼šè©±åé›†
        conversations = []
        for file_path in self.detailed_conversations.glob("*.json"):
            with open(file_path, 'r', encoding='utf-8') as f:
                conv_data = json.load(f)
                conv_time = datetime.datetime.fromisoformat(conv_data['timestamp'])
                if conv_time > cutoff_time:
                    conversations.append(conv_data)
        
        if not conversations:
            return "æŒ‡å®šæœŸé–“å†…ã«è¨˜éŒ²ã•ã‚ŒãŸä¼šè©±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        # é‡è¦åº¦åˆ¥åˆ†é¡
        critical_conversations = [c for c in conversations if c['importance_level'] == 'critical']
        high_conversations = [c for c in conversations if c['importance_level'] == 'high']
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é›†è¨ˆ
        all_entities = {}
        all_actions = []
        all_decisions = []
        
        for conv in conversations:
            for entity_type, entities in conv.get('extracted_entities', {}).items():
                if entity_type not in all_entities:
                    all_entities[entity_type] = set()
                all_entities[entity_type].update(entities)
            
            all_actions.extend(conv.get('action_items', []))
            all_decisions.extend(conv.get('decisions_made', []))
        
        # è¦ç´„ç”Ÿæˆ
        summary = f"""# ğŸ§  Enhanced Memory Auto Summary ({timeframe_hours}æ™‚é–“)

**ç”Ÿæˆæ™‚åˆ»**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š çµ±è¨ˆ
- **ç·ä¼šè©±æ•°**: {len(conversations)}
- **é‡è¦åº¦Critical**: {len(critical_conversations)}
- **é‡è¦åº¦High**: {len(high_conversations)}

## ğŸ¯ ä¸»è¦ãªæ´»å‹•
"""
        
        if critical_conversations:
            summary += "\n### ğŸ”´ Criticalç´šã®é‡è¦äº‹é …\n"
            for conv in critical_conversations:
                summary += f"- {conv['conversation']['user_message'][:100]}...\n"
        
        if all_decisions:
            summary += "\n### âœ… æ±ºå®šäº‹é …\n"
            for decision in set(all_decisions):
                summary += f"- {decision}\n"
        
        if all_actions:
            summary += "\n### ğŸ“ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ \n"
            for action in set(all_actions):
                summary += f"- {action}\n"
        
        if all_entities:
            summary += "\n### ğŸ·ï¸ é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£\n"
            for entity_type, entities in all_entities.items():
                if entities:
                    summary += f"**{entity_type}**: {', '.join(list(entities))}\n"
        
        # è¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        summary_file = self.auto_summaries / f"summary_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        return summary
    
    def _classify_question_type(self, message: str) -> str:
        """è³ªå•ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ["ãªãœ", "ã©ã†ã—ã¦", "ç†ç”±"]):
            return "why_question"
        elif any(word in message_lower for word in ["ã©ã®ã‚ˆã†ã«", "ã‚„ã‚Šæ–¹", "æ–¹æ³•"]):
            return "how_question"
        elif any(word in message_lower for word in ["ã„ã¤", "ã‚¿ã‚¤ãƒŸãƒ³ã‚°"]):
            return "when_question"
        elif any(word in message_lower for word in ["ã©ã“", "å ´æ‰€"]):
            return "where_question"
        elif "ï¼Ÿ" in message or "?" in message:
            return "general_question"
        else:
            return "statement"
    
    def _extract_project_references(self, user_message: str, ai_response: str) -> List[str]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‚ç…§æŠ½å‡º"""
        text = f"{user_message} {ai_response}"
        
        projects = []
        project_patterns = [
            r'(voice-roleplay[^s\s]*)',
            r'(paper-research[^s\s]*)',
            r'(monitoring[^s\s]*)',
            r'(cursor[^s\s]*)',
            r'(obsidian[^s\s]*)'
        ]
        
        for pattern in project_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            projects.extend(matches)
        
        return list(set(projects))
    
    def _extract_technical_content(self, user_message: str, ai_response: str) -> Dict[str, Any]:
        """æŠ€è¡“çš„å†…å®¹æŠ½å‡º"""
        text = f"{user_message} {ai_response}"
        
        return {
            "code_mentioned": bool(re.search(r'`[^`]+`', text)),
            "file_operations": bool(any(word in text.lower() for word in ["å‰Šé™¤", "ä½œæˆ", "å¤‰æ›´", "ç§»å‹•"])),
            "system_operations": bool(any(word in text.lower() for word in ["å®Ÿè¡Œ", "èµ·å‹•", "åœæ­¢", "å†èµ·å‹•"])),
            "data_operations": bool(any(word in text.lower() for word in ["ä¿å­˜", "èª­ã¿è¾¼ã¿", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", "å¾©å…ƒ"]))
        }
    
    def _extract_time_references(self, user_message: str, ai_response: str) -> List[str]:
        """æ™‚é–“å‚ç…§æŠ½å‡º"""
        text = f"{user_message} {ai_response}"
        
        time_refs = []
        time_patterns = [
            r'(å‰å›|ä»Šå›|æ¬¡å›)',
            r'(æ˜¨æ—¥|ä»Šæ—¥|æ˜æ—¥)',
            r'(\d+æ—¥å‰|\d+æ™‚é–“å‰)',
            r'(å…ˆé€±|ä»Šé€±|æ¥é€±)',
            r'(æœ€è¿‘|æœ€å¾Œ|æœ€æ–°)'
        ]
        
        for pattern in time_patterns:
            matches = re.findall(pattern, text)
            time_refs.extend(matches)
        
        return list(set(time_refs))
    
    def _detect_tone(self, message: str) -> str:
        """ãƒˆãƒ¼ãƒ³æ¤œå‡º"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ["ã‚ã‚ŠãŒã¨ã†", "æ„Ÿè¬", "åŠ©ã‹ã‚‹", "ç´ æ™´ã‚‰ã—ã„"]):
            return "positive"
        elif any(word in message_lower for word in ["å›°ã‚‹", "æ€–ã„", "å¿ƒé…", "å‹˜å¼", "ä¸å®‰"]):
            return "negative"
        elif any(word in message_lower for word in ["ï¼Ÿ", "?", "ã‚ã‹ã‚‰ãªã„", "ç¢ºèª"]):
            return "questioning"
        elif any(word in message_lower for word in ["ï¼", "!", "ã™ã”ã„", "å®Œç’§"]):
            return "excited"
        else:
            return "neutral"
    
    def _assess_confidence_level(self, user_message: str, ai_response: str) -> str:
        """ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«è©•ä¾¡"""
        ai_lower = ai_response.lower()
        
        if any(word in ai_lower for word in ["ç¢ºå®Ÿ", "é–“é•ã„ãªã", "å®Œç’§", "100%"]):
            return "high"
        elif any(word in ai_lower for word in ["ãŠãã‚‰ã", "ãŸã¶ã‚“", "å¯èƒ½æ€§", "æ€ã„ã¾ã™"]):
            return "medium"
        elif any(word in ai_lower for word in ["ã‚ã‹ã‚‰ãªã„", "ä¸æ˜", "ç¢ºèªãŒå¿…è¦"]):
            return "low"
        else:
            return "medium"
    
    def _update_conversation_patterns(self, conversation_record: Dict[str, Any]) -> None:
        """ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’"""
        
        patterns_file = self.conversation_patterns / "patterns.json"
        
        if patterns_file.exists():
            with open(patterns_file, 'r', encoding='utf-8') as f:
                patterns = json.load(f)
        else:
            patterns = {
                "common_questions": {},
                "user_preferences": {},
                "interaction_patterns": {}
            }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        question_type = conversation_record['context_analysis']['question_type']
        if question_type not in patterns["common_questions"]:
            patterns["common_questions"][question_type] = 0
        patterns["common_questions"][question_type] += 1
        
        # ä¿å­˜
        with open(patterns_file, 'w', encoding='utf-8') as f:
            json.dump(patterns, f, ensure_ascii=False, indent=2)
    
    def record_workflow_step(self, step_name: str, step_details: Dict[str, Any]) -> str:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¨˜æ†¶æ©Ÿèƒ½ - è«–æ–‡æ¤œç´¢â†’åŸç¨¿ä½œæˆç­‰ã®é‡è¦ãƒ•ãƒ­ãƒ¼ã‚’è¨˜éŒ²"""
        
        timestamp = datetime.datetime.now()
        workflow_id = hashlib.sha256(f"{timestamp.isoformat()}{step_name}".encode()).hexdigest()[:8]
        
        workflow_record = {
            "workflow_id": workflow_id,
            "step_name": step_name,
            "timestamp": timestamp.isoformat(),
            "details": step_details,
            "workflow_type": self._classify_workflow_type(step_name, step_details)
        }
        
        # é‡è¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åˆ¤å®š
        if any(keyword in step_name.lower() for keyword in ["è«–æ–‡æ¤œç´¢", "äº‹å®Ÿç¢ºèª", "åŸç¨¿ä½œæˆ", "youtube"]):
            workflow_record["importance"] = "critical"
            
            # è«–æ–‡æ¤œç´¢â†’åŸç¨¿ä½œæˆãƒ•ãƒ­ãƒ¼ã®ç¢ºèª
            if "åŸç¨¿ä½œæˆ" in step_name and not self._check_paper_search_prerequisite():
                workflow_record["warning"] = "è«–æ–‡æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ æœªå®Ÿè¡Œ - ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯"
        
        # ä¿å­˜
        workflow_file = self.workflow_memory / f"workflow_{workflow_id}_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        with open(workflow_file, 'w', encoding='utf-8') as f:
            json.dump(workflow_record, f, ensure_ascii=False, indent=2)
        
        return workflow_id
    
    def _classify_workflow_type(self, step_name: str, step_details: Dict[str, Any]) -> str:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        
        step_lower = step_name.lower()
        
        if any(keyword in step_lower for keyword in ["è«–æ–‡æ¤œç´¢", "research", "paper"]):
            return "research_workflow"
        elif any(keyword in step_lower for keyword in ["åŸç¨¿ä½œæˆ", "youtube", "script"]):
            return "content_creation_workflow"
        elif any(keyword in step_lower for keyword in ["äº‹å®Ÿç¢ºèª", "fact", "verify"]):
            return "verification_workflow"
        elif any(keyword in step_lower for keyword in ["ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰", "system", "build"]):
            return "system_workflow"
        else:
            return "general_workflow"
    
    def _check_paper_search_prerequisite(self) -> bool:
        """è«–æ–‡æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œæ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        
        # éå»24æ™‚é–“ä»¥å†…ã®è«–æ–‡æ¤œç´¢ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç¢ºèª
        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=24)
        
        for workflow_file in self.workflow_memory.glob("workflow_*.json"):
            try:
                with open(workflow_file, 'r', encoding='utf-8') as f:
                    workflow_data = json.load(f)
                    workflow_time = datetime.datetime.fromisoformat(workflow_data['timestamp'])
                    
                    if (workflow_time > cutoff_time and 
                        workflow_data.get('workflow_type') == 'research_workflow'):
                        return True
            except:
                continue
        
        return False
    
    def get_workflow_context(self, workflow_type: str = None, hours: int = 24) -> Dict[str, Any]:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ–‡è„ˆå–å¾—"""
        
        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=hours)
        workflows = []
        
        for workflow_file in self.workflow_memory.glob("workflow_*.json"):
            try:
                with open(workflow_file, 'r', encoding='utf-8') as f:
                    workflow_data = json.load(f)
                    workflow_time = datetime.datetime.fromisoformat(workflow_data['timestamp'])
                    
                    if workflow_time > cutoff_time:
                        if workflow_type is None or workflow_data.get('workflow_type') == workflow_type:
                            workflows.append(workflow_data)
            except:
                continue
        
        return {
            "total_workflows": len(workflows),
            "workflow_types": list(set(w.get('workflow_type', 'unknown') for w in workflows)),
            "critical_workflows": [w for w in workflows if w.get('importance') == 'critical'],
            "warnings": [w for w in workflows if 'warning' in w],
            "recent_workflows": sorted(workflows, key=lambda x: x['timestamp'], reverse=True)[:5]
        }


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # é«˜ç²¾åº¦ç‰ˆãƒ†ã‚¹ãƒˆ
    enhanced_memory = CursorEnhancedMemory()
    
    # æ‹¡å¼µè¨˜éŒ²ãƒ†ã‚¹ãƒˆ
    conversation_id = enhanced_memory.record_enhanced_conversation(
        "ã“ã®è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦ã‚’ã‚‚ã£ã¨ä¸Šã’ã‚‰ã‚Œã‚‹ã®ï¼Ÿ",
        "ã¯ã„ï¼ç¢ºå®Ÿã«ç²¾åº¦ã‚’ä¸Šã’ã‚‰ã‚Œã¾ã™ã€‚ç¾çŠ¶ã¯åŸºæœ¬ç‰ˆã§ã€ã¾ã ã¾ã æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚",
        context_before="ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ 100%å¾©æ—§èƒ½åŠ›ç¢ºèªå¾Œ",
        context_after="å…·ä½“çš„ãªæ”¹å–„ç­–ã®ææ¡ˆã¸"
    )
    
    print(f"âœ… æ‹¡å¼µè¨˜éŒ²å®Œäº†: {conversation_id}")
    
    # è‡ªå‹•è¦ç´„ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    summary = enhanced_memory.generate_auto_summary(1)  # éå»1æ™‚é–“
    print("\nğŸ“‹ è‡ªå‹•è¦ç´„:")
    print(summary)