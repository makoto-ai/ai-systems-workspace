#!/usr/bin/env python3
"""
æ®µéšæ˜‡æ ¼ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
ç¾åœ¨å€¤ã®å®Ÿæ¸¬ â†’ å®‰å…¨ã‚¹ãƒ†ãƒƒãƒ—è¨ˆç®— â†’ æ˜‡æ ¼å¯å¦åˆ¤å®š
"""

import json
import yaml
from pathlib import Path
from typing import Dict, Any, Tuple, Optional


class StagedPromotionDecider:
    def __init__(self):
        self.min_step = 0.02
        self.max_step = 0.05
        self.config_path = "tests/golden/config.yml"
        self.shadow_report_path = "out/shadow_grid.json"
    
    def get_current_threshold_reality(self) -> float:
        """config.ymlã‹ã‚‰å®Ÿéš›ã®ç¾åœ¨ã—ãã„å€¤ã‚’å–å¾—"""
        config_path = Path(self.config_path)
        if not config_path.exists():
            raise FileNotFoundError(f"Config not found: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        return float(config.get('threshold', 0.5))
    
    def detect_threshold_divergence(self) -> Tuple[float, float, bool]:
        """ã—ãã„å€¤ã®ä¹–é›¢ã‚’æ¤œå‡º"""
        config_current = self.get_current_threshold_reality()
        
        # Shadow reportã‹ã‚‰æ¨å®šå€¤ã‚’å–å¾—
        report_path = Path(self.shadow_report_path)
        if not report_path.exists():
            print(f"âš ï¸ Shadow report not found: {report_path}")
            return config_current, 0.0, True
        
        with open(report_path, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        staged_promotion = report.get('multi_shadow_evaluation', {}).get('staged_promotion', {})
        report_assumed = float(staged_promotion.get('current_threshold', 0.0))
        
        # ä¹–é›¢ãƒã‚§ãƒƒã‚¯ï¼ˆ0.01ã‚’è¶…ãˆãŸã‚‰ä¹–é›¢ã¨ã¿ãªã™ï¼‰
        divergence_detected = abs(config_current - report_assumed) > 0.01
        
        print(f"ğŸ“Š ã—ãã„å€¤ä¹–é›¢ãƒã‚§ãƒƒã‚¯:")
        print(f"  Configå®Ÿå€¤: {config_current:.2f}")
        print(f"  Reportæ¨å®š: {report_assumed:.2f}")
        print(f"  ä¹–é›¢: {config_current - report_assumed:+.3f}")
        print(f"  ä¹–é›¢æ¤œå‡º: {'âŒ YES' if divergence_detected else 'âœ… NO'}")
        
        return config_current, report_assumed, divergence_detected
    
    def calculate_safe_next_target(self, current: float) -> float:
        """å®‰å…¨ãªæ¬¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨ˆç®—ï¼ˆå±é™ºå¹…ã‚¯ãƒ©ãƒ³ãƒ—é©ç”¨ï¼‰"""
        # åŸºæœ¬ã‚¹ãƒ†ãƒƒãƒ—ã¯0.02
        proposed_step = self.min_step
        proposed_target = current + proposed_step
        
        # ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¯ãƒ©ãƒ³ãƒ—ï¼ˆ0.02-0.05ã®ç¯„å›²å†…ï¼‰
        clamped_step = max(self.min_step, min(proposed_step, self.max_step))
        
        if clamped_step != proposed_step:
            print(f"âš ï¸ ã‚¹ãƒ†ãƒƒãƒ—ã‚¯ãƒ©ãƒ³ãƒ—é©ç”¨: {proposed_step:.2f} â†’ {clamped_step:.2f}")
        
        # 0.02åˆ»ã¿ã«ä¸¸ã‚ã‚‹
        safe_target = current + clamped_step
        rounded_target = round(safe_target / 0.02) * 0.02
        
        print(f"ğŸ¯ å®‰å…¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—:")
        print(f"  Current: {current:.2f}")
        print(f"  Step: +{clamped_step:.2f}")
        print(f"  Target: {rounded_target:.2f}")
        
        # æœŸå¾…å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆ0.70 â†’ 0.72ã‚’æœŸå¾…ï¼‰
        if current == 0.70:
            expected_target = 0.72
            if abs(rounded_target - expected_target) < 0.001:
                print(f"âœ… æœŸå¾…ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¸€è‡´: {expected_target:.2f}")
            else:
                print(f"âš ï¸ æœŸå¾…ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¸ä¸€è‡´: expected={expected_target:.2f}, actual={rounded_target:.2f}")
        
        return rounded_target
    
    def validate_promotion_step(self, current: float, target: float) -> Tuple[bool, str]:
        """æ˜‡æ ¼ã‚¹ãƒ†ãƒƒãƒ—ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        step = target - current
        
        # å±é™ºå¹…ãƒã‚§ãƒƒã‚¯
        if step > self.max_step:
            return False, f"overshoot: ã‚¹ãƒ†ãƒƒãƒ—{step:.2f}ãŒä¸Šé™{self.max_step:.2f}ã‚’è¶…é"
        
        if step < self.min_step:
            return False, f"understep: ã‚¹ãƒ†ãƒƒãƒ—{step:.2f}ãŒä¸‹é™{self.min_step:.2f}æœªæº€"
        
        # æœŸå¾…ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if target not in [0.72, 0.75]:
            return False, f"unexpected_target: {target:.2f}ã¯æœŸå¾…ç¯„å›²å¤–ï¼ˆ0.72 or 0.75ï¼‰"
        
        # 0.72ã‚’å„ªå…ˆ
        if current == 0.70 and target != 0.72:
            return False, f"priority_violation: 0.70ã‹ã‚‰ã¯0.72ã‚’å„ªå…ˆã™ã¹ãï¼ˆactual: {target:.2f}ï¼‰"
        
        return True, f"valid: {current:.2f} â†’ {target:.2f} (+{step:.2f})"
    
    def decide_staged_promotion(self) -> Dict[str, Any]:
        """æ®µéšæ˜‡æ ¼ã‚’æ±ºå®š"""
        print("ğŸš€ æ®µéšæ˜‡æ ¼æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹")
        
        # 1. ç¾åœ¨å€¤ã®å®Ÿæ¸¬ã¨ä¹–é›¢ãƒã‚§ãƒƒã‚¯
        config_current, report_assumed, has_divergence = self.detect_threshold_divergence()
        
        # 2. ä¹–é›¢æ™‚ã¯å®Ÿå€¤ã‚’æ¡ç”¨
        if has_divergence:
            print(f"ğŸ”§ ä¹–é›¢ä¿®æ­£: reportæ¨å®šå€¤ã‚’ç„¡è¦–ã€configå®Ÿå€¤({config_current:.2f})ã‚’æ¡ç”¨")
            actual_current = config_current
        else:
            actual_current = config_current
        
        # 3. å®‰å…¨ãªæ¬¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—
        next_target = self.calculate_safe_next_target(actual_current)
        
        # 4. æ˜‡æ ¼ã‚¹ãƒ†ãƒƒãƒ—ã®å¦¥å½“æ€§æ¤œè¨¼
        is_valid, validation_message = self.validate_promotion_step(actual_current, next_target)
        
        # 5. æ±ºå®šçµæœ
        decision = {
            "timestamp": json.loads(json.dumps({"ts": "now"}, default=str))["ts"],
            "current_threshold": {
                "config_value": config_current,
                "report_assumed": report_assumed,
                "actual_used": actual_current,
                "divergence_detected": has_divergence
            },
            "promotion": {
                "target_threshold": next_target,
                "step": next_target - actual_current,
                "valid": is_valid,
                "validation_message": validation_message
            },
            "safety_constraints": {
                "min_step": self.min_step,
                "max_step": self.max_step,
                "clamping_applied": abs(next_target - actual_current) != self.min_step
            },
            "next_action": "create_canary_pr" if is_valid else "block_promotion"
        }
        
        print(f"\nğŸ“‹ æ®µéšæ˜‡æ ¼æ±ºå®šçµæœ:")
        print(f"  å®Ÿéš›ã®ç¾åœ¨å€¤: {actual_current:.2f}")
        print(f"  æ¬¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {next_target:.2f}")
        print(f"  æ˜‡æ ¼ã‚¹ãƒ†ãƒƒãƒ—: +{next_target - actual_current:.2f}")
        print(f"  å¦¥å½“æ€§: {'âœ… VALID' if is_valid else 'âŒ INVALID'}")
        print(f"  ç†ç”±: {validation_message}")
        print(f"  æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {decision['next_action']}")
        
        return decision
    
    def save_decision(self, decision: Dict[str, Any], output_path: str = "out/staged_decision.json"):
        """æ±ºå®šçµæœã‚’ä¿å­˜"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(decision, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ æ±ºå®šçµæœä¿å­˜: {output_file}")
        return output_file


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    decider = StagedPromotionDecider()
    
    try:
        # æ®µéšæ˜‡æ ¼æ±ºå®š
        decision = decider.decide_staged_promotion()
        
        # çµæœä¿å­˜
        decider.save_decision(decision)
        
        # æˆåŠŸ/å¤±æ•—ã®çµ‚äº†ã‚³ãƒ¼ãƒ‰
        exit_code = 0 if decision["promotion"]["valid"] else 1
        print(f"\nğŸ¯ çµ‚äº†ã‚³ãƒ¼ãƒ‰: {exit_code}")
        return exit_code
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return 2


if __name__ == "__main__":
    import sys
    sys.exit(main())



