#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§æ¤œè¨¼
- è‡ªå‹•å¾©å…ƒãƒ†ã‚¹ãƒˆ
"""

import os
import json
import hashlib
import datetime
import subprocess
import sys
from pathlib import Path


class DataIntegrityChecker:
    def __init__(self):
        self.project_root = Path.cwd()
        self.data_dir = self.project_root / "data"
        self.backup_dir = self.project_root
        self.results = {
            "timestamp": datetime.datetime.now().isoformat(),
            "checks": [],
            "overall_status": "PENDING",
        }

    def calculate_file_hash(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA256ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        sha256_hash = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            return sha256_hash.hexdigest()
        except Exception as e:
            return f"ERROR: {str(e)}"

    def check_critical_files(self):
        """é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã¨æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        critical_files = [
            "data/usage_limits.json",
            "requirements.txt",
            "app/main.py",
            ".env.local",
        ]

        for file_path in critical_files:
            full_path = self.project_root / file_path
            check_result = {
                "name": f"Critical File: {file_path}",
                "type": "file_integrity",
                "status": "UNKNOWN",
                "details": {},
            }

            if full_path.exists():
                file_hash = self.calculate_file_hash(full_path)
                file_size = full_path.stat().st_size
                check_result.update(
                    {
                        "status": "OK",
                        "details": {
                            "exists": True,
                            "size_bytes": file_size,
                            "hash": file_hash,
                            "last_modified": datetime.datetime.fromtimestamp(
                                full_path.stat().st_mtime
                            ).isoformat(),
                        },
                    }
                )
            else:
                check_result.update(
                    {
                        "status": "FAILED",
                        "details": {"exists": False, "error": "File not found"},
                    }
                )

            self.results["checks"].append(check_result)

    def check_backup_integrity(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        import os
        from pathlib import Path

        backup_locations = []
        base_dir = self.project_root

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        if (base_dir / "backups").exists():
            backup_files = list((base_dir / "backups").rglob("*"))
            backup_locations.extend([str(f) for f in backup_files if f.is_file()])

        # .bakãƒ•ã‚¡ã‚¤ãƒ«
        bak_files = list(base_dir.glob("**/*.bak"))
        backup_locations.extend(
            [
                str(f)
                for f in bak_files
                if "node_modules" not in str(f) and ".venv" not in str(f)
            ]
        )

        # .backupãƒ•ã‚¡ã‚¤ãƒ«
        backup_files = list(base_dir.glob("**/*.backup"))
        backup_locations.extend(
            [
                str(f)
                for f in backup_files
                if "node_modules" not in str(f) and ".venv" not in str(f)
            ]
        )

        # tar.gzãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«
        tar_files = list(base_dir.glob("**/*backup*.tar.gz"))
        backup_locations.extend([str(f) for f in tar_files])

        check_result = {
            "name": "Backup Files Integrity",
            "type": "backup_integrity",
            "status": "OK" if len(backup_locations) > 0 else "WARNING",
            "details": {
                "backup_count": len(backup_locations),
                "backup_files": backup_locations[:5],  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
                "status_message": (
                    f"Found {len(backup_locations)} backup files"
                    if len(backup_locations) > 0
                    else "No backup files found"
                ),
            },
        }

        self.results["checks"].append(check_result)

    def check_data_directory(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        check_result = {
            "name": "Data Directory Structure",
            "type": "data_structure",
            "status": "UNKNOWN",
            "details": {},
        }

        if not self.data_dir.exists():
            check_result.update(
                {
                    "status": "FAILED",
                    "details": {"error": "Data directory does not exist"},
                }
            )
        else:
            # usage_limits.jsonã®è©³ç´°ãƒã‚§ãƒƒã‚¯
            usage_limits_file = self.data_dir / "usage_limits.json"
            if usage_limits_file.exists():
                try:
                    with open(usage_limits_file, "r", encoding="utf-8") as f:
                        data = json.load(f)

                    check_result.update(
                        {
                            "status": "OK",
                            "details": {
                                "usage_limits_json": {
                                    "exists": True,
                                    "user_count": len(data),
                                    "total_entries": (
                                        sum(
                                            len(user_data)
                                            for user_data in data.values()
                                        )
                                        if isinstance(data, dict)
                                        else 0
                                    ),
                                    "file_size_kb": round(
                                        usage_limits_file.stat().st_size / 1024, 2
                                    ),
                                }
                            },
                        }
                    )
                except json.JSONDecodeError as e:
                    check_result.update(
                        {
                            "status": "FAILED",
                            "details": {
                                "usage_limits_json": {
                                    "exists": True,
                                    "error": f"JSON parse error: {str(e)}",
                                }
                            },
                        }
                    )
                except Exception as e:
                    check_result.update(
                        {
                            "status": "FAILED",
                            "details": {
                                "usage_limits_json": {
                                    "exists": True,
                                    "error": f"Read error: {str(e)}",
                                }
                            },
                        }
                    )
            else:
                check_result.update(
                    {
                        "status": "WARNING",
                        "details": {
                            "usage_limits_json": {
                                "exists": False,
                                "warning": "usage_limits.json not found",
                            }
                        },
                    }
                )

        self.results["checks"].append(check_result)

    def run_all_checks(self):
        """å…¨ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
        print("ğŸ” ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§æ¤œè¨¼é–‹å§‹...")

        self.check_critical_files()
        self.check_backup_integrity()
        self.check_data_directory()

        # å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®æ±ºå®š
        failed_checks = [c for c in self.results["checks"] if c["status"] == "FAILED"]
        warning_checks = [c for c in self.results["checks"] if c["status"] == "WARNING"]

        if failed_checks:
            self.results["overall_status"] = "FAILED"
        elif warning_checks:
            self.results["overall_status"] = "WARNING"
        else:
            self.results["overall_status"] = "OK"

        return self.results

    def save_report(self, filename="data_integrity_report.json"):
        """æ¤œè¨¼çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        report_path = self.project_root / filename
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“‹ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        return report_path


def main():
    checker = DataIntegrityChecker()
    results = checker.run_all_checks()

    # çµæœã®è¡¨ç¤º
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§æ¤œè¨¼çµæœ")
    print(f"ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {results['overall_status']}")
    print(f"å®Ÿè¡Œæ™‚åˆ»: {results['timestamp']}")
    print(f"ãƒã‚§ãƒƒã‚¯é …ç›®æ•°: {len(results['checks'])}")

    # å„ãƒã‚§ãƒƒã‚¯çµæœã®æ¦‚è¦
    for check in results["checks"]:
        status_emoji = (
            "âœ…"
            if check["status"] == "OK"
            else "âš ï¸" if check["status"] == "WARNING" else "âŒ"
        )
        print(f"{status_emoji} {check['name']}: {check['status']}")

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    checker.save_report()

    # å¤±æ•—ãŒã‚ã‚Œã°çµ‚äº†ã‚³ãƒ¼ãƒ‰1
    if results["overall_status"] == "FAILED":
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ã¯ä¸Šè¨˜ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    elif results["overall_status"] == "WARNING":
        print("\nâš ï¸ ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã«è»½å¾®ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚")
        sys.exit(0)
    else:
        print("\nâœ… ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã«å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚")
        sys.exit(0)


if __name__ == "__main__":
    main()
